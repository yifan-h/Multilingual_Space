Fine-tuning /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/xlm-roberta-large on mlqa using GPU 1
Load data from /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download/, and save models to /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter/
************************
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/xlm-roberta-large
************************

Predictions on mlqa
  en 
2022-05-09 23:46:29.170067: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/09/2022 23:46:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'qa_outputs.bias', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.self.key.weight', 'qa_outputs.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.21.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/09/2022 23:46:55 - INFO - __main__ -   lang2id = None
05/09/2022 23:46:58 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='en', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-en-question-en.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/09/2022 23:46:58 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/09/2022 23:46:58 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'qa_outputs.bias', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.self.key.weight', 'qa_outputs.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.21.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/09/2022 23:47:27 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/5010 [00:00<?, ?it/s]  0%|          | 25/5010 [00:00<00:20, 247.57it/s]  1%|▏         | 68/5010 [00:00<00:15, 326.50it/s]  2%|▏         | 109/5010 [00:00<00:13, 362.27it/s]  3%|▎         | 159/5010 [00:00<00:11, 410.02it/s]  4%|▍         | 211/5010 [00:00<00:10, 443.89it/s]  5%|▌         | 256/5010 [00:00<00:11, 411.67it/s]  7%|▋         | 334/5010 [00:00<00:08, 522.81it/s]  8%|▊         | 396/5010 [00:00<00:08, 537.66it/s] 10%|▉         | 482/5010 [00:00<00:07, 632.22it/s] 11%|█         | 561/5010 [00:01<00:06, 672.33it/s] 13%|█▎        | 629/5010 [00:01<00:06, 663.98it/s] 14%|█▍        | 702/5010 [00:01<00:09, 432.77it/s] 15%|█▌        | 757/5010 [00:01<00:10, 414.93it/s] 16%|█▌        | 811/5010 [00:01<00:09, 438.29it/s] 17%|█▋        | 861/5010 [00:01<00:09, 446.02it/s] 18%|█▊        | 910/5010 [00:01<00:09, 454.64it/s] 21%|██        | 1052/5010 [00:02<00:05, 700.26it/s] 23%|██▎       | 1128/5010 [00:02<00:05, 708.72it/s] 24%|██▍       | 1203/5010 [00:02<00:05, 682.74it/s] 26%|██▌       | 1282/5010 [00:02<00:05, 711.71it/s] 27%|██▋       | 1356/5010 [00:02<00:05, 697.05it/s] 29%|██▉       | 1457/5010 [00:02<00:04, 764.47it/s] 31%|███       | 1559/5010 [00:02<00:04, 833.33it/s] 33%|███▎      | 1665/5010 [00:02<00:03, 897.81it/s] 36%|███▌      | 1816/5010 [00:02<00:02, 1064.86it/s] 38%|███▊      | 1924/5010 [00:02<00:02, 1053.52it/s] 42%|████▏     | 2126/5010 [00:03<00:02, 1329.52it/s] 48%|████▊     | 2396/5010 [00:03<00:01, 1728.67it/s] 51%|█████▏    | 2571/5010 [00:03<00:01, 1350.33it/s] 54%|█████▍    | 2720/5010 [00:03<00:01, 1269.56it/s] 57%|█████▋    | 2857/5010 [00:03<00:01, 1267.45it/s] 60%|█████▉    | 2991/5010 [00:03<00:01, 1214.14it/s] 62%|██████▏   | 3118/5010 [00:03<00:01, 980.23it/s]  64%|██████▍   | 3226/5010 [00:04<00:01, 922.35it/s] 66%|██████▋   | 3325/5010 [00:04<00:01, 875.70it/s] 68%|██████▊   | 3417/5010 [00:04<00:02, 724.54it/s] 70%|███████   | 3507/5010 [00:04<00:01, 759.13it/s] 72%|███████▏  | 3589/5010 [00:04<00:01, 719.99it/s] 76%|███████▌  | 3787/5010 [00:04<00:01, 1015.61it/s] 79%|███████▉  | 3963/5010 [00:04<00:00, 1197.99it/s] 82%|████████▏ | 4128/5010 [00:04<00:00, 1278.32it/s] 85%|████████▌ | 4264/5010 [00:05<00:00, 1169.99it/s] 88%|████████▊ | 4388/5010 [00:05<00:00, 1126.93it/s] 91%|█████████ | 4553/5010 [00:05<00:00, 1220.67it/s] 94%|█████████▍| 4721/5010 [00:05<00:00, 1340.37it/s] 98%|█████████▊| 4916/5010 [00:05<00:00, 1505.64it/s]100%|██████████| 5010/5010 [00:05<00:00, 899.22it/s] 
convert squad examples to features:   0%|          | 0/11590 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/11590 [00:00<42:07,  4.59it/s]convert squad examples to features:   0%|          | 33/11590 [00:00<02:08, 90.26it/s]convert squad examples to features:   1%|          | 65/11590 [00:00<01:46, 108.16it/s]convert squad examples to features:   1%|          | 97/11590 [00:01<02:05, 91.42it/s] convert squad examples to features:   1%|          | 129/11590 [00:01<01:52, 102.27it/s]convert squad examples to features:   1%|▏         | 161/11590 [00:01<01:52, 101.35it/s]convert squad examples to features:   2%|▏         | 193/11590 [00:01<01:49, 104.38it/s]convert squad examples to features:   2%|▏         | 225/11590 [00:02<01:38, 114.85it/s]convert squad examples to features:   2%|▏         | 257/11590 [00:02<01:44, 108.87it/s]convert squad examples to features:   2%|▏         | 289/11590 [00:02<01:43, 109.08it/s]convert squad examples to features:   3%|▎         | 321/11590 [00:03<01:44, 107.49it/s]convert squad examples to features:   3%|▎         | 353/11590 [00:03<02:01, 92.22it/s] convert squad examples to features:   3%|▎         | 385/11590 [00:03<01:54, 97.50it/s]convert squad examples to features:   4%|▎         | 417/11590 [00:04<01:43, 108.45it/s]convert squad examples to features:   4%|▍         | 449/11590 [00:04<01:37, 114.17it/s]convert squad examples to features:   4%|▍         | 481/11590 [00:04<01:44, 106.43it/s]convert squad examples to features:   4%|▍         | 513/11590 [00:04<01:36, 114.44it/s]convert squad examples to features:   5%|▍         | 545/11590 [00:05<01:35, 116.14it/s]convert squad examples to features:   5%|▍         | 577/11590 [00:05<01:21, 135.83it/s]convert squad examples to features:   5%|▌         | 609/11590 [00:05<01:14, 148.10it/s]convert squad examples to features:   6%|▌         | 641/11590 [00:05<01:09, 156.80it/s]convert squad examples to features:   6%|▌         | 673/11590 [00:05<01:15, 145.48it/s]convert squad examples to features:   6%|▌         | 705/11590 [00:06<01:13, 148.41it/s]convert squad examples to features:   6%|▋         | 737/11590 [00:06<01:29, 120.76it/s]convert squad examples to features:   7%|▋         | 769/11590 [00:06<01:20, 134.47it/s]convert squad examples to features:   7%|▋         | 801/11590 [00:06<01:25, 125.59it/s]convert squad examples to features:   7%|▋         | 833/11590 [00:07<01:30, 119.28it/s]convert squad examples to features:   7%|▋         | 865/11590 [00:07<01:25, 125.32it/s]convert squad examples to features:   8%|▊         | 897/11590 [00:07<01:16, 140.30it/s]convert squad examples to features:   8%|▊         | 929/11590 [00:07<01:19, 134.70it/s]convert squad examples to features:   8%|▊         | 961/11590 [00:08<01:13, 143.99it/s]convert squad examples to features:   9%|▊         | 993/11590 [00:08<01:12, 146.70it/s]convert squad examples to features:   9%|▉         | 1025/11590 [00:08<01:24, 125.63it/s]convert squad examples to features:   9%|▉         | 1057/11590 [00:09<01:50, 94.96it/s] convert squad examples to features:   9%|▉         | 1089/11590 [00:09<02:04, 84.29it/s]convert squad examples to features:  10%|▉         | 1121/11590 [00:09<01:47, 97.72it/s]convert squad examples to features:  10%|▉         | 1153/11590 [00:10<01:31, 113.68it/s]convert squad examples to features:  10%|█         | 1185/11590 [00:10<01:28, 117.28it/s]convert squad examples to features:  11%|█         | 1217/11590 [00:10<01:20, 128.97it/s]convert squad examples to features:  11%|█         | 1249/11590 [00:10<01:21, 126.85it/s]convert squad examples to features:  11%|█         | 1281/11590 [00:10<01:09, 149.35it/s]convert squad examples to features:  11%|█▏        | 1313/11590 [00:11<01:13, 139.82it/s]convert squad examples to features:  12%|█▏        | 1345/11590 [00:11<01:34, 108.31it/s]convert squad examples to features:  12%|█▏        | 1377/11590 [00:11<01:28, 115.56it/s]convert squad examples to features:  12%|█▏        | 1409/11590 [00:12<01:29, 113.16it/s]convert squad examples to features:  12%|█▏        | 1441/11590 [00:12<01:29, 113.92it/s]convert squad examples to features:  13%|█▎        | 1473/11590 [00:12<01:35, 106.30it/s]convert squad examples to features:  13%|█▎        | 1505/11590 [00:12<01:19, 126.11it/s]convert squad examples to features:  13%|█▎        | 1537/11590 [00:13<01:08, 145.75it/s]convert squad examples to features:  14%|█▎        | 1569/11590 [00:13<01:06, 151.12it/s]convert squad examples to features:  14%|█▍        | 1601/11590 [00:13<01:14, 134.37it/s]convert squad examples to features:  14%|█▍        | 1633/11590 [00:13<01:15, 131.74it/s]convert squad examples to features:  14%|█▍        | 1665/11590 [00:14<01:26, 114.29it/s]convert squad examples to features:  15%|█▍        | 1697/11590 [00:14<01:20, 122.25it/s]convert squad examples to features:  15%|█▍        | 1729/11590 [00:14<01:12, 135.76it/s]convert squad examples to features:  15%|█▌        | 1761/11590 [00:14<01:16, 128.71it/s]convert squad examples to features:  15%|█▌        | 1793/11590 [00:14<01:07, 145.07it/s]convert squad examples to features:  16%|█▌        | 1825/11590 [00:15<01:06, 147.02it/s]convert squad examples to features:  16%|█▌        | 1857/11590 [00:15<01:03, 154.28it/s]convert squad examples to features:  16%|█▋        | 1889/11590 [00:15<01:38, 98.70it/s] convert squad examples to features:  17%|█▋        | 1921/11590 [00:16<01:18, 122.42it/s]convert squad examples to features:  17%|█▋        | 1953/11590 [00:16<01:26, 110.94it/s]convert squad examples to features:  17%|█▋        | 1985/11590 [00:16<01:19, 121.28it/s]convert squad examples to features:  17%|█▋        | 2017/11590 [00:16<01:14, 128.12it/s]convert squad examples to features:  18%|█▊        | 2049/11590 [00:17<01:12, 131.60it/s]convert squad examples to features:  18%|█▊        | 2081/11590 [00:17<01:15, 126.71it/s]convert squad examples to features:  18%|█▊        | 2113/11590 [00:17<01:15, 125.09it/s]convert squad examples to features:  19%|█▊        | 2145/11590 [00:17<01:06, 141.75it/s]convert squad examples to features:  19%|█▉        | 2177/11590 [00:17<01:05, 143.62it/s]convert squad examples to features:  19%|█▉        | 2209/11590 [00:18<01:02, 150.91it/s]convert squad examples to features:  19%|█▉        | 2241/11590 [00:18<01:10, 132.68it/s]convert squad examples to features:  20%|█▉        | 2273/11590 [00:18<01:18, 119.32it/s]convert squad examples to features:  20%|█▉        | 2305/11590 [00:18<01:03, 146.13it/s]convert squad examples to features:  20%|██        | 2337/11590 [00:19<00:59, 155.85it/s]convert squad examples to features:  20%|██        | 2369/11590 [00:19<00:57, 160.67it/s]convert squad examples to features:  21%|██        | 2401/11590 [00:19<01:05, 139.83it/s]convert squad examples to features:  21%|██        | 2433/11590 [00:19<01:02, 145.73it/s]convert squad examples to features:  21%|██▏       | 2465/11590 [00:19<00:56, 160.97it/s]convert squad examples to features:  22%|██▏       | 2497/11590 [00:20<00:49, 184.94it/s]convert squad examples to features:  22%|██▏       | 2529/11590 [00:20<01:01, 147.49it/s]convert squad examples to features:  22%|██▏       | 2561/11590 [00:20<00:55, 164.07it/s]convert squad examples to features:  22%|██▏       | 2593/11590 [00:20<00:58, 152.79it/s]convert squad examples to features:  23%|██▎       | 2625/11590 [00:21<01:13, 122.21it/s]convert squad examples to features:  23%|██▎       | 2657/11590 [00:21<01:38, 90.98it/s] convert squad examples to features:  23%|██▎       | 2689/11590 [00:21<01:25, 103.90it/s]convert squad examples to features:  23%|██▎       | 2721/11590 [00:22<01:21, 108.68it/s]convert squad examples to features:  24%|██▍       | 2753/11590 [00:22<01:10, 125.41it/s]convert squad examples to features:  24%|██▍       | 2785/11590 [00:22<01:01, 143.29it/s]convert squad examples to features:  24%|██▍       | 2817/11590 [00:22<00:55, 158.72it/s]convert squad examples to features:  25%|██▍       | 2849/11590 [00:22<00:58, 150.35it/s]convert squad examples to features:  25%|██▍       | 2881/11590 [00:23<00:56, 155.34it/s]convert squad examples to features:  25%|██▌       | 2913/11590 [00:23<01:01, 141.45it/s]convert squad examples to features:  25%|██▌       | 2945/11590 [00:23<00:54, 157.96it/s]convert squad examples to features:  26%|██▌       | 2977/11590 [00:23<00:53, 161.38it/s]convert squad examples to features:  26%|██▌       | 3009/11590 [00:23<01:02, 136.94it/s]convert squad examples to features:  26%|██▌       | 3041/11590 [00:24<01:06, 129.22it/s]convert squad examples to features:  27%|██▋       | 3073/11590 [00:24<01:16, 111.62it/s]convert squad examples to features:  27%|██▋       | 3105/11590 [00:24<01:11, 118.37it/s]convert squad examples to features:  27%|██▋       | 3137/11590 [00:25<01:09, 122.45it/s]convert squad examples to features:  27%|██▋       | 3169/11590 [00:25<01:00, 138.26it/s]convert squad examples to features:  28%|██▊       | 3233/11590 [00:25<00:46, 180.67it/s]convert squad examples to features:  28%|██▊       | 3265/11590 [00:25<00:49, 168.53it/s]convert squad examples to features:  29%|██▊       | 3329/11590 [00:26<00:44, 185.16it/s]convert squad examples to features:  29%|██▉       | 3361/11590 [00:26<00:42, 194.70it/s]convert squad examples to features:  29%|██▉       | 3393/11590 [00:26<00:39, 207.56it/s]convert squad examples to features:  30%|██▉       | 3425/11590 [00:26<00:39, 206.72it/s]convert squad examples to features:  30%|██▉       | 3457/11590 [00:26<00:44, 182.99it/s]convert squad examples to features:  30%|███       | 3489/11590 [00:27<01:21, 99.70it/s] convert squad examples to features:  31%|███       | 3553/11590 [00:27<00:50, 159.12it/s]convert squad examples to features:  31%|███       | 3617/11590 [00:27<00:42, 185.99it/s]convert squad examples to features:  31%|███▏      | 3649/11590 [00:28<00:52, 152.55it/s]convert squad examples to features:  32%|███▏      | 3681/11590 [00:28<00:56, 141.04it/s]convert squad examples to features:  32%|███▏      | 3713/11590 [00:28<00:57, 137.40it/s]convert squad examples to features:  32%|███▏      | 3745/11590 [00:28<00:57, 136.98it/s]convert squad examples to features:  33%|███▎      | 3777/11590 [00:29<00:59, 131.85it/s]convert squad examples to features:  33%|███▎      | 3809/11590 [00:29<00:58, 131.91it/s]convert squad examples to features:  33%|███▎      | 3841/11590 [00:29<01:12, 107.31it/s]convert squad examples to features:  33%|███▎      | 3873/11590 [00:30<01:10, 109.46it/s]convert squad examples to features:  34%|███▎      | 3905/11590 [00:30<01:05, 118.21it/s]convert squad examples to features:  34%|███▍      | 3937/11590 [00:30<01:12, 105.89it/s]convert squad examples to features:  34%|███▍      | 3969/11590 [00:30<01:14, 102.29it/s]convert squad examples to features:  35%|███▍      | 4001/11590 [00:31<01:04, 117.23it/s]convert squad examples to features:  35%|███▍      | 4033/11590 [00:31<01:02, 120.77it/s]convert squad examples to features:  35%|███▌      | 4065/11590 [00:31<01:04, 116.20it/s]convert squad examples to features:  35%|███▌      | 4097/11590 [00:31<01:02, 119.74it/s]convert squad examples to features:  36%|███▌      | 4129/11590 [00:32<00:59, 126.07it/s]convert squad examples to features:  36%|███▌      | 4161/11590 [00:32<01:07, 109.68it/s]convert squad examples to features:  36%|███▌      | 4193/11590 [00:32<01:00, 123.05it/s]convert squad examples to features:  36%|███▋      | 4225/11590 [00:32<00:56, 131.48it/s]convert squad examples to features:  37%|███▋      | 4257/11590 [00:33<01:07, 108.80it/s]convert squad examples to features:  37%|███▋      | 4289/11590 [00:33<01:14, 98.39it/s] convert squad examples to features:  37%|███▋      | 4321/11590 [00:34<01:20, 90.65it/s]convert squad examples to features:  38%|███▊      | 4353/11590 [00:34<01:18, 92.71it/s]convert squad examples to features:  38%|███▊      | 4385/11590 [00:34<01:06, 107.78it/s]convert squad examples to features:  38%|███▊      | 4417/11590 [00:34<01:00, 118.50it/s]convert squad examples to features:  38%|███▊      | 4449/11590 [00:35<00:55, 128.37it/s]convert squad examples to features:  39%|███▊      | 4481/11590 [00:35<01:01, 115.32it/s]convert squad examples to features:  39%|███▉      | 4513/11590 [00:35<01:05, 107.47it/s]convert squad examples to features:  39%|███▉      | 4545/11590 [00:36<01:18, 89.32it/s] convert squad examples to features:  39%|███▉      | 4577/11590 [00:36<01:07, 103.17it/s]convert squad examples to features:  40%|███▉      | 4609/11590 [00:36<01:01, 113.10it/s]convert squad examples to features:  40%|████      | 4641/11590 [00:36<00:54, 127.10it/s]convert squad examples to features:  40%|████      | 4673/11590 [00:37<00:49, 140.50it/s]convert squad examples to features:  41%|████      | 4705/11590 [00:37<00:43, 157.12it/s]convert squad examples to features:  41%|████      | 4737/11590 [00:37<00:51, 133.33it/s]convert squad examples to features:  41%|████      | 4769/11590 [00:37<00:45, 148.48it/s]convert squad examples to features:  41%|████▏     | 4801/11590 [00:37<00:42, 160.43it/s]convert squad examples to features:  42%|████▏     | 4833/11590 [00:38<00:42, 157.69it/s]convert squad examples to features:  42%|████▏     | 4865/11590 [00:38<00:40, 166.47it/s]convert squad examples to features:  42%|████▏     | 4897/11590 [00:38<00:39, 169.83it/s]convert squad examples to features:  43%|████▎     | 4929/11590 [00:38<00:41, 158.67it/s]convert squad examples to features:  43%|████▎     | 4961/11590 [00:38<00:40, 164.84it/s]convert squad examples to features:  43%|████▎     | 4993/11590 [00:39<00:51, 128.25it/s]convert squad examples to features:  43%|████▎     | 5025/11590 [00:39<00:45, 144.11it/s]convert squad examples to features:  44%|████▎     | 5057/11590 [00:39<00:47, 136.95it/s]convert squad examples to features:  44%|████▍     | 5089/11590 [00:39<00:42, 151.27it/s]convert squad examples to features:  44%|████▍     | 5121/11590 [00:39<00:37, 171.49it/s]convert squad examples to features:  44%|████▍     | 5153/11590 [00:40<00:36, 178.29it/s]convert squad examples to features:  45%|████▍     | 5185/11590 [00:40<00:38, 165.58it/s]convert squad examples to features:  45%|████▌     | 5217/11590 [00:40<00:34, 184.09it/s]convert squad examples to features:  45%|████▌     | 5249/11590 [00:40<00:33, 188.37it/s]convert squad examples to features:  46%|████▌     | 5281/11590 [00:40<00:45, 139.70it/s]convert squad examples to features:  46%|████▌     | 5313/11590 [00:41<00:42, 148.18it/s]convert squad examples to features:  46%|████▌     | 5345/11590 [00:41<00:39, 157.43it/s]convert squad examples to features:  46%|████▋     | 5377/11590 [00:41<00:40, 155.30it/s]convert squad examples to features:  47%|████▋     | 5409/11590 [00:41<00:42, 146.35it/s]convert squad examples to features:  47%|████▋     | 5441/11590 [00:42<00:43, 140.51it/s]convert squad examples to features:  47%|████▋     | 5473/11590 [00:42<00:46, 131.79it/s]convert squad examples to features:  47%|████▋     | 5505/11590 [00:42<00:50, 120.39it/s]convert squad examples to features:  48%|████▊     | 5537/11590 [00:42<00:49, 122.42it/s]convert squad examples to features:  48%|████▊     | 5569/11590 [00:43<00:55, 107.68it/s]convert squad examples to features:  48%|████▊     | 5601/11590 [00:43<00:48, 123.14it/s]convert squad examples to features:  49%|████▊     | 5633/11590 [00:43<00:44, 134.76it/s]convert squad examples to features:  49%|████▉     | 5665/11590 [00:43<00:39, 150.43it/s]convert squad examples to features:  49%|████▉     | 5697/11590 [00:43<00:35, 167.68it/s]convert squad examples to features:  49%|████▉     | 5729/11590 [00:44<00:38, 152.22it/s]convert squad examples to features:  50%|████▉     | 5761/11590 [00:44<00:37, 155.07it/s]convert squad examples to features:  50%|████▉     | 5793/11590 [00:44<00:32, 180.85it/s]convert squad examples to features:  50%|█████     | 5825/11590 [00:44<00:29, 194.10it/s]convert squad examples to features:  51%|█████     | 5857/11590 [00:44<00:36, 158.57it/s]convert squad examples to features:  51%|█████     | 5889/11590 [00:45<00:32, 174.85it/s]convert squad examples to features:  51%|█████▏    | 5953/11590 [00:45<00:28, 201.02it/s]convert squad examples to features:  52%|█████▏    | 5985/11590 [00:45<00:28, 196.18it/s]convert squad examples to features:  52%|█████▏    | 6017/11590 [00:45<00:29, 189.75it/s]convert squad examples to features:  52%|█████▏    | 6049/11590 [00:45<00:27, 204.83it/s]convert squad examples to features:  52%|█████▏    | 6081/11590 [00:45<00:25, 216.08it/s]convert squad examples to features:  53%|█████▎    | 6113/11590 [00:46<00:25, 217.19it/s]convert squad examples to features:  53%|█████▎    | 6145/11590 [00:46<00:25, 215.05it/s]convert squad examples to features:  53%|█████▎    | 6177/11590 [00:46<00:26, 206.84it/s]convert squad examples to features:  54%|█████▎    | 6209/11590 [00:46<00:24, 219.11it/s]convert squad examples to features:  54%|█████▍    | 6241/11590 [00:46<00:27, 197.90it/s]convert squad examples to features:  54%|█████▍    | 6273/11590 [00:46<00:24, 219.53it/s]convert squad examples to features:  54%|█████▍    | 6305/11590 [00:46<00:22, 234.95it/s]convert squad examples to features:  55%|█████▍    | 6337/11590 [00:47<00:24, 211.27it/s]convert squad examples to features:  55%|█████▍    | 6369/11590 [00:47<00:23, 219.10it/s]convert squad examples to features:  55%|█████▌    | 6401/11590 [00:47<00:27, 191.86it/s]convert squad examples to features:  56%|█████▌    | 6433/11590 [00:47<00:30, 170.92it/s]convert squad examples to features:  56%|█████▌    | 6465/11590 [00:47<00:27, 187.95it/s]convert squad examples to features:  56%|█████▌    | 6497/11590 [00:48<00:32, 159.07it/s]convert squad examples to features:  56%|█████▋    | 6529/11590 [00:48<00:32, 157.16it/s]convert squad examples to features:  57%|█████▋    | 6561/11590 [00:48<00:33, 152.07it/s]convert squad examples to features:  57%|█████▋    | 6593/11590 [00:48<00:36, 138.02it/s]convert squad examples to features:  57%|█████▋    | 6625/11590 [00:48<00:31, 159.81it/s]convert squad examples to features:  57%|█████▋    | 6657/11590 [00:49<00:33, 146.60it/s]convert squad examples to features:  58%|█████▊    | 6689/11590 [00:49<00:32, 150.96it/s]convert squad examples to features:  58%|█████▊    | 6721/11590 [00:49<00:28, 169.46it/s]convert squad examples to features:  58%|█████▊    | 6753/11590 [00:49<00:33, 145.09it/s]convert squad examples to features:  59%|█████▊    | 6785/11590 [00:49<00:30, 159.57it/s]convert squad examples to features:  59%|█████▉    | 6817/11590 [00:50<00:27, 171.13it/s]convert squad examples to features:  59%|█████▉    | 6849/11590 [00:50<00:29, 160.01it/s]convert squad examples to features:  59%|█████▉    | 6881/11590 [00:50<00:26, 177.72it/s]convert squad examples to features:  60%|█████▉    | 6913/11590 [00:50<00:30, 153.89it/s]convert squad examples to features:  60%|█████▉    | 6945/11590 [00:50<00:25, 179.74it/s]convert squad examples to features:  60%|██████    | 6977/11590 [00:51<00:26, 171.13it/s]convert squad examples to features:  60%|██████    | 7009/11590 [00:51<00:24, 185.10it/s]convert squad examples to features:  61%|██████    | 7041/11590 [00:51<00:38, 117.26it/s]convert squad examples to features:  61%|██████    | 7073/11590 [00:51<00:34, 130.79it/s]convert squad examples to features:  61%|██████▏   | 7105/11590 [00:52<00:31, 142.43it/s]convert squad examples to features:  62%|██████▏   | 7137/11590 [00:52<00:28, 157.06it/s]convert squad examples to features:  62%|██████▏   | 7169/11590 [00:52<00:24, 177.81it/s]convert squad examples to features:  62%|██████▏   | 7201/11590 [00:52<00:24, 180.62it/s]convert squad examples to features:  62%|██████▏   | 7233/11590 [00:52<00:24, 180.20it/s]convert squad examples to features:  63%|██████▎   | 7265/11590 [00:52<00:21, 201.17it/s]convert squad examples to features:  63%|██████▎   | 7297/11590 [00:52<00:19, 225.27it/s]convert squad examples to features:  63%|██████▎   | 7329/11590 [00:53<00:19, 219.02it/s]convert squad examples to features:  64%|██████▎   | 7361/11590 [00:53<00:17, 236.89it/s]convert squad examples to features:  64%|██████▍   | 7393/11590 [00:53<00:24, 171.45it/s]convert squad examples to features:  64%|██████▍   | 7425/11590 [00:53<00:24, 171.91it/s]convert squad examples to features:  64%|██████▍   | 7457/11590 [00:53<00:23, 176.74it/s]convert squad examples to features:  65%|██████▍   | 7489/11590 [00:54<00:22, 181.32it/s]convert squad examples to features:  65%|██████▍   | 7521/11590 [00:54<00:21, 186.65it/s]convert squad examples to features:  65%|██████▌   | 7553/11590 [00:54<00:21, 192.14it/s]convert squad examples to features:  65%|██████▌   | 7585/11590 [00:54<00:21, 185.49it/s]convert squad examples to features:  66%|██████▌   | 7617/11590 [00:54<00:25, 158.54it/s]convert squad examples to features:  66%|██████▌   | 7649/11590 [00:55<00:26, 147.48it/s]convert squad examples to features:  66%|██████▋   | 7681/11590 [00:55<00:24, 158.62it/s]convert squad examples to features:  67%|██████▋   | 7713/11590 [00:55<00:21, 182.53it/s]convert squad examples to features:  67%|██████▋   | 7745/11590 [00:55<00:18, 206.54it/s]convert squad examples to features:  67%|██████▋   | 7777/11590 [00:55<00:18, 201.08it/s]convert squad examples to features:  67%|██████▋   | 7809/11590 [00:55<00:20, 185.83it/s]convert squad examples to features:  68%|██████▊   | 7841/11590 [00:56<00:22, 166.92it/s]convert squad examples to features:  68%|██████▊   | 7873/11590 [00:56<00:20, 182.79it/s]convert squad examples to features:  68%|██████▊   | 7905/11590 [00:56<00:22, 166.66it/s]convert squad examples to features:  68%|██████▊   | 7937/11590 [00:56<00:19, 187.44it/s]convert squad examples to features:  69%|██████▉   | 7969/11590 [00:56<00:23, 156.79it/s]convert squad examples to features:  69%|██████▉   | 8001/11590 [00:57<00:23, 151.18it/s]convert squad examples to features:  69%|██████▉   | 8033/11590 [00:57<00:24, 142.30it/s]convert squad examples to features:  70%|██████▉   | 8065/11590 [00:57<00:23, 147.20it/s]convert squad examples to features:  70%|██████▉   | 8097/11590 [00:57<00:25, 137.26it/s]convert squad examples to features:  70%|███████   | 8129/11590 [00:57<00:24, 139.62it/s]convert squad examples to features:  70%|███████   | 8161/11590 [00:58<00:23, 144.69it/s]convert squad examples to features:  71%|███████   | 8193/11590 [00:58<00:24, 136.09it/s]convert squad examples to features:  71%|███████   | 8225/11590 [00:58<00:24, 135.75it/s]convert squad examples to features:  71%|███████   | 8257/11590 [00:58<00:22, 146.85it/s]convert squad examples to features:  72%|███████▏  | 8289/11590 [00:59<00:26, 125.47it/s]convert squad examples to features:  72%|███████▏  | 8321/11590 [00:59<00:24, 132.41it/s]convert squad examples to features:  72%|███████▏  | 8353/11590 [00:59<00:24, 131.95it/s]convert squad examples to features:  72%|███████▏  | 8385/11590 [00:59<00:22, 140.49it/s]convert squad examples to features:  73%|███████▎  | 8417/11590 [01:00<00:21, 145.65it/s]convert squad examples to features:  73%|███████▎  | 8449/11590 [01:00<00:20, 152.29it/s]convert squad examples to features:  73%|███████▎  | 8481/11590 [01:00<00:19, 161.38it/s]convert squad examples to features:  73%|███████▎  | 8513/11590 [01:00<00:16, 181.58it/s]convert squad examples to features:  74%|███████▎  | 8545/11590 [01:00<00:16, 179.99it/s]convert squad examples to features:  74%|███████▍  | 8577/11590 [01:01<00:23, 126.93it/s]convert squad examples to features:  74%|███████▍  | 8609/11590 [01:01<00:30, 96.87it/s] convert squad examples to features:  75%|███████▍  | 8641/11590 [01:01<00:25, 116.87it/s]convert squad examples to features:  75%|███████▍  | 8673/11590 [01:01<00:21, 132.60it/s]convert squad examples to features:  75%|███████▌  | 8737/11590 [01:02<00:17, 163.96it/s]convert squad examples to features:  76%|███████▌  | 8769/11590 [01:02<00:18, 154.38it/s]convert squad examples to features:  76%|███████▌  | 8801/11590 [01:02<00:18, 150.07it/s]convert squad examples to features:  76%|███████▌  | 8833/11590 [01:02<00:17, 154.25it/s]convert squad examples to features:  76%|███████▋  | 8865/11590 [01:03<00:20, 135.04it/s]convert squad examples to features:  77%|███████▋  | 8897/11590 [01:03<00:18, 142.36it/s]convert squad examples to features:  77%|███████▋  | 8929/11590 [01:03<00:20, 127.51it/s]convert squad examples to features:  78%|███████▊  | 8993/11590 [01:04<00:16, 153.27it/s]convert squad examples to features:  78%|███████▊  | 9025/11590 [01:04<00:16, 159.82it/s]convert squad examples to features:  78%|███████▊  | 9057/11590 [01:04<00:15, 162.78it/s]convert squad examples to features:  78%|███████▊  | 9089/11590 [01:04<00:18, 137.77it/s]convert squad examples to features:  79%|███████▊  | 9121/11590 [01:04<00:15, 159.26it/s]convert squad examples to features:  79%|███████▉  | 9153/11590 [01:05<00:16, 148.73it/s]convert squad examples to features:  79%|███████▉  | 9185/11590 [01:05<00:15, 152.40it/s]convert squad examples to features:  80%|███████▉  | 9217/11590 [01:05<00:16, 147.49it/s]convert squad examples to features:  80%|███████▉  | 9249/11590 [01:05<00:15, 154.28it/s]convert squad examples to features:  80%|████████  | 9281/11590 [01:05<00:14, 155.30it/s]convert squad examples to features:  80%|████████  | 9313/11590 [01:06<00:22, 100.02it/s]convert squad examples to features:  81%|████████  | 9345/11590 [01:06<00:22, 99.83it/s] convert squad examples to features:  81%|████████  | 9377/11590 [01:07<00:19, 111.25it/s]convert squad examples to features:  81%|████████  | 9409/11590 [01:07<00:16, 130.65it/s]convert squad examples to features:  82%|████████▏ | 9473/11590 [01:07<00:11, 176.52it/s]convert squad examples to features:  82%|████████▏ | 9537/11590 [01:07<00:10, 198.49it/s]convert squad examples to features:  83%|████████▎ | 9569/11590 [01:07<00:09, 203.05it/s]convert squad examples to features:  83%|████████▎ | 9601/11590 [01:07<00:10, 194.87it/s]convert squad examples to features:  83%|████████▎ | 9665/11590 [01:08<00:09, 212.99it/s]convert squad examples to features:  84%|████████▎ | 9697/11590 [01:08<00:09, 208.17it/s]convert squad examples to features:  84%|████████▍ | 9729/11590 [01:08<00:09, 196.38it/s]convert squad examples to features:  84%|████████▍ | 9761/11590 [01:08<00:08, 205.65it/s]convert squad examples to features:  84%|████████▍ | 9793/11590 [01:08<00:08, 200.42it/s]convert squad examples to features:  85%|████████▍ | 9825/11590 [01:09<00:10, 171.13it/s]convert squad examples to features:  85%|████████▌ | 9857/11590 [01:09<00:10, 161.94it/s]convert squad examples to features:  85%|████████▌ | 9889/11590 [01:09<00:11, 147.85it/s]convert squad examples to features:  86%|████████▌ | 9921/11590 [01:09<00:10, 163.38it/s]convert squad examples to features:  86%|████████▌ | 9953/11590 [01:09<00:09, 173.25it/s]convert squad examples to features:  86%|████████▌ | 9985/11590 [01:10<00:09, 175.69it/s]convert squad examples to features:  86%|████████▋ | 10017/11590 [01:10<00:08, 180.26it/s]convert squad examples to features:  87%|████████▋ | 10049/11590 [01:10<00:08, 179.11it/s]convert squad examples to features:  87%|████████▋ | 10081/11590 [01:10<00:07, 199.58it/s]convert squad examples to features:  87%|████████▋ | 10113/11590 [01:10<00:08, 177.84it/s]convert squad examples to features:  88%|████████▊ | 10145/11590 [01:10<00:07, 193.50it/s]convert squad examples to features:  88%|████████▊ | 10177/11590 [01:11<00:06, 206.88it/s]convert squad examples to features:  88%|████████▊ | 10209/11590 [01:11<00:09, 141.37it/s]convert squad examples to features:  88%|████████▊ | 10241/11590 [01:11<00:09, 136.16it/s]convert squad examples to features:  89%|████████▊ | 10273/11590 [01:12<00:11, 111.46it/s]convert squad examples to features:  89%|████████▉ | 10305/11590 [01:12<00:10, 117.42it/s]convert squad examples to features:  89%|████████▉ | 10337/11590 [01:12<00:10, 115.58it/s]convert squad examples to features:  89%|████████▉ | 10369/11590 [01:12<00:09, 129.68it/s]convert squad examples to features:  90%|████████▉ | 10401/11590 [01:13<00:09, 127.10it/s]convert squad examples to features:  90%|█████████ | 10433/11590 [01:13<00:09, 123.66it/s]convert squad examples to features:  90%|█████████ | 10465/11590 [01:13<00:08, 139.30it/s]convert squad examples to features:  91%|█████████ | 10497/11590 [01:13<00:07, 154.29it/s]convert squad examples to features:  91%|█████████ | 10529/11590 [01:14<00:16, 66.13it/s] convert squad examples to features:  91%|█████████▏| 10593/11590 [01:15<00:09, 106.69it/s]convert squad examples to features:  92%|█████████▏| 10625/11590 [01:15<00:08, 114.34it/s]convert squad examples to features:  92%|█████████▏| 10657/11590 [01:15<00:08, 110.80it/s]convert squad examples to features:  92%|█████████▏| 10689/11590 [01:15<00:06, 131.08it/s]convert squad examples to features:  93%|█████████▎| 10721/11590 [01:15<00:07, 122.03it/s]convert squad examples to features:  93%|█████████▎| 10753/11590 [01:16<00:06, 126.52it/s]convert squad examples to features:  93%|█████████▎| 10785/11590 [01:16<00:06, 126.08it/s]convert squad examples to features:  93%|█████████▎| 10817/11590 [01:16<00:06, 126.52it/s]convert squad examples to features:  94%|█████████▎| 10849/11590 [01:16<00:05, 132.02it/s]convert squad examples to features:  94%|█████████▍| 10881/11590 [01:17<00:04, 145.83it/s]convert squad examples to features:  94%|█████████▍| 10913/11590 [01:17<00:03, 170.18it/s]convert squad examples to features:  94%|█████████▍| 10945/11590 [01:17<00:03, 183.10it/s]convert squad examples to features:  95%|█████████▍| 10977/11590 [01:17<00:03, 195.39it/s]convert squad examples to features:  95%|█████████▍| 11009/11590 [01:17<00:02, 207.46it/s]convert squad examples to features:  95%|█████████▌| 11041/11590 [01:17<00:02, 184.63it/s]convert squad examples to features:  96%|█████████▌| 11073/11590 [01:17<00:02, 201.54it/s]convert squad examples to features:  96%|█████████▌| 11137/11590 [01:18<00:02, 217.93it/s]convert squad examples to features:  96%|█████████▋| 11169/11590 [01:18<00:02, 198.31it/s]convert squad examples to features:  97%|█████████▋| 11201/11590 [01:18<00:01, 204.95it/s]convert squad examples to features:  97%|█████████▋| 11233/11590 [01:18<00:01, 201.31it/s]convert squad examples to features:  97%|█████████▋| 11297/11590 [01:19<00:01, 180.66it/s]convert squad examples to features:  98%|█████████▊| 11329/11590 [01:19<00:01, 196.81it/s]convert squad examples to features:  98%|█████████▊| 11361/11590 [01:19<00:01, 202.60it/s]convert squad examples to features:  98%|█████████▊| 11393/11590 [01:19<00:00, 207.13it/s]convert squad examples to features:  99%|█████████▊| 11425/11590 [01:19<00:00, 206.12it/s]convert squad examples to features:  99%|█████████▉| 11457/11590 [01:19<00:00, 210.89it/s]convert squad examples to features:  99%|█████████▉| 11489/11590 [01:20<00:00, 197.53it/s]convert squad examples to features:  99%|█████████▉| 11521/11590 [01:20<00:00, 194.17it/s]convert squad examples to features: 100%|█████████▉| 11553/11590 [01:20<00:00, 196.50it/s]convert squad examples to features: 100%|██████████| 11590/11590 [01:20<00:00, 144.17it/s]
add example index and unique id:   0%|          | 0/11590 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 11590/11590 [00:00<00:00, 518743.62it/s]
05/09/2022 23:48:54 - INFO - __main__ -   Saving features into cached file ./cached_test-context-en-question-en.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_en
05/09/2022 23:49:12 - INFO - __main__ -   ***** Running evaluation  *****
05/09/2022 23:49:12 - INFO - __main__ -     Num examples = 17054
05/09/2022 23:49:12 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/2132 [00:00<?, ?it/s]Evaluating:   0%|          | 1/2132 [00:00<30:32,  1.16it/s]Evaluating:   0%|          | 2/2132 [00:01<19:58,  1.78it/s]Evaluating:   0%|          | 3/2132 [00:01<16:32,  2.14it/s]Evaluating:   0%|          | 4/2132 [00:01<15:00,  2.36it/s]Evaluating:   0%|          | 5/2132 [00:02<14:08,  2.51it/s]Evaluating:   0%|          | 6/2132 [00:02<13:37,  2.60it/s]Evaluating:   0%|          | 7/2132 [00:02<13:18,  2.66it/s]Evaluating:   0%|          | 8/2132 [00:03<13:04,  2.71it/s]Evaluating:   0%|          | 9/2132 [00:03<12:56,  2.74it/s]Evaluating:   0%|          | 10/2132 [00:04<12:51,  2.75it/s]Evaluating:   1%|          | 11/2132 [00:04<12:47,  2.76it/s]Evaluating:   1%|          | 12/2132 [00:04<12:45,  2.77it/s]Evaluating:   1%|          | 13/2132 [00:05<12:39,  2.79it/s]Evaluating:   1%|          | 14/2132 [00:05<12:35,  2.80it/s]Evaluating:   1%|          | 15/2132 [00:05<12:36,  2.80it/s]Evaluating:   1%|          | 16/2132 [00:06<12:35,  2.80it/s]Evaluating:   1%|          | 17/2132 [00:06<12:35,  2.80it/s]Evaluating:   1%|          | 18/2132 [00:06<12:36,  2.79it/s]Evaluating:   1%|          | 19/2132 [00:07<12:37,  2.79it/s]Evaluating:   1%|          | 20/2132 [00:07<12:39,  2.78it/s]Evaluating:   1%|          | 21/2132 [00:08<12:42,  2.77it/s]Evaluating:   1%|          | 22/2132 [00:08<12:45,  2.76it/s]Evaluating:   1%|          | 23/2132 [00:08<12:46,  2.75it/s]Evaluating:   1%|          | 24/2132 [00:09<12:46,  2.75it/s]Evaluating:   1%|          | 25/2132 [00:09<12:45,  2.75it/s]Evaluating:   1%|          | 26/2132 [00:09<12:41,  2.77it/s]Evaluating:   1%|▏         | 27/2132 [00:10<12:39,  2.77it/s]Evaluating:   1%|▏         | 28/2132 [00:10<12:41,  2.76it/s]Evaluating:   1%|▏         | 29/2132 [00:10<12:36,  2.78it/s]Evaluating:   1%|▏         | 30/2132 [00:11<12:36,  2.78it/s]Evaluating:   1%|▏         | 31/2132 [00:11<12:36,  2.78it/s]Evaluating:   2%|▏         | 32/2132 [00:11<12:36,  2.77it/s]Evaluating:   2%|▏         | 33/2132 [00:12<12:35,  2.78it/s]Evaluating:   2%|▏         | 34/2132 [00:12<12:37,  2.77it/s]Evaluating:   2%|▏         | 35/2132 [00:13<12:34,  2.78it/s]Evaluating:   2%|▏         | 36/2132 [00:13<12:33,  2.78it/s]Evaluating:   2%|▏         | 37/2132 [00:13<12:32,  2.78it/s]Evaluating:   2%|▏         | 38/2132 [00:14<12:37,  2.76it/s]Evaluating:   2%|▏         | 39/2132 [00:14<12:40,  2.75it/s]Evaluating:   2%|▏         | 40/2132 [00:14<13:03,  2.67it/s]Evaluating:   2%|▏         | 41/2132 [00:15<12:52,  2.71it/s]Evaluating:   2%|▏         | 42/2132 [00:15<12:48,  2.72it/s]Evaluating:   2%|▏         | 43/2132 [00:15<12:45,  2.73it/s]Evaluating:   2%|▏         | 44/2132 [00:16<12:40,  2.74it/s]Evaluating:   2%|▏         | 45/2132 [00:16<12:37,  2.75it/s]Evaluating:   2%|▏         | 46/2132 [00:17<12:37,  2.75it/s]Evaluating:   2%|▏         | 47/2132 [00:17<12:38,  2.75it/s]Evaluating:   2%|▏         | 48/2132 [00:17<12:34,  2.76it/s]Evaluating:   2%|▏         | 49/2132 [00:18<12:34,  2.76it/s]Evaluating:   2%|▏         | 50/2132 [00:18<12:36,  2.75it/s]Evaluating:   2%|▏         | 51/2132 [00:18<12:35,  2.76it/s]Evaluating:   2%|▏         | 52/2132 [00:19<12:36,  2.75it/s]Evaluating:   2%|▏         | 53/2132 [00:19<12:38,  2.74it/s]Evaluating:   3%|▎         | 54/2132 [00:19<12:33,  2.76it/s]Evaluating:   3%|▎         | 55/2132 [00:20<12:36,  2.75it/s]Evaluating:   3%|▎         | 56/2132 [00:20<12:33,  2.76it/s]Evaluating:   3%|▎         | 57/2132 [00:21<12:30,  2.76it/s]Evaluating:   3%|▎         | 58/2132 [00:21<12:32,  2.76it/s]Evaluating:   3%|▎         | 59/2132 [00:21<12:32,  2.76it/s]Evaluating:   3%|▎         | 60/2132 [00:22<12:35,  2.74it/s]Evaluating:   3%|▎         | 61/2132 [00:22<12:35,  2.74it/s]Evaluating:   3%|▎         | 62/2132 [00:22<12:29,  2.76it/s]Evaluating:   3%|▎         | 63/2132 [00:23<12:30,  2.76it/s]Evaluating:   3%|▎         | 64/2132 [00:23<12:35,  2.74it/s]Evaluating:   3%|▎         | 65/2132 [00:23<12:34,  2.74it/s]Evaluating:   3%|▎         | 66/2132 [00:24<12:31,  2.75it/s]Evaluating:   3%|▎         | 67/2132 [00:24<12:26,  2.77it/s]Evaluating:   3%|▎         | 68/2132 [00:25<12:22,  2.78it/s]Evaluating:   3%|▎         | 69/2132 [00:25<12:22,  2.78it/s]Evaluating:   3%|▎         | 70/2132 [00:25<12:26,  2.76it/s]Evaluating:   3%|▎         | 71/2132 [00:26<12:26,  2.76it/s]Evaluating:   3%|▎         | 72/2132 [00:26<12:28,  2.75it/s]Evaluating:   3%|▎         | 73/2132 [00:26<12:27,  2.75it/s]Evaluating:   3%|▎         | 74/2132 [00:27<12:26,  2.76it/s]Evaluating:   4%|▎         | 75/2132 [00:27<12:26,  2.76it/s]Evaluating:   4%|▎         | 76/2132 [00:27<12:28,  2.75it/s]Evaluating:   4%|▎         | 77/2132 [00:28<12:29,  2.74it/s]Evaluating:   4%|▎         | 78/2132 [00:28<12:26,  2.75it/s]Evaluating:   4%|▎         | 79/2132 [00:29<12:25,  2.75it/s]Evaluating:   4%|▍         | 80/2132 [00:29<12:24,  2.76it/s]Evaluating:   4%|▍         | 81/2132 [00:29<12:26,  2.75it/s]Evaluating:   4%|▍         | 82/2132 [00:30<12:23,  2.76it/s]Evaluating:   4%|▍         | 83/2132 [00:30<12:23,  2.75it/s]Evaluating:   4%|▍         | 84/2132 [00:30<12:22,  2.76it/s]Evaluating:   4%|▍         | 85/2132 [00:31<12:22,  2.76it/s]Evaluating:   4%|▍         | 86/2132 [00:31<12:20,  2.76it/s]Evaluating:   4%|▍         | 87/2132 [00:31<12:17,  2.77it/s]Evaluating:   4%|▍         | 88/2132 [00:32<12:16,  2.77it/s]Evaluating:   4%|▍         | 89/2132 [00:32<12:18,  2.77it/s]Evaluating:   4%|▍         | 90/2132 [00:33<12:16,  2.77it/s]Evaluating:   4%|▍         | 91/2132 [00:33<12:15,  2.77it/s]Evaluating:   4%|▍         | 92/2132 [00:33<12:14,  2.78it/s]Evaluating:   4%|▍         | 93/2132 [00:34<12:14,  2.78it/s]Evaluating:   4%|▍         | 94/2132 [00:34<12:13,  2.78it/s]Evaluating:   4%|▍         | 95/2132 [00:34<12:13,  2.78it/s]Evaluating:   5%|▍         | 96/2132 [00:35<12:18,  2.76it/s]Evaluating:   5%|▍         | 97/2132 [00:35<12:17,  2.76it/s]Evaluating:   5%|▍         | 98/2132 [00:35<12:18,  2.75it/s]Evaluating:   5%|▍         | 99/2132 [00:36<12:18,  2.75it/s]Evaluating:   5%|▍         | 100/2132 [00:36<12:20,  2.74it/s]Evaluating:   5%|▍         | 101/2132 [00:37<12:20,  2.74it/s]Evaluating:   5%|▍         | 102/2132 [00:37<12:18,  2.75it/s]Evaluating:   5%|▍         | 103/2132 [00:37<12:19,  2.74it/s]Evaluating:   5%|▍         | 104/2132 [00:38<12:18,  2.75it/s]Evaluating:   5%|▍         | 105/2132 [00:38<12:16,  2.75it/s]Evaluating:   5%|▍         | 106/2132 [00:38<12:16,  2.75it/s]Evaluating:   5%|▌         | 107/2132 [00:39<12:14,  2.76it/s]Evaluating:   5%|▌         | 108/2132 [00:39<12:12,  2.76it/s]Evaluating:   5%|▌         | 109/2132 [00:39<12:10,  2.77it/s]Evaluating:   5%|▌         | 110/2132 [00:40<12:11,  2.77it/s]Evaluating:   5%|▌         | 111/2132 [00:40<12:09,  2.77it/s]Evaluating:   5%|▌         | 112/2132 [00:41<12:08,  2.77it/s]Evaluating:   5%|▌         | 113/2132 [00:41<12:10,  2.76it/s]Evaluating:   5%|▌         | 114/2132 [00:41<12:12,  2.75it/s]Evaluating:   5%|▌         | 115/2132 [00:42<12:15,  2.74it/s]Evaluating:   5%|▌         | 116/2132 [00:42<12:12,  2.75it/s]Evaluating:   5%|▌         | 117/2132 [00:42<12:09,  2.76it/s]Evaluating:   6%|▌         | 118/2132 [00:43<12:09,  2.76it/s]Evaluating:   6%|▌         | 119/2132 [00:43<12:07,  2.77it/s]Evaluating:   6%|▌         | 120/2132 [00:43<12:06,  2.77it/s]Evaluating:   6%|▌         | 121/2132 [00:44<12:06,  2.77it/s]Evaluating:   6%|▌         | 122/2132 [00:44<12:03,  2.78it/s]Evaluating:   6%|▌         | 123/2132 [00:44<12:05,  2.77it/s]Evaluating:   6%|▌         | 124/2132 [00:45<12:05,  2.77it/s]Evaluating:   6%|▌         | 125/2132 [00:45<12:07,  2.76it/s]Evaluating:   6%|▌         | 126/2132 [00:46<12:11,  2.74it/s]Evaluating:   6%|▌         | 127/2132 [00:46<12:10,  2.75it/s]Evaluating:   6%|▌         | 128/2132 [00:46<12:08,  2.75it/s]Evaluating:   6%|▌         | 129/2132 [00:47<12:04,  2.77it/s]Evaluating:   6%|▌         | 130/2132 [00:47<12:03,  2.77it/s]Evaluating:   6%|▌         | 131/2132 [00:47<12:04,  2.76it/s]Evaluating:   6%|▌         | 132/2132 [00:48<12:04,  2.76it/s]Evaluating:   6%|▌         | 133/2132 [00:48<12:02,  2.77it/s]Evaluating:   6%|▋         | 134/2132 [00:48<12:05,  2.75it/s]Evaluating:   6%|▋         | 135/2132 [00:49<12:04,  2.76it/s]Evaluating:   6%|▋         | 136/2132 [00:49<12:06,  2.75it/s]Evaluating:   6%|▋         | 137/2132 [00:50<12:08,  2.74it/s]Evaluating:   6%|▋         | 138/2132 [00:50<12:09,  2.73it/s]Evaluating:   7%|▋         | 139/2132 [00:50<12:10,  2.73it/s]Evaluating:   7%|▋         | 140/2132 [00:51<12:10,  2.73it/s]Evaluating:   7%|▋         | 141/2132 [00:51<12:11,  2.72it/s]Evaluating:   7%|▋         | 142/2132 [00:51<12:13,  2.71it/s]Evaluating:   7%|▋         | 143/2132 [00:52<12:14,  2.71it/s]Evaluating:   7%|▋         | 144/2132 [00:52<12:12,  2.71it/s]Evaluating:   7%|▋         | 145/2132 [00:53<12:08,  2.73it/s]Evaluating:   7%|▋         | 146/2132 [00:53<12:07,  2.73it/s]Evaluating:   7%|▋         | 147/2132 [00:53<12:03,  2.74it/s]Evaluating:   7%|▋         | 148/2132 [00:54<12:01,  2.75it/s]Evaluating:   7%|▋         | 149/2132 [00:54<12:04,  2.74it/s]Evaluating:   7%|▋         | 150/2132 [00:54<12:04,  2.74it/s]Evaluating:   7%|▋         | 151/2132 [00:55<12:06,  2.73it/s]Evaluating:   7%|▋         | 152/2132 [00:55<12:03,  2.74it/s]Evaluating:   7%|▋         | 153/2132 [00:55<12:01,  2.74it/s]Evaluating:   7%|▋         | 154/2132 [00:56<11:59,  2.75it/s]Evaluating:   7%|▋         | 155/2132 [00:56<12:00,  2.74it/s]Evaluating:   7%|▋         | 156/2132 [00:57<11:59,  2.75it/s]Evaluating:   7%|▋         | 157/2132 [00:57<11:58,  2.75it/s]Evaluating:   7%|▋         | 158/2132 [00:57<11:58,  2.75it/s]Evaluating:   7%|▋         | 159/2132 [00:58<11:56,  2.75it/s]Evaluating:   8%|▊         | 160/2132 [00:58<11:58,  2.74it/s]Evaluating:   8%|▊         | 161/2132 [00:58<11:58,  2.74it/s]Evaluating:   8%|▊         | 162/2132 [00:59<12:00,  2.73it/s]Evaluating:   8%|▊         | 163/2132 [00:59<11:58,  2.74it/s]Evaluating:   8%|▊         | 164/2132 [00:59<11:57,  2.74it/s]Evaluating:   8%|▊         | 165/2132 [01:00<11:58,  2.74it/s]Evaluating:   8%|▊         | 166/2132 [01:00<11:56,  2.75it/s]Evaluating:   8%|▊         | 167/2132 [01:01<11:55,  2.75it/s]Evaluating:   8%|▊         | 168/2132 [01:01<11:55,  2.74it/s]Evaluating:   8%|▊         | 169/2132 [01:01<11:56,  2.74it/s]Evaluating:   8%|▊         | 170/2132 [01:02<11:54,  2.74it/s]Evaluating:   8%|▊         | 171/2132 [01:02<11:55,  2.74it/s]Evaluating:   8%|▊         | 172/2132 [01:02<11:55,  2.74it/s]Evaluating:   8%|▊         | 173/2132 [01:03<11:52,  2.75it/s]Evaluating:   8%|▊         | 174/2132 [01:03<11:50,  2.76it/s]Evaluating:   8%|▊         | 175/2132 [01:03<11:50,  2.76it/s]Evaluating:   8%|▊         | 176/2132 [01:04<11:53,  2.74it/s]Evaluating:   8%|▊         | 177/2132 [01:04<11:52,  2.74it/s]Evaluating:   8%|▊         | 178/2132 [01:05<11:52,  2.74it/s]Evaluating:   8%|▊         | 179/2132 [01:05<11:51,  2.75it/s]Evaluating:   8%|▊         | 180/2132 [01:05<11:48,  2.76it/s]Evaluating:   8%|▊         | 181/2132 [01:06<11:45,  2.76it/s]Evaluating:   9%|▊         | 182/2132 [01:06<11:47,  2.76it/s]Evaluating:   9%|▊         | 183/2132 [01:06<11:46,  2.76it/s]Evaluating:   9%|▊         | 184/2132 [01:07<11:46,  2.76it/s]Evaluating:   9%|▊         | 185/2132 [01:07<11:44,  2.76it/s]Evaluating:   9%|▊         | 186/2132 [01:07<11:43,  2.76it/s]Evaluating:   9%|▉         | 187/2132 [01:08<11:45,  2.76it/s]Evaluating:   9%|▉         | 188/2132 [01:08<11:44,  2.76it/s]Evaluating:   9%|▉         | 189/2132 [01:09<11:44,  2.76it/s]Evaluating:   9%|▉         | 190/2132 [01:09<11:43,  2.76it/s]Evaluating:   9%|▉         | 191/2132 [01:09<11:42,  2.76it/s]Evaluating:   9%|▉         | 192/2132 [01:10<11:44,  2.75it/s]Evaluating:   9%|▉         | 193/2132 [01:10<11:47,  2.74it/s]Evaluating:   9%|▉         | 194/2132 [01:10<11:48,  2.74it/s]Evaluating:   9%|▉         | 195/2132 [01:11<11:46,  2.74it/s]Evaluating:   9%|▉         | 196/2132 [01:11<11:46,  2.74it/s]Evaluating:   9%|▉         | 197/2132 [01:11<11:44,  2.74it/s]Evaluating:   9%|▉         | 198/2132 [01:12<11:42,  2.75it/s]Evaluating:   9%|▉         | 199/2132 [01:12<11:44,  2.75it/s]Evaluating:   9%|▉         | 200/2132 [01:13<11:43,  2.74it/s]Evaluating:   9%|▉         | 201/2132 [01:13<11:47,  2.73it/s]Evaluating:   9%|▉         | 202/2132 [01:13<11:47,  2.73it/s]Evaluating:  10%|▉         | 203/2132 [01:14<11:46,  2.73it/s]Evaluating:  10%|▉         | 204/2132 [01:14<11:45,  2.73it/s]Evaluating:  10%|▉         | 205/2132 [01:14<11:44,  2.73it/s]Evaluating:  10%|▉         | 206/2132 [01:15<11:43,  2.74it/s]Evaluating:  10%|▉         | 207/2132 [01:15<11:46,  2.73it/s]Evaluating:  10%|▉         | 208/2132 [01:15<11:45,  2.73it/s]Evaluating:  10%|▉         | 209/2132 [01:16<11:46,  2.72it/s]Evaluating:  10%|▉         | 210/2132 [01:16<11:46,  2.72it/s]Evaluating:  10%|▉         | 211/2132 [01:17<11:45,  2.72it/s]Evaluating:  10%|▉         | 212/2132 [01:17<11:45,  2.72it/s]Evaluating:  10%|▉         | 213/2132 [01:17<11:44,  2.72it/s]Evaluating:  10%|█         | 214/2132 [01:18<11:42,  2.73it/s]Evaluating:  10%|█         | 215/2132 [01:18<11:42,  2.73it/s]Evaluating:  10%|█         | 216/2132 [01:18<11:40,  2.73it/s]Evaluating:  10%|█         | 217/2132 [01:19<11:40,  2.74it/s]Evaluating:  10%|█         | 218/2132 [01:19<11:41,  2.73it/s]Evaluating:  10%|█         | 219/2132 [01:20<11:38,  2.74it/s]Evaluating:  10%|█         | 220/2132 [01:20<11:52,  2.68it/s]Evaluating:  10%|█         | 221/2132 [01:20<11:47,  2.70it/s]Evaluating:  10%|█         | 222/2132 [01:21<11:43,  2.71it/s]Evaluating:  10%|█         | 223/2132 [01:21<11:40,  2.72it/s]Evaluating:  11%|█         | 224/2132 [01:21<11:38,  2.73it/s]Evaluating:  11%|█         | 225/2132 [01:22<11:36,  2.74it/s]Evaluating:  11%|█         | 226/2132 [01:22<11:37,  2.73it/s]Evaluating:  11%|█         | 227/2132 [01:22<11:37,  2.73it/s]Evaluating:  11%|█         | 228/2132 [01:23<11:39,  2.72it/s]Evaluating:  11%|█         | 229/2132 [01:23<11:40,  2.72it/s]Evaluating:  11%|█         | 230/2132 [01:24<11:38,  2.72it/s]Evaluating:  11%|█         | 231/2132 [01:24<11:38,  2.72it/s]Evaluating:  11%|█         | 232/2132 [01:24<11:37,  2.73it/s]Evaluating:  11%|█         | 233/2132 [01:25<11:38,  2.72it/s]Evaluating:  11%|█         | 234/2132 [01:25<11:42,  2.70it/s]Evaluating:  11%|█         | 235/2132 [01:25<11:40,  2.71it/s]Evaluating:  11%|█         | 236/2132 [01:26<11:36,  2.72it/s]Evaluating:  11%|█         | 237/2132 [01:26<11:33,  2.73it/s]Evaluating:  11%|█         | 238/2132 [01:26<11:32,  2.73it/s]Evaluating:  11%|█         | 239/2132 [01:27<11:31,  2.74it/s]Evaluating:  11%|█▏        | 240/2132 [01:27<11:29,  2.75it/s]Evaluating:  11%|█▏        | 241/2132 [01:28<11:29,  2.74it/s]Evaluating:  11%|█▏        | 242/2132 [01:28<11:31,  2.73it/s]Evaluating:  11%|█▏        | 243/2132 [01:28<11:33,  2.72it/s]Evaluating:  11%|█▏        | 244/2132 [01:29<11:32,  2.73it/s]Evaluating:  11%|█▏        | 245/2132 [01:29<11:29,  2.74it/s]Evaluating:  12%|█▏        | 246/2132 [01:29<11:28,  2.74it/s]Evaluating:  12%|█▏        | 247/2132 [01:30<11:26,  2.74it/s]Evaluating:  12%|█▏        | 248/2132 [01:30<11:26,  2.75it/s]Evaluating:  12%|█▏        | 249/2132 [01:31<11:28,  2.74it/s]Evaluating:  12%|█▏        | 250/2132 [01:31<11:27,  2.74it/s]Evaluating:  12%|█▏        | 251/2132 [01:31<11:29,  2.73it/s]Evaluating:  12%|█▏        | 252/2132 [01:32<11:29,  2.73it/s]Evaluating:  12%|█▏        | 253/2132 [01:32<11:29,  2.72it/s]Evaluating:  12%|█▏        | 254/2132 [01:32<11:28,  2.73it/s]Evaluating:  12%|█▏        | 255/2132 [01:33<11:30,  2.72it/s]Evaluating:  12%|█▏        | 256/2132 [01:33<11:28,  2.72it/s]Evaluating:  12%|█▏        | 257/2132 [01:33<11:32,  2.71it/s]Evaluating:  12%|█▏        | 258/2132 [01:34<11:33,  2.70it/s]Evaluating:  12%|█▏        | 259/2132 [01:34<11:32,  2.70it/s]Evaluating:  12%|█▏        | 260/2132 [01:35<11:33,  2.70it/s]Evaluating:  12%|█▏        | 261/2132 [01:35<11:30,  2.71it/s]Evaluating:  12%|█▏        | 262/2132 [01:35<11:28,  2.72it/s]Evaluating:  12%|█▏        | 263/2132 [01:36<11:29,  2.71it/s]Evaluating:  12%|█▏        | 264/2132 [01:36<11:31,  2.70it/s]Evaluating:  12%|█▏        | 265/2132 [01:36<11:32,  2.69it/s]Evaluating:  12%|█▏        | 266/2132 [01:37<11:30,  2.70it/s]Evaluating:  13%|█▎        | 267/2132 [01:37<11:28,  2.71it/s]Evaluating:  13%|█▎        | 268/2132 [01:38<11:24,  2.72it/s]Evaluating:  13%|█▎        | 269/2132 [01:38<11:22,  2.73it/s]Evaluating:  13%|█▎        | 270/2132 [01:38<11:19,  2.74it/s]Evaluating:  13%|█▎        | 271/2132 [01:39<11:19,  2.74it/s]Evaluating:  13%|█▎        | 272/2132 [01:39<11:18,  2.74it/s]Evaluating:  13%|█▎        | 273/2132 [01:39<11:16,  2.75it/s]Evaluating:  13%|█▎        | 274/2132 [01:40<11:18,  2.74it/s]Evaluating:  13%|█▎        | 275/2132 [01:40<11:19,  2.73it/s]Evaluating:  13%|█▎        | 276/2132 [01:40<11:18,  2.73it/s]Evaluating:  13%|█▎        | 277/2132 [01:41<11:18,  2.73it/s]Evaluating:  13%|█▎        | 278/2132 [01:41<11:16,  2.74it/s]Evaluating:  13%|█▎        | 279/2132 [01:42<11:13,  2.75it/s]Evaluating:  13%|█▎        | 280/2132 [01:42<11:13,  2.75it/s]Evaluating:  13%|█▎        | 281/2132 [01:42<11:14,  2.75it/s]Evaluating:  13%|█▎        | 282/2132 [01:43<11:14,  2.74it/s]Evaluating:  13%|█▎        | 283/2132 [01:43<11:15,  2.74it/s]Evaluating:  13%|█▎        | 284/2132 [01:43<11:15,  2.74it/s]Evaluating:  13%|█▎        | 285/2132 [01:44<11:15,  2.74it/s]Evaluating:  13%|█▎        | 286/2132 [01:44<11:13,  2.74it/s]Evaluating:  13%|█▎        | 287/2132 [01:44<11:15,  2.73it/s]Evaluating:  14%|█▎        | 288/2132 [01:45<11:16,  2.73it/s]Evaluating:  14%|█▎        | 289/2132 [01:45<11:16,  2.73it/s]Evaluating:  14%|█▎        | 290/2132 [01:46<11:16,  2.72it/s]Evaluating:  14%|█▎        | 291/2132 [01:46<11:17,  2.72it/s]Evaluating:  14%|█▎        | 292/2132 [01:46<11:14,  2.73it/s]Evaluating:  14%|█▎        | 293/2132 [01:47<11:11,  2.74it/s]Evaluating:  14%|█▍        | 294/2132 [01:47<11:09,  2.74it/s]Evaluating:  14%|█▍        | 295/2132 [01:47<11:09,  2.74it/s]Evaluating:  14%|█▍        | 296/2132 [01:48<11:11,  2.73it/s]Evaluating:  14%|█▍        | 297/2132 [01:48<11:11,  2.73it/s]Evaluating:  14%|█▍        | 298/2132 [01:48<11:11,  2.73it/s]Evaluating:  14%|█▍        | 299/2132 [01:49<11:09,  2.74it/s]Evaluating:  14%|█▍        | 300/2132 [01:49<11:10,  2.73it/s]Evaluating:  14%|█▍        | 301/2132 [01:50<11:10,  2.73it/s]Evaluating:  14%|█▍        | 302/2132 [01:50<11:09,  2.73it/s]Evaluating:  14%|█▍        | 303/2132 [01:50<11:07,  2.74it/s]Evaluating:  14%|█▍        | 304/2132 [01:51<11:07,  2.74it/s]Evaluating:  14%|█▍        | 305/2132 [01:51<11:05,  2.74it/s]Evaluating:  14%|█▍        | 306/2132 [01:51<11:04,  2.75it/s]Evaluating:  14%|█▍        | 307/2132 [01:52<11:04,  2.75it/s]Evaluating:  14%|█▍        | 308/2132 [01:52<11:04,  2.75it/s]Evaluating:  14%|█▍        | 309/2132 [01:52<11:06,  2.74it/s]Evaluating:  15%|█▍        | 310/2132 [01:53<11:07,  2.73it/s]Evaluating:  15%|█▍        | 311/2132 [01:53<11:07,  2.73it/s]Evaluating:  15%|█▍        | 312/2132 [01:54<11:07,  2.72it/s]Evaluating:  15%|█▍        | 313/2132 [01:54<11:07,  2.73it/s]Evaluating:  15%|█▍        | 314/2132 [01:54<11:08,  2.72it/s]Evaluating:  15%|█▍        | 315/2132 [01:55<11:09,  2.71it/s]Evaluating:  15%|█▍        | 316/2132 [01:55<11:07,  2.72it/s]Evaluating:  15%|█▍        | 317/2132 [01:55<11:11,  2.70it/s]Evaluating:  15%|█▍        | 318/2132 [01:56<11:08,  2.71it/s]Evaluating:  15%|█▍        | 319/2132 [01:56<11:04,  2.73it/s]Evaluating:  15%|█▌        | 320/2132 [01:57<11:01,  2.74it/s]Evaluating:  15%|█▌        | 321/2132 [01:57<11:03,  2.73it/s]Evaluating:  15%|█▌        | 322/2132 [01:57<11:01,  2.74it/s]Evaluating:  15%|█▌        | 323/2132 [01:58<11:00,  2.74it/s]Evaluating:  15%|█▌        | 324/2132 [01:58<11:00,  2.74it/s]Evaluating:  15%|█▌        | 325/2132 [01:58<11:03,  2.72it/s]Evaluating:  15%|█▌        | 326/2132 [01:59<11:05,  2.71it/s]Evaluating:  15%|█▌        | 327/2132 [01:59<11:05,  2.71it/s]Evaluating:  15%|█▌        | 328/2132 [01:59<11:05,  2.71it/s]Evaluating:  15%|█▌        | 329/2132 [02:00<11:03,  2.72it/s]Evaluating:  15%|█▌        | 330/2132 [02:00<11:01,  2.72it/s]Evaluating:  16%|█▌        | 331/2132 [02:01<11:01,  2.72it/s]Evaluating:  16%|█▌        | 332/2132 [02:01<11:01,  2.72it/s]Evaluating:  16%|█▌        | 333/2132 [02:01<11:01,  2.72it/s]Evaluating:  16%|█▌        | 334/2132 [02:02<11:01,  2.72it/s]Evaluating:  16%|█▌        | 335/2132 [02:02<10:59,  2.72it/s]Evaluating:  16%|█▌        | 336/2132 [02:02<10:56,  2.73it/s]Evaluating:  16%|█▌        | 337/2132 [02:03<10:55,  2.74it/s]Evaluating:  16%|█▌        | 338/2132 [02:03<10:55,  2.74it/s]Evaluating:  16%|█▌        | 339/2132 [02:03<10:53,  2.74it/s]Evaluating:  16%|█▌        | 340/2132 [02:04<10:54,  2.74it/s]Evaluating:  16%|█▌        | 341/2132 [02:04<10:54,  2.74it/s]Evaluating:  16%|█▌        | 342/2132 [02:05<10:55,  2.73it/s]Evaluating:  16%|█▌        | 343/2132 [02:05<10:54,  2.73it/s]Evaluating:  16%|█▌        | 344/2132 [02:05<10:52,  2.74it/s]Evaluating:  16%|█▌        | 345/2132 [02:06<10:52,  2.74it/s]Evaluating:  16%|█▌        | 346/2132 [02:06<10:53,  2.73it/s]Evaluating:  16%|█▋        | 347/2132 [02:06<10:54,  2.73it/s]Evaluating:  16%|█▋        | 348/2132 [02:07<10:51,  2.74it/s]Evaluating:  16%|█▋        | 349/2132 [02:07<10:51,  2.74it/s]Evaluating:  16%|█▋        | 350/2132 [02:08<10:48,  2.75it/s]Evaluating:  16%|█▋        | 351/2132 [02:08<10:49,  2.74it/s]Evaluating:  17%|█▋        | 352/2132 [02:08<10:49,  2.74it/s]Evaluating:  17%|█▋        | 353/2132 [02:09<10:49,  2.74it/s]Evaluating:  17%|█▋        | 354/2132 [02:09<10:50,  2.74it/s]Evaluating:  17%|█▋        | 355/2132 [02:09<10:50,  2.73it/s]Evaluating:  17%|█▋        | 356/2132 [02:10<10:50,  2.73it/s]Evaluating:  17%|█▋        | 357/2132 [02:10<10:48,  2.74it/s]Evaluating:  17%|█▋        | 358/2132 [02:10<10:48,  2.74it/s]Evaluating:  17%|█▋        | 359/2132 [02:11<10:48,  2.74it/s]Evaluating:  17%|█▋        | 360/2132 [02:11<10:50,  2.72it/s]Evaluating:  17%|█▋        | 361/2132 [02:12<10:52,  2.72it/s]Evaluating:  17%|█▋        | 362/2132 [02:12<10:50,  2.72it/s]Evaluating:  17%|█▋        | 363/2132 [02:12<10:50,  2.72it/s]Evaluating:  17%|█▋        | 364/2132 [02:13<10:50,  2.72it/s]Evaluating:  17%|█▋        | 365/2132 [02:13<10:49,  2.72it/s]Evaluating:  17%|█▋        | 366/2132 [02:13<10:49,  2.72it/s]Evaluating:  17%|█▋        | 367/2132 [02:14<10:49,  2.72it/s]Evaluating:  17%|█▋        | 368/2132 [02:14<10:48,  2.72it/s]Evaluating:  17%|█▋        | 369/2132 [02:14<10:47,  2.72it/s]Evaluating:  17%|█▋        | 370/2132 [02:15<10:46,  2.73it/s]Evaluating:  17%|█▋        | 371/2132 [02:15<10:45,  2.73it/s]Evaluating:  17%|█▋        | 372/2132 [02:16<10:44,  2.73it/s]Evaluating:  17%|█▋        | 373/2132 [02:16<10:40,  2.74it/s]Evaluating:  18%|█▊        | 374/2132 [02:16<10:38,  2.75it/s]Evaluating:  18%|█▊        | 375/2132 [02:17<10:38,  2.75it/s]Evaluating:  18%|█▊        | 376/2132 [02:17<10:38,  2.75it/s]Evaluating:  18%|█▊        | 377/2132 [02:17<10:38,  2.75it/s]Evaluating:  18%|█▊        | 378/2132 [02:18<10:38,  2.75it/s]Evaluating:  18%|█▊        | 379/2132 [02:18<10:36,  2.75it/s]Evaluating:  18%|█▊        | 380/2132 [02:18<10:38,  2.75it/s]Evaluating:  18%|█▊        | 381/2132 [02:19<10:40,  2.73it/s]Evaluating:  18%|█▊        | 382/2132 [02:19<10:40,  2.73it/s]Evaluating:  18%|█▊        | 383/2132 [02:20<10:39,  2.74it/s]Evaluating:  18%|█▊        | 384/2132 [02:20<10:41,  2.72it/s]Evaluating:  18%|█▊        | 385/2132 [02:20<10:42,  2.72it/s]Evaluating:  18%|█▊        | 386/2132 [02:21<10:41,  2.72it/s]Evaluating:  18%|█▊        | 387/2132 [02:21<10:39,  2.73it/s]Evaluating:  18%|█▊        | 388/2132 [02:21<10:36,  2.74it/s]Evaluating:  18%|█▊        | 389/2132 [02:22<10:35,  2.74it/s]Evaluating:  18%|█▊        | 390/2132 [02:22<10:34,  2.74it/s]Evaluating:  18%|█▊        | 391/2132 [02:23<10:34,  2.74it/s]Evaluating:  18%|█▊        | 392/2132 [02:23<10:36,  2.73it/s]Evaluating:  18%|█▊        | 393/2132 [02:23<10:36,  2.73it/s]Evaluating:  18%|█▊        | 394/2132 [02:24<10:36,  2.73it/s]Evaluating:  19%|█▊        | 395/2132 [02:24<10:35,  2.73it/s]Evaluating:  19%|█▊        | 396/2132 [02:24<10:37,  2.72it/s]Evaluating:  19%|█▊        | 397/2132 [02:25<10:37,  2.72it/s]Evaluating:  19%|█▊        | 398/2132 [02:25<10:39,  2.71it/s]Evaluating:  19%|█▊        | 399/2132 [02:25<10:39,  2.71it/s]Evaluating:  19%|█▉        | 400/2132 [02:26<10:38,  2.71it/s]Evaluating:  19%|█▉        | 401/2132 [02:26<10:37,  2.72it/s]Evaluating:  19%|█▉        | 402/2132 [02:27<10:38,  2.71it/s]Evaluating:  19%|█▉        | 403/2132 [02:27<10:37,  2.71it/s]Evaluating:  19%|█▉        | 404/2132 [02:27<10:36,  2.72it/s]Evaluating:  19%|█▉        | 405/2132 [02:28<10:35,  2.72it/s]Evaluating:  19%|█▉        | 406/2132 [02:28<10:32,  2.73it/s]Evaluating:  19%|█▉        | 407/2132 [02:28<10:33,  2.72it/s]Evaluating:  19%|█▉        | 408/2132 [02:29<10:32,  2.73it/s]Evaluating:  19%|█▉        | 409/2132 [02:29<10:29,  2.74it/s]Evaluating:  19%|█▉        | 410/2132 [02:30<10:29,  2.74it/s]Evaluating:  19%|█▉        | 411/2132 [02:30<10:28,  2.74it/s]Evaluating:  19%|█▉        | 412/2132 [02:30<10:28,  2.74it/s]Evaluating:  19%|█▉        | 413/2132 [02:31<10:28,  2.73it/s]Evaluating:  19%|█▉        | 414/2132 [02:31<10:28,  2.73it/s]Evaluating:  19%|█▉        | 415/2132 [02:31<10:26,  2.74it/s]Evaluating:  20%|█▉        | 416/2132 [02:32<10:28,  2.73it/s]Evaluating:  20%|█▉        | 417/2132 [02:32<10:29,  2.73it/s]Evaluating:  20%|█▉        | 418/2132 [02:32<10:26,  2.74it/s]Evaluating:  20%|█▉        | 419/2132 [02:33<10:23,  2.75it/s]Evaluating:  20%|█▉        | 420/2132 [02:33<10:25,  2.74it/s]Evaluating:  20%|█▉        | 421/2132 [02:34<10:25,  2.74it/s]Evaluating:  20%|█▉        | 422/2132 [02:34<10:24,  2.74it/s]Evaluating:  20%|█▉        | 423/2132 [02:34<10:24,  2.74it/s]Evaluating:  20%|█▉        | 424/2132 [02:35<10:26,  2.73it/s]Evaluating:  20%|█▉        | 425/2132 [02:35<10:24,  2.73it/s]Evaluating:  20%|█▉        | 426/2132 [02:35<10:23,  2.74it/s]Evaluating:  20%|██        | 427/2132 [02:36<10:25,  2.73it/s]Evaluating:  20%|██        | 428/2132 [02:36<10:25,  2.73it/s]Evaluating:  20%|██        | 429/2132 [02:36<10:26,  2.72it/s]Evaluating:  20%|██        | 430/2132 [02:37<10:24,  2.73it/s]Evaluating:  20%|██        | 431/2132 [02:37<10:21,  2.74it/s]Evaluating:  20%|██        | 432/2132 [02:38<10:20,  2.74it/s]Evaluating:  20%|██        | 433/2132 [02:38<10:21,  2.74it/s]Evaluating:  20%|██        | 434/2132 [02:38<10:21,  2.73it/s]Evaluating:  20%|██        | 435/2132 [02:39<10:36,  2.67it/s]Evaluating:  20%|██        | 436/2132 [02:39<10:31,  2.69it/s]Evaluating:  20%|██        | 437/2132 [02:39<10:28,  2.69it/s]Evaluating:  21%|██        | 438/2132 [02:40<10:28,  2.70it/s]Evaluating:  21%|██        | 439/2132 [02:40<10:26,  2.70it/s]Evaluating:  21%|██        | 440/2132 [02:41<10:23,  2.72it/s]Evaluating:  21%|██        | 441/2132 [02:41<10:20,  2.73it/s]Evaluating:  21%|██        | 442/2132 [02:41<10:17,  2.74it/s]Evaluating:  21%|██        | 443/2132 [02:42<10:15,  2.74it/s]Evaluating:  21%|██        | 444/2132 [02:42<10:13,  2.75it/s]Evaluating:  21%|██        | 445/2132 [02:42<10:13,  2.75it/s]Evaluating:  21%|██        | 446/2132 [02:43<10:16,  2.74it/s]Evaluating:  21%|██        | 447/2132 [02:43<10:16,  2.73it/s]Evaluating:  21%|██        | 448/2132 [02:43<10:14,  2.74it/s]Evaluating:  21%|██        | 449/2132 [02:44<10:14,  2.74it/s]Evaluating:  21%|██        | 450/2132 [02:44<10:13,  2.74it/s]Evaluating:  21%|██        | 451/2132 [02:45<10:13,  2.74it/s]Evaluating:  21%|██        | 452/2132 [02:45<10:12,  2.74it/s]Evaluating:  21%|██        | 453/2132 [02:45<10:14,  2.73it/s]Evaluating:  21%|██▏       | 454/2132 [02:46<10:15,  2.72it/s]Evaluating:  21%|██▏       | 455/2132 [02:46<10:17,  2.72it/s]Evaluating:  21%|██▏       | 456/2132 [02:46<10:13,  2.73it/s]Evaluating:  21%|██▏       | 457/2132 [02:47<10:12,  2.73it/s]Evaluating:  21%|██▏       | 458/2132 [02:47<10:12,  2.73it/s]Evaluating:  22%|██▏       | 459/2132 [02:47<10:09,  2.74it/s]Evaluating:  22%|██▏       | 460/2132 [02:48<10:09,  2.74it/s]Evaluating:  22%|██▏       | 461/2132 [02:48<10:09,  2.74it/s]Evaluating:  22%|██▏       | 462/2132 [02:49<10:11,  2.73it/s]Evaluating:  22%|██▏       | 463/2132 [02:49<10:11,  2.73it/s]Evaluating:  22%|██▏       | 464/2132 [02:49<10:13,  2.72it/s]Evaluating:  22%|██▏       | 465/2132 [02:50<10:17,  2.70it/s]Evaluating:  22%|██▏       | 466/2132 [02:50<10:15,  2.71it/s]Evaluating:  22%|██▏       | 467/2132 [02:50<10:14,  2.71it/s]Evaluating:  22%|██▏       | 468/2132 [02:51<10:13,  2.71it/s]Evaluating:  22%|██▏       | 469/2132 [02:51<10:10,  2.73it/s]Evaluating:  22%|██▏       | 470/2132 [02:51<10:08,  2.73it/s]Evaluating:  22%|██▏       | 471/2132 [02:52<10:09,  2.73it/s]Evaluating:  22%|██▏       | 472/2132 [02:52<10:06,  2.74it/s]Evaluating:  22%|██▏       | 473/2132 [02:53<10:05,  2.74it/s]Evaluating:  22%|██▏       | 474/2132 [02:53<10:03,  2.75it/s]Evaluating:  22%|██▏       | 475/2132 [02:53<10:04,  2.74it/s]Evaluating:  22%|██▏       | 476/2132 [02:54<10:01,  2.75it/s]Evaluating:  22%|██▏       | 477/2132 [02:54<10:03,  2.74it/s]Evaluating:  22%|██▏       | 478/2132 [02:54<10:02,  2.74it/s]Evaluating:  22%|██▏       | 479/2132 [02:55<10:00,  2.75it/s]Evaluating:  23%|██▎       | 480/2132 [02:55<10:01,  2.75it/s]Evaluating:  23%|██▎       | 481/2132 [02:56<10:02,  2.74it/s]Evaluating:  23%|██▎       | 482/2132 [02:56<10:02,  2.74it/s]Evaluating:  23%|██▎       | 483/2132 [02:56<10:03,  2.73it/s]Evaluating:  23%|██▎       | 484/2132 [02:57<10:02,  2.74it/s]Evaluating:  23%|██▎       | 485/2132 [02:57<10:02,  2.73it/s]Evaluating:  23%|██▎       | 486/2132 [02:57<10:02,  2.73it/s]Evaluating:  23%|██▎       | 487/2132 [02:58<10:03,  2.73it/s]Evaluating:  23%|██▎       | 488/2132 [02:58<10:01,  2.73it/s]Evaluating:  23%|██▎       | 489/2132 [02:58<10:03,  2.72it/s]Evaluating:  23%|██▎       | 490/2132 [02:59<10:02,  2.72it/s]Evaluating:  23%|██▎       | 491/2132 [02:59<10:01,  2.73it/s]Evaluating:  23%|██▎       | 492/2132 [03:00<09:58,  2.74it/s]Evaluating:  23%|██▎       | 493/2132 [03:00<09:58,  2.74it/s]Evaluating:  23%|██▎       | 494/2132 [03:00<10:00,  2.73it/s]Evaluating:  23%|██▎       | 495/2132 [03:01<09:59,  2.73it/s]Evaluating:  23%|██▎       | 496/2132 [03:01<09:59,  2.73it/s]Evaluating:  23%|██▎       | 497/2132 [03:01<09:57,  2.73it/s]Evaluating:  23%|██▎       | 498/2132 [03:02<09:58,  2.73it/s]Evaluating:  23%|██▎       | 499/2132 [03:02<09:57,  2.73it/s]Evaluating:  23%|██▎       | 500/2132 [03:02<09:56,  2.73it/s]Evaluating:  23%|██▎       | 501/2132 [03:03<09:56,  2.73it/s]Evaluating:  24%|██▎       | 502/2132 [03:03<09:57,  2.73it/s]Evaluating:  24%|██▎       | 503/2132 [03:04<09:58,  2.72it/s]Evaluating:  24%|██▎       | 504/2132 [03:04<09:58,  2.72it/s]Evaluating:  24%|██▎       | 505/2132 [03:04<09:57,  2.72it/s]Evaluating:  24%|██▎       | 506/2132 [03:05<09:57,  2.72it/s]Evaluating:  24%|██▍       | 507/2132 [03:05<09:58,  2.71it/s]Evaluating:  24%|██▍       | 508/2132 [03:05<09:58,  2.71it/s]Evaluating:  24%|██▍       | 509/2132 [03:06<09:56,  2.72it/s]Evaluating:  24%|██▍       | 510/2132 [03:06<09:55,  2.72it/s]Evaluating:  24%|██▍       | 511/2132 [03:07<09:56,  2.72it/s]Evaluating:  24%|██▍       | 512/2132 [03:07<09:56,  2.72it/s]Evaluating:  24%|██▍       | 513/2132 [03:07<09:56,  2.72it/s]Evaluating:  24%|██▍       | 514/2132 [03:08<09:57,  2.71it/s]Evaluating:  24%|██▍       | 515/2132 [03:08<09:57,  2.71it/s]Evaluating:  24%|██▍       | 516/2132 [03:08<09:57,  2.70it/s]Evaluating:  24%|██▍       | 517/2132 [03:09<09:56,  2.71it/s]Evaluating:  24%|██▍       | 518/2132 [03:09<09:55,  2.71it/s]Evaluating:  24%|██▍       | 519/2132 [03:09<09:54,  2.71it/s]Evaluating:  24%|██▍       | 520/2132 [03:10<09:53,  2.72it/s]Evaluating:  24%|██▍       | 521/2132 [03:10<09:52,  2.72it/s]Evaluating:  24%|██▍       | 522/2132 [03:11<09:51,  2.72it/s]Evaluating:  25%|██▍       | 523/2132 [03:11<09:48,  2.73it/s]Evaluating:  25%|██▍       | 524/2132 [03:11<09:48,  2.73it/s]Evaluating:  25%|██▍       | 525/2132 [03:12<09:51,  2.72it/s]Evaluating:  25%|██▍       | 526/2132 [03:12<09:53,  2.71it/s]Evaluating:  25%|██▍       | 527/2132 [03:12<09:52,  2.71it/s]Evaluating:  25%|██▍       | 528/2132 [03:13<09:50,  2.71it/s]Evaluating:  25%|██▍       | 529/2132 [03:13<09:51,  2.71it/s]Evaluating:  25%|██▍       | 530/2132 [03:14<09:49,  2.72it/s]Evaluating:  25%|██▍       | 531/2132 [03:14<09:51,  2.71it/s]Evaluating:  25%|██▍       | 532/2132 [03:14<09:49,  2.71it/s]Evaluating:  25%|██▌       | 533/2132 [03:15<09:51,  2.70it/s]Evaluating:  25%|██▌       | 534/2132 [03:15<09:49,  2.71it/s]Evaluating:  25%|██▌       | 535/2132 [03:15<09:48,  2.71it/s]Evaluating:  25%|██▌       | 536/2132 [03:16<09:51,  2.70it/s]Evaluating:  25%|██▌       | 537/2132 [03:16<09:48,  2.71it/s]Evaluating:  25%|██▌       | 538/2132 [03:16<09:46,  2.72it/s]Evaluating:  25%|██▌       | 539/2132 [03:17<09:47,  2.71it/s]Evaluating:  25%|██▌       | 540/2132 [03:17<09:48,  2.71it/s]Evaluating:  25%|██▌       | 541/2132 [03:18<09:48,  2.70it/s]Evaluating:  25%|██▌       | 542/2132 [03:18<09:47,  2.71it/s]Evaluating:  25%|██▌       | 543/2132 [03:18<09:44,  2.72it/s]Evaluating:  26%|██▌       | 544/2132 [03:19<09:42,  2.72it/s]Evaluating:  26%|██▌       | 545/2132 [03:19<09:41,  2.73it/s]Evaluating:  26%|██▌       | 546/2132 [03:19<09:39,  2.74it/s]Evaluating:  26%|██▌       | 547/2132 [03:20<09:38,  2.74it/s]Evaluating:  26%|██▌       | 548/2132 [03:20<09:37,  2.74it/s]Evaluating:  26%|██▌       | 549/2132 [03:20<09:38,  2.73it/s]Evaluating:  26%|██▌       | 550/2132 [03:21<09:39,  2.73it/s]Evaluating:  26%|██▌       | 551/2132 [03:21<09:37,  2.74it/s]Evaluating:  26%|██▌       | 552/2132 [03:22<09:36,  2.74it/s]Evaluating:  26%|██▌       | 553/2132 [03:22<09:38,  2.73it/s]Evaluating:  26%|██▌       | 554/2132 [03:22<09:39,  2.72it/s]Evaluating:  26%|██▌       | 555/2132 [03:23<09:40,  2.72it/s]Evaluating:  26%|██▌       | 556/2132 [03:23<09:41,  2.71it/s]Evaluating:  26%|██▌       | 557/2132 [03:23<09:40,  2.71it/s]Evaluating:  26%|██▌       | 558/2132 [03:24<09:39,  2.72it/s]Evaluating:  26%|██▌       | 559/2132 [03:24<09:39,  2.71it/s]Evaluating:  26%|██▋       | 560/2132 [03:25<09:38,  2.72it/s]Evaluating:  26%|██▋       | 561/2132 [03:25<09:40,  2.71it/s]Evaluating:  26%|██▋       | 562/2132 [03:25<09:38,  2.71it/s]Evaluating:  26%|██▋       | 563/2132 [03:26<09:39,  2.71it/s]Evaluating:  26%|██▋       | 564/2132 [03:26<09:38,  2.71it/s]Evaluating:  27%|██▋       | 565/2132 [03:26<09:37,  2.71it/s]Evaluating:  27%|██▋       | 566/2132 [03:27<09:35,  2.72it/s]Evaluating:  27%|██▋       | 567/2132 [03:27<09:36,  2.72it/s]Evaluating:  27%|██▋       | 568/2132 [03:27<09:35,  2.72it/s]Evaluating:  27%|██▋       | 569/2132 [03:28<09:35,  2.72it/s]Evaluating:  27%|██▋       | 570/2132 [03:28<09:33,  2.72it/s]Evaluating:  27%|██▋       | 571/2132 [03:29<09:33,  2.72it/s]Evaluating:  27%|██▋       | 572/2132 [03:29<09:32,  2.72it/s]Evaluating:  27%|██▋       | 573/2132 [03:29<09:31,  2.73it/s]Evaluating:  27%|██▋       | 574/2132 [03:30<09:32,  2.72it/s]Evaluating:  27%|██▋       | 575/2132 [03:30<09:31,  2.72it/s]Evaluating:  27%|██▋       | 576/2132 [03:30<09:30,  2.73it/s]Evaluating:  27%|██▋       | 577/2132 [03:31<09:29,  2.73it/s]Evaluating:  27%|██▋       | 578/2132 [03:31<09:28,  2.73it/s]Evaluating:  27%|██▋       | 579/2132 [03:32<09:30,  2.72it/s]Evaluating:  27%|██▋       | 580/2132 [03:32<09:31,  2.72it/s]Evaluating:  27%|██▋       | 581/2132 [03:32<09:28,  2.73it/s]Evaluating:  27%|██▋       | 582/2132 [03:33<09:30,  2.72it/s]Evaluating:  27%|██▋       | 583/2132 [03:33<09:31,  2.71it/s]Evaluating:  27%|██▋       | 584/2132 [03:33<09:30,  2.71it/s]Evaluating:  27%|██▋       | 585/2132 [03:34<09:30,  2.71it/s]Evaluating:  27%|██▋       | 586/2132 [03:34<09:30,  2.71it/s]Evaluating:  28%|██▊       | 587/2132 [03:34<09:30,  2.71it/s]Evaluating:  28%|██▊       | 588/2132 [03:35<09:30,  2.71it/s]Evaluating:  28%|██▊       | 589/2132 [03:35<09:29,  2.71it/s]Evaluating:  28%|██▊       | 590/2132 [03:36<09:28,  2.71it/s]Evaluating:  28%|██▊       | 591/2132 [03:36<09:27,  2.72it/s]Evaluating:  28%|██▊       | 592/2132 [03:36<09:25,  2.72it/s]Evaluating:  28%|██▊       | 593/2132 [03:37<09:25,  2.72it/s]Evaluating:  28%|██▊       | 594/2132 [03:37<09:26,  2.71it/s]Evaluating:  28%|██▊       | 595/2132 [03:37<09:27,  2.71it/s]Evaluating:  28%|██▊       | 596/2132 [03:38<09:27,  2.71it/s]Evaluating:  28%|██▊       | 597/2132 [03:38<09:25,  2.71it/s]Evaluating:  28%|██▊       | 598/2132 [03:39<09:26,  2.71it/s]Evaluating:  28%|██▊       | 599/2132 [03:39<09:25,  2.71it/s]Evaluating:  28%|██▊       | 600/2132 [03:39<09:25,  2.71it/s]Evaluating:  28%|██▊       | 601/2132 [03:40<09:23,  2.72it/s]Evaluating:  28%|██▊       | 602/2132 [03:40<09:27,  2.70it/s]Evaluating:  28%|██▊       | 603/2132 [03:40<09:26,  2.70it/s]Evaluating:  28%|██▊       | 604/2132 [03:41<09:25,  2.70it/s]Evaluating:  28%|██▊       | 605/2132 [03:41<09:24,  2.70it/s]Evaluating:  28%|██▊       | 606/2132 [03:41<09:21,  2.72it/s]Evaluating:  28%|██▊       | 607/2132 [03:42<09:20,  2.72it/s]Evaluating:  29%|██▊       | 608/2132 [03:42<09:20,  2.72it/s]Evaluating:  29%|██▊       | 609/2132 [03:43<09:21,  2.71it/s]Evaluating:  29%|██▊       | 610/2132 [03:43<09:19,  2.72it/s]Evaluating:  29%|██▊       | 611/2132 [03:43<09:20,  2.71it/s]Evaluating:  29%|██▊       | 612/2132 [03:44<09:21,  2.71it/s]Evaluating:  29%|██▉       | 613/2132 [03:44<09:20,  2.71it/s]Evaluating:  29%|██▉       | 614/2132 [03:44<09:20,  2.71it/s]Evaluating:  29%|██▉       | 615/2132 [03:45<09:20,  2.71it/s]Evaluating:  29%|██▉       | 616/2132 [03:45<09:17,  2.72it/s]Evaluating:  29%|██▉       | 617/2132 [03:46<09:16,  2.72it/s]Evaluating:  29%|██▉       | 618/2132 [03:46<09:16,  2.72it/s]Evaluating:  29%|██▉       | 619/2132 [03:46<09:16,  2.72it/s]Evaluating:  29%|██▉       | 620/2132 [03:47<09:15,  2.72it/s]Evaluating:  29%|██▉       | 621/2132 [03:47<09:14,  2.73it/s]Evaluating:  29%|██▉       | 622/2132 [03:47<09:12,  2.73it/s]Evaluating:  29%|██▉       | 623/2132 [03:48<09:11,  2.74it/s]Evaluating:  29%|██▉       | 624/2132 [03:48<09:10,  2.74it/s]Evaluating:  29%|██▉       | 625/2132 [03:48<09:09,  2.74it/s]Evaluating:  29%|██▉       | 626/2132 [03:49<09:09,  2.74it/s]Evaluating:  29%|██▉       | 627/2132 [03:49<09:09,  2.74it/s]Evaluating:  29%|██▉       | 628/2132 [03:50<09:10,  2.73it/s]Evaluating:  30%|██▉       | 629/2132 [03:50<09:08,  2.74it/s]Evaluating:  30%|██▉       | 630/2132 [03:50<09:10,  2.73it/s]Evaluating:  30%|██▉       | 631/2132 [03:51<09:09,  2.73it/s]Evaluating:  30%|██▉       | 632/2132 [03:51<09:10,  2.73it/s]Evaluating:  30%|██▉       | 633/2132 [03:51<09:08,  2.73it/s]Evaluating:  30%|██▉       | 634/2132 [03:52<09:09,  2.73it/s]Evaluating:  30%|██▉       | 635/2132 [03:52<09:08,  2.73it/s]Evaluating:  30%|██▉       | 636/2132 [03:52<09:09,  2.72it/s]Evaluating:  30%|██▉       | 637/2132 [03:53<09:08,  2.73it/s]Evaluating:  30%|██▉       | 638/2132 [03:53<09:05,  2.74it/s]Evaluating:  30%|██▉       | 639/2132 [03:54<09:04,  2.74it/s]Evaluating:  30%|███       | 640/2132 [03:54<09:02,  2.75it/s]Evaluating:  30%|███       | 641/2132 [03:54<09:01,  2.76it/s]Evaluating:  30%|███       | 642/2132 [03:55<09:01,  2.75it/s]Evaluating:  30%|███       | 643/2132 [03:55<09:03,  2.74it/s]Evaluating:  30%|███       | 644/2132 [03:55<09:01,  2.75it/s]Evaluating:  30%|███       | 645/2132 [03:56<09:03,  2.74it/s]Evaluating:  30%|███       | 646/2132 [03:56<09:05,  2.72it/s]Evaluating:  30%|███       | 647/2132 [03:57<09:05,  2.72it/s]Evaluating:  30%|███       | 648/2132 [03:57<09:04,  2.73it/s]Evaluating:  30%|███       | 649/2132 [03:57<09:02,  2.73it/s]Evaluating:  30%|███       | 650/2132 [03:58<09:02,  2.73it/s]Evaluating:  31%|███       | 651/2132 [03:58<09:17,  2.66it/s]Evaluating:  31%|███       | 652/2132 [03:58<09:11,  2.68it/s]Evaluating:  31%|███       | 653/2132 [03:59<09:09,  2.69it/s]Evaluating:  31%|███       | 654/2132 [03:59<09:07,  2.70it/s]Evaluating:  31%|███       | 655/2132 [03:59<09:05,  2.71it/s]Evaluating:  31%|███       | 656/2132 [04:00<09:01,  2.72it/s]Evaluating:  31%|███       | 657/2132 [04:00<08:59,  2.74it/s]Evaluating:  31%|███       | 658/2132 [04:01<08:58,  2.74it/s]Evaluating:  31%|███       | 659/2132 [04:01<08:58,  2.74it/s]Evaluating:  31%|███       | 660/2132 [04:01<08:57,  2.74it/s]Evaluating:  31%|███       | 661/2132 [04:02<08:56,  2.74it/s]Evaluating:  31%|███       | 662/2132 [04:02<08:54,  2.75it/s]Evaluating:  31%|███       | 663/2132 [04:02<08:54,  2.75it/s]Evaluating:  31%|███       | 664/2132 [04:03<08:53,  2.75it/s]Evaluating:  31%|███       | 665/2132 [04:03<08:54,  2.74it/s]Evaluating:  31%|███       | 666/2132 [04:03<08:55,  2.74it/s]Evaluating:  31%|███▏      | 667/2132 [04:04<08:54,  2.74it/s]Evaluating:  31%|███▏      | 668/2132 [04:04<08:53,  2.74it/s]Evaluating:  31%|███▏      | 669/2132 [04:05<08:52,  2.75it/s]Evaluating:  31%|███▏      | 670/2132 [04:05<08:52,  2.75it/s]Evaluating:  31%|███▏      | 671/2132 [04:05<08:51,  2.75it/s]Evaluating:  32%|███▏      | 672/2132 [04:06<08:51,  2.75it/s]Evaluating:  32%|███▏      | 673/2132 [04:06<08:50,  2.75it/s]Evaluating:  32%|███▏      | 674/2132 [04:06<08:51,  2.74it/s]Evaluating:  32%|███▏      | 675/2132 [04:07<08:51,  2.74it/s]Evaluating:  32%|███▏      | 676/2132 [04:07<08:50,  2.75it/s]Evaluating:  32%|███▏      | 677/2132 [04:07<08:50,  2.74it/s]Evaluating:  32%|███▏      | 678/2132 [04:08<08:51,  2.74it/s]Evaluating:  32%|███▏      | 679/2132 [04:08<08:51,  2.73it/s]Evaluating:  32%|███▏      | 680/2132 [04:09<08:52,  2.73it/s]Evaluating:  32%|███▏      | 681/2132 [04:09<08:51,  2.73it/s]Evaluating:  32%|███▏      | 682/2132 [04:09<08:51,  2.73it/s]Evaluating:  32%|███▏      | 683/2132 [04:10<08:50,  2.73it/s]Evaluating:  32%|███▏      | 684/2132 [04:10<08:49,  2.73it/s]Evaluating:  32%|███▏      | 685/2132 [04:10<08:52,  2.72it/s]Evaluating:  32%|███▏      | 686/2132 [04:11<08:50,  2.72it/s]Evaluating:  32%|███▏      | 687/2132 [04:11<08:51,  2.72it/s]Evaluating:  32%|███▏      | 688/2132 [04:12<08:51,  2.72it/s]Evaluating:  32%|███▏      | 689/2132 [04:12<08:49,  2.73it/s]Evaluating:  32%|███▏      | 690/2132 [04:12<08:48,  2.73it/s]Evaluating:  32%|███▏      | 691/2132 [04:13<08:47,  2.73it/s]Evaluating:  32%|███▏      | 692/2132 [04:13<08:49,  2.72it/s]Evaluating:  33%|███▎      | 693/2132 [04:13<08:49,  2.72it/s]Evaluating:  33%|███▎      | 694/2132 [04:14<08:50,  2.71it/s]Evaluating:  33%|███▎      | 695/2132 [04:14<08:49,  2.72it/s]Evaluating:  33%|███▎      | 696/2132 [04:14<08:46,  2.73it/s]Evaluating:  33%|███▎      | 697/2132 [04:15<08:45,  2.73it/s]Evaluating:  33%|███▎      | 698/2132 [04:15<08:50,  2.71it/s]Evaluating:  33%|███▎      | 699/2132 [04:16<08:50,  2.70it/s]Evaluating:  33%|███▎      | 700/2132 [04:16<08:54,  2.68it/s]Evaluating:  33%|███▎      | 701/2132 [04:16<08:53,  2.68it/s]Evaluating:  33%|███▎      | 702/2132 [04:17<08:52,  2.68it/s]Evaluating:  33%|███▎      | 703/2132 [04:17<08:49,  2.70it/s]Evaluating:  33%|███▎      | 704/2132 [04:17<08:49,  2.70it/s]Evaluating:  33%|███▎      | 705/2132 [04:18<08:47,  2.70it/s]Evaluating:  33%|███▎      | 706/2132 [04:18<08:45,  2.71it/s]Evaluating:  33%|███▎      | 707/2132 [04:19<08:44,  2.72it/s]Evaluating:  33%|███▎      | 708/2132 [04:19<08:42,  2.73it/s]Evaluating:  33%|███▎      | 709/2132 [04:19<08:42,  2.73it/s]Evaluating:  33%|███▎      | 710/2132 [04:20<08:42,  2.72it/s]Evaluating:  33%|███▎      | 711/2132 [04:20<08:42,  2.72it/s]Evaluating:  33%|███▎      | 712/2132 [04:20<08:46,  2.70it/s]Evaluating:  33%|███▎      | 713/2132 [04:21<08:44,  2.70it/s]Evaluating:  33%|███▎      | 714/2132 [04:21<08:43,  2.71it/s]Evaluating:  34%|███▎      | 715/2132 [04:21<08:42,  2.71it/s]Evaluating:  34%|███▎      | 716/2132 [04:22<08:41,  2.72it/s]Evaluating:  34%|███▎      | 717/2132 [04:22<08:42,  2.71it/s]Evaluating:  34%|███▎      | 718/2132 [04:23<08:40,  2.71it/s]Evaluating:  34%|███▎      | 719/2132 [04:23<08:39,  2.72it/s]Evaluating:  34%|███▍      | 720/2132 [04:23<08:38,  2.72it/s]Evaluating:  34%|███▍      | 721/2132 [04:24<08:38,  2.72it/s]Evaluating:  34%|███▍      | 722/2132 [04:24<08:37,  2.72it/s]Evaluating:  34%|███▍      | 723/2132 [04:24<08:38,  2.72it/s]Evaluating:  34%|███▍      | 724/2132 [04:25<08:37,  2.72it/s]Evaluating:  34%|███▍      | 725/2132 [04:25<08:38,  2.72it/s]Evaluating:  34%|███▍      | 726/2132 [04:26<08:37,  2.72it/s]Evaluating:  34%|███▍      | 727/2132 [04:26<08:35,  2.72it/s]Evaluating:  34%|███▍      | 728/2132 [04:26<08:36,  2.72it/s]Evaluating:  34%|███▍      | 729/2132 [04:27<08:35,  2.72it/s]Evaluating:  34%|███▍      | 730/2132 [04:27<08:34,  2.72it/s]Evaluating:  34%|███▍      | 731/2132 [04:27<08:37,  2.71it/s]Evaluating:  34%|███▍      | 732/2132 [04:28<08:39,  2.70it/s]Evaluating:  34%|███▍      | 733/2132 [04:28<08:38,  2.70it/s]Evaluating:  34%|███▍      | 734/2132 [04:28<08:35,  2.71it/s]Evaluating:  34%|███▍      | 735/2132 [04:29<08:36,  2.70it/s]Evaluating:  35%|███▍      | 736/2132 [04:29<08:36,  2.70it/s]Evaluating:  35%|███▍      | 737/2132 [04:30<08:35,  2.71it/s]Evaluating:  35%|███▍      | 738/2132 [04:30<08:33,  2.72it/s]Evaluating:  35%|███▍      | 739/2132 [04:30<08:32,  2.72it/s]Evaluating:  35%|███▍      | 740/2132 [04:31<08:32,  2.72it/s]Evaluating:  35%|███▍      | 741/2132 [04:31<08:30,  2.72it/s]Evaluating:  35%|███▍      | 742/2132 [04:31<08:28,  2.73it/s]Evaluating:  35%|███▍      | 743/2132 [04:32<08:29,  2.73it/s]Evaluating:  35%|███▍      | 744/2132 [04:32<08:29,  2.73it/s]Evaluating:  35%|███▍      | 745/2132 [04:33<08:29,  2.72it/s]Evaluating:  35%|███▍      | 746/2132 [04:33<08:29,  2.72it/s]Evaluating:  35%|███▌      | 747/2132 [04:33<08:29,  2.72it/s]Evaluating:  35%|███▌      | 748/2132 [04:34<08:26,  2.73it/s]Evaluating:  35%|███▌      | 749/2132 [04:34<08:26,  2.73it/s]Evaluating:  35%|███▌      | 750/2132 [04:34<08:27,  2.72it/s]Evaluating:  35%|███▌      | 751/2132 [04:35<08:29,  2.71it/s]Evaluating:  35%|███▌      | 752/2132 [04:35<08:28,  2.72it/s]Evaluating:  35%|███▌      | 753/2132 [04:35<08:28,  2.71it/s]Evaluating:  35%|███▌      | 754/2132 [04:36<08:25,  2.72it/s]Evaluating:  35%|███▌      | 755/2132 [04:36<08:24,  2.73it/s]Evaluating:  35%|███▌      | 756/2132 [04:37<08:24,  2.73it/s]Evaluating:  36%|███▌      | 757/2132 [04:37<08:24,  2.72it/s]Evaluating:  36%|███▌      | 758/2132 [04:37<08:24,  2.72it/s]Evaluating:  36%|███▌      | 759/2132 [04:38<08:25,  2.72it/s]Evaluating:  36%|███▌      | 760/2132 [04:38<08:25,  2.72it/s]Evaluating:  36%|███▌      | 761/2132 [04:38<08:26,  2.71it/s]Evaluating:  36%|███▌      | 762/2132 [04:39<08:27,  2.70it/s]Evaluating:  36%|███▌      | 763/2132 [04:39<08:27,  2.70it/s]Evaluating:  36%|███▌      | 764/2132 [04:40<08:27,  2.69it/s]Evaluating:  36%|███▌      | 765/2132 [04:40<08:26,  2.70it/s]Evaluating:  36%|███▌      | 766/2132 [04:40<08:26,  2.69it/s]Evaluating:  36%|███▌      | 767/2132 [04:41<08:26,  2.70it/s]Evaluating:  36%|███▌      | 768/2132 [04:41<08:24,  2.70it/s]Evaluating:  36%|███▌      | 769/2132 [04:41<08:24,  2.70it/s]Evaluating:  36%|███▌      | 770/2132 [04:42<08:23,  2.70it/s]Evaluating:  36%|███▌      | 771/2132 [04:42<08:24,  2.70it/s]Evaluating:  36%|███▌      | 772/2132 [04:42<08:25,  2.69it/s]Evaluating:  36%|███▋      | 773/2132 [04:43<08:24,  2.69it/s]Evaluating:  36%|███▋      | 774/2132 [04:43<08:21,  2.71it/s]Evaluating:  36%|███▋      | 775/2132 [04:44<08:19,  2.71it/s]Evaluating:  36%|███▋      | 776/2132 [04:44<08:18,  2.72it/s]Evaluating:  36%|███▋      | 777/2132 [04:44<08:18,  2.72it/s]Evaluating:  36%|███▋      | 778/2132 [04:45<08:19,  2.71it/s]Evaluating:  37%|███▋      | 779/2132 [04:45<08:18,  2.71it/s]Evaluating:  37%|███▋      | 780/2132 [04:45<08:18,  2.71it/s]Evaluating:  37%|███▋      | 781/2132 [04:46<08:19,  2.71it/s]Evaluating:  37%|███▋      | 782/2132 [04:46<08:18,  2.71it/s]Evaluating:  37%|███▋      | 783/2132 [04:47<08:18,  2.71it/s]Evaluating:  37%|███▋      | 784/2132 [04:47<08:16,  2.72it/s]Evaluating:  37%|███▋      | 785/2132 [04:47<08:15,  2.72it/s]Evaluating:  37%|███▋      | 786/2132 [04:48<08:13,  2.73it/s]Evaluating:  37%|███▋      | 787/2132 [04:48<08:12,  2.73it/s]Evaluating:  37%|███▋      | 788/2132 [04:48<08:12,  2.73it/s]Evaluating:  37%|███▋      | 789/2132 [04:49<08:12,  2.72it/s]Evaluating:  37%|███▋      | 790/2132 [04:49<08:11,  2.73it/s]Evaluating:  37%|███▋      | 791/2132 [04:49<08:10,  2.73it/s]Evaluating:  37%|███▋      | 792/2132 [04:50<08:11,  2.73it/s]Evaluating:  37%|███▋      | 793/2132 [04:50<08:14,  2.71it/s]Evaluating:  37%|███▋      | 794/2132 [04:51<08:13,  2.71it/s]Evaluating:  37%|███▋      | 795/2132 [04:51<08:12,  2.71it/s]Evaluating:  37%|███▋      | 796/2132 [04:51<08:12,  2.71it/s]Evaluating:  37%|███▋      | 797/2132 [04:52<08:11,  2.71it/s]Evaluating:  37%|███▋      | 798/2132 [04:52<08:13,  2.70it/s]Evaluating:  37%|███▋      | 799/2132 [04:52<08:12,  2.71it/s]Evaluating:  38%|███▊      | 800/2132 [04:53<08:14,  2.69it/s]Evaluating:  38%|███▊      | 801/2132 [04:53<08:13,  2.70it/s]Evaluating:  38%|███▊      | 802/2132 [04:54<08:13,  2.69it/s]Evaluating:  38%|███▊      | 803/2132 [04:54<08:11,  2.70it/s]Evaluating:  38%|███▊      | 804/2132 [04:54<08:10,  2.71it/s]Evaluating:  38%|███▊      | 805/2132 [04:55<08:10,  2.71it/s]Evaluating:  38%|███▊      | 806/2132 [04:55<08:10,  2.70it/s]Evaluating:  38%|███▊      | 807/2132 [04:55<08:08,  2.71it/s]Evaluating:  38%|███▊      | 808/2132 [04:56<08:07,  2.72it/s]Evaluating:  38%|███▊      | 809/2132 [04:56<08:07,  2.72it/s]Evaluating:  38%|███▊      | 810/2132 [04:56<08:05,  2.72it/s]Evaluating:  38%|███▊      | 811/2132 [04:57<08:05,  2.72it/s]Evaluating:  38%|███▊      | 812/2132 [04:57<08:05,  2.72it/s]Evaluating:  38%|███▊      | 813/2132 [04:58<08:08,  2.70it/s]Evaluating:  38%|███▊      | 814/2132 [04:58<08:06,  2.71it/s]Evaluating:  38%|███▊      | 815/2132 [04:58<08:03,  2.72it/s]Evaluating:  38%|███▊      | 816/2132 [04:59<08:04,  2.71it/s]Evaluating:  38%|███▊      | 817/2132 [04:59<08:04,  2.72it/s]Evaluating:  38%|███▊      | 818/2132 [04:59<08:06,  2.70it/s]Evaluating:  38%|███▊      | 819/2132 [05:00<08:05,  2.70it/s]Evaluating:  38%|███▊      | 820/2132 [05:00<08:05,  2.70it/s]Evaluating:  39%|███▊      | 821/2132 [05:01<08:04,  2.71it/s]Evaluating:  39%|███▊      | 822/2132 [05:01<08:04,  2.70it/s]Evaluating:  39%|███▊      | 823/2132 [05:01<08:03,  2.71it/s]Evaluating:  39%|███▊      | 824/2132 [05:02<08:02,  2.71it/s]Evaluating:  39%|███▊      | 825/2132 [05:02<08:02,  2.71it/s]Evaluating:  39%|███▊      | 826/2132 [05:02<08:02,  2.71it/s]Evaluating:  39%|███▉      | 827/2132 [05:03<08:02,  2.71it/s]Evaluating:  39%|███▉      | 828/2132 [05:03<08:02,  2.70it/s]Evaluating:  39%|███▉      | 829/2132 [05:04<08:02,  2.70it/s]Evaluating:  39%|███▉      | 830/2132 [05:04<08:01,  2.71it/s]Evaluating:  39%|███▉      | 831/2132 [05:04<08:01,  2.70it/s]Evaluating:  39%|███▉      | 832/2132 [05:05<08:01,  2.70it/s]Evaluating:  39%|███▉      | 833/2132 [05:05<08:10,  2.65it/s]Evaluating:  39%|███▉      | 834/2132 [05:05<08:07,  2.66it/s]Evaluating:  39%|███▉      | 835/2132 [05:06<08:05,  2.67it/s]Evaluating:  39%|███▉      | 836/2132 [05:06<08:03,  2.68it/s]Evaluating:  39%|███▉      | 837/2132 [05:07<08:03,  2.68it/s]Evaluating:  39%|███▉      | 838/2132 [05:07<08:01,  2.69it/s]Evaluating:  39%|███▉      | 839/2132 [05:07<08:00,  2.69it/s]Evaluating:  39%|███▉      | 840/2132 [05:08<07:59,  2.70it/s]Evaluating:  39%|███▉      | 841/2132 [05:08<07:57,  2.70it/s]Evaluating:  39%|███▉      | 842/2132 [05:08<07:56,  2.71it/s]Evaluating:  40%|███▉      | 843/2132 [05:09<07:55,  2.71it/s]Evaluating:  40%|███▉      | 844/2132 [05:09<07:54,  2.72it/s]Evaluating:  40%|███▉      | 845/2132 [05:09<07:52,  2.72it/s]Evaluating:  40%|███▉      | 846/2132 [05:10<07:51,  2.73it/s]Evaluating:  40%|███▉      | 847/2132 [05:10<07:52,  2.72it/s]Evaluating:  40%|███▉      | 848/2132 [05:11<07:51,  2.72it/s]Evaluating:  40%|███▉      | 849/2132 [05:11<07:51,  2.72it/s]Evaluating:  40%|███▉      | 850/2132 [05:11<07:51,  2.72it/s]Evaluating:  40%|███▉      | 851/2132 [05:12<07:49,  2.73it/s]Evaluating:  40%|███▉      | 852/2132 [05:12<07:49,  2.72it/s]Evaluating:  40%|████      | 853/2132 [05:12<07:49,  2.72it/s]Evaluating:  40%|████      | 854/2132 [05:13<07:49,  2.72it/s]Evaluating:  40%|████      | 855/2132 [05:13<07:48,  2.73it/s]Evaluating:  40%|████      | 856/2132 [05:13<07:48,  2.72it/s]Evaluating:  40%|████      | 857/2132 [05:14<07:49,  2.72it/s]Evaluating:  40%|████      | 858/2132 [05:14<07:49,  2.71it/s]Evaluating:  40%|████      | 859/2132 [05:15<07:49,  2.71it/s]Evaluating:  40%|████      | 860/2132 [05:15<07:48,  2.71it/s]Evaluating:  40%|████      | 861/2132 [05:15<07:47,  2.72it/s]Evaluating:  40%|████      | 862/2132 [05:16<07:47,  2.71it/s]Evaluating:  40%|████      | 863/2132 [05:16<07:46,  2.72it/s]Evaluating:  41%|████      | 864/2132 [05:16<07:47,  2.71it/s]Evaluating:  41%|████      | 865/2132 [05:17<07:48,  2.70it/s]Evaluating:  41%|████      | 866/2132 [05:17<07:46,  2.72it/s]Evaluating:  41%|████      | 867/2132 [05:18<08:01,  2.63it/s]Evaluating:  41%|████      | 868/2132 [05:18<07:56,  2.65it/s]Evaluating:  41%|████      | 869/2132 [05:18<07:55,  2.66it/s]Evaluating:  41%|████      | 870/2132 [05:19<07:55,  2.65it/s]Evaluating:  41%|████      | 871/2132 [05:19<07:53,  2.67it/s]Evaluating:  41%|████      | 872/2132 [05:19<07:50,  2.68it/s]Evaluating:  41%|████      | 873/2132 [05:20<07:48,  2.69it/s]Evaluating:  41%|████      | 874/2132 [05:20<07:46,  2.70it/s]Evaluating:  41%|████      | 875/2132 [05:21<07:45,  2.70it/s]Evaluating:  41%|████      | 876/2132 [05:21<07:45,  2.70it/s]Evaluating:  41%|████      | 877/2132 [05:21<07:44,  2.70it/s]Evaluating:  41%|████      | 878/2132 [05:22<07:46,  2.69it/s]Evaluating:  41%|████      | 879/2132 [05:22<07:45,  2.69it/s]Evaluating:  41%|████▏     | 880/2132 [05:22<07:46,  2.68it/s]Evaluating:  41%|████▏     | 881/2132 [05:23<07:46,  2.68it/s]Evaluating:  41%|████▏     | 882/2132 [05:23<07:45,  2.68it/s]Evaluating:  41%|████▏     | 883/2132 [05:24<07:44,  2.69it/s]Evaluating:  41%|████▏     | 884/2132 [05:24<07:42,  2.70it/s]Evaluating:  42%|████▏     | 885/2132 [05:24<07:42,  2.70it/s]Evaluating:  42%|████▏     | 886/2132 [05:25<07:41,  2.70it/s]Evaluating:  42%|████▏     | 887/2132 [05:25<07:40,  2.70it/s]Evaluating:  42%|████▏     | 888/2132 [05:25<07:39,  2.70it/s]Evaluating:  42%|████▏     | 889/2132 [05:26<07:39,  2.71it/s]Evaluating:  42%|████▏     | 890/2132 [05:26<07:40,  2.70it/s]Evaluating:  42%|████▏     | 891/2132 [05:26<07:38,  2.71it/s]Evaluating:  42%|████▏     | 892/2132 [05:27<07:36,  2.72it/s]Evaluating:  42%|████▏     | 893/2132 [05:27<07:36,  2.72it/s]Evaluating:  42%|████▏     | 894/2132 [05:28<07:34,  2.72it/s]Evaluating:  42%|████▏     | 895/2132 [05:28<07:36,  2.71it/s]Evaluating:  42%|████▏     | 896/2132 [05:28<07:34,  2.72it/s]Evaluating:  42%|████▏     | 897/2132 [05:29<07:35,  2.71it/s]Evaluating:  42%|████▏     | 898/2132 [05:29<07:34,  2.72it/s]Evaluating:  42%|████▏     | 899/2132 [05:29<07:33,  2.72it/s]Evaluating:  42%|████▏     | 900/2132 [05:30<07:33,  2.72it/s]Evaluating:  42%|████▏     | 901/2132 [05:30<07:33,  2.71it/s]Evaluating:  42%|████▏     | 902/2132 [05:31<07:33,  2.71it/s]Evaluating:  42%|████▏     | 903/2132 [05:31<07:34,  2.71it/s]Evaluating:  42%|████▏     | 904/2132 [05:31<07:33,  2.71it/s]Evaluating:  42%|████▏     | 905/2132 [05:32<07:32,  2.71it/s]Evaluating:  42%|████▏     | 906/2132 [05:32<07:32,  2.71it/s]Evaluating:  43%|████▎     | 907/2132 [05:32<07:30,  2.72it/s]Evaluating:  43%|████▎     | 908/2132 [05:33<07:28,  2.73it/s]Evaluating:  43%|████▎     | 909/2132 [05:33<07:28,  2.73it/s]Evaluating:  43%|████▎     | 910/2132 [05:33<07:26,  2.73it/s]Evaluating:  43%|████▎     | 911/2132 [05:34<07:26,  2.74it/s]Evaluating:  43%|████▎     | 912/2132 [05:34<07:26,  2.73it/s]Evaluating:  43%|████▎     | 913/2132 [05:35<07:26,  2.73it/s]Evaluating:  43%|████▎     | 914/2132 [05:35<07:25,  2.73it/s]Evaluating:  43%|████▎     | 915/2132 [05:35<07:25,  2.73it/s]Evaluating:  43%|████▎     | 916/2132 [05:36<07:24,  2.73it/s]Evaluating:  43%|████▎     | 917/2132 [05:36<07:22,  2.74it/s]Evaluating:  43%|████▎     | 918/2132 [05:36<07:24,  2.73it/s]Evaluating:  43%|████▎     | 919/2132 [05:37<07:24,  2.73it/s]Evaluating:  43%|████▎     | 920/2132 [05:37<07:24,  2.73it/s]Evaluating:  43%|████▎     | 921/2132 [05:38<07:25,  2.72it/s]Evaluating:  43%|████▎     | 922/2132 [05:38<07:25,  2.71it/s]Evaluating:  43%|████▎     | 923/2132 [05:38<07:25,  2.71it/s]Evaluating:  43%|████▎     | 924/2132 [05:39<07:24,  2.72it/s]Evaluating:  43%|████▎     | 925/2132 [05:39<07:23,  2.72it/s]Evaluating:  43%|████▎     | 926/2132 [05:39<07:23,  2.72it/s]Evaluating:  43%|████▎     | 927/2132 [05:40<07:23,  2.72it/s]Evaluating:  44%|████▎     | 928/2132 [05:40<07:23,  2.72it/s]Evaluating:  44%|████▎     | 929/2132 [05:40<07:23,  2.71it/s]Evaluating:  44%|████▎     | 930/2132 [05:41<07:22,  2.72it/s]Evaluating:  44%|████▎     | 931/2132 [05:41<07:23,  2.71it/s]Evaluating:  44%|████▎     | 932/2132 [05:42<07:21,  2.72it/s]Evaluating:  44%|████▍     | 933/2132 [05:42<07:20,  2.72it/s]Evaluating:  44%|████▍     | 934/2132 [05:42<07:19,  2.72it/s]Evaluating:  44%|████▍     | 935/2132 [05:43<07:18,  2.73it/s]Evaluating:  44%|████▍     | 936/2132 [05:43<07:18,  2.73it/s]Evaluating:  44%|████▍     | 937/2132 [05:43<07:17,  2.73it/s]Evaluating:  44%|████▍     | 938/2132 [05:44<07:17,  2.73it/s]Evaluating:  44%|████▍     | 939/2132 [05:44<07:18,  2.72it/s]Evaluating:  44%|████▍     | 940/2132 [05:44<07:18,  2.72it/s]Evaluating:  44%|████▍     | 941/2132 [05:45<07:17,  2.72it/s]Evaluating:  44%|████▍     | 942/2132 [05:45<07:18,  2.71it/s]Evaluating:  44%|████▍     | 943/2132 [05:46<07:17,  2.72it/s]Evaluating:  44%|████▍     | 944/2132 [05:46<07:16,  2.72it/s]Evaluating:  44%|████▍     | 945/2132 [05:46<07:14,  2.73it/s]Evaluating:  44%|████▍     | 946/2132 [05:47<07:15,  2.72it/s]Evaluating:  44%|████▍     | 947/2132 [05:47<07:14,  2.73it/s]Evaluating:  44%|████▍     | 948/2132 [05:47<07:13,  2.73it/s]Evaluating:  45%|████▍     | 949/2132 [05:48<07:13,  2.73it/s]Evaluating:  45%|████▍     | 950/2132 [05:48<07:15,  2.71it/s]Evaluating:  45%|████▍     | 951/2132 [05:49<07:13,  2.72it/s]Evaluating:  45%|████▍     | 952/2132 [05:49<07:12,  2.73it/s]Evaluating:  45%|████▍     | 953/2132 [05:49<07:12,  2.73it/s]Evaluating:  45%|████▍     | 954/2132 [05:50<07:11,  2.73it/s]Evaluating:  45%|████▍     | 955/2132 [05:50<07:09,  2.74it/s]Evaluating:  45%|████▍     | 956/2132 [05:50<07:10,  2.73it/s]Evaluating:  45%|████▍     | 957/2132 [05:51<07:10,  2.73it/s]Evaluating:  45%|████▍     | 958/2132 [05:51<07:09,  2.74it/s]Evaluating:  45%|████▍     | 959/2132 [05:51<07:08,  2.74it/s]Evaluating:  45%|████▌     | 960/2132 [05:52<07:08,  2.74it/s]Evaluating:  45%|████▌     | 961/2132 [05:52<07:09,  2.73it/s]Evaluating:  45%|████▌     | 962/2132 [05:53<07:09,  2.73it/s]Evaluating:  45%|████▌     | 963/2132 [05:53<07:08,  2.73it/s]Evaluating:  45%|████▌     | 964/2132 [05:53<07:07,  2.73it/s]Evaluating:  45%|████▌     | 965/2132 [05:54<07:07,  2.73it/s]Evaluating:  45%|████▌     | 966/2132 [05:54<07:07,  2.73it/s]Evaluating:  45%|████▌     | 967/2132 [05:54<07:07,  2.72it/s]Evaluating:  45%|████▌     | 968/2132 [05:55<07:09,  2.71it/s]Evaluating:  45%|████▌     | 969/2132 [05:55<07:10,  2.70it/s]Evaluating:  45%|████▌     | 970/2132 [05:56<07:10,  2.70it/s]Evaluating:  46%|████▌     | 971/2132 [05:56<07:10,  2.70it/s]Evaluating:  46%|████▌     | 972/2132 [05:56<07:08,  2.71it/s]Evaluating:  46%|████▌     | 973/2132 [05:57<07:05,  2.72it/s]Evaluating:  46%|████▌     | 974/2132 [05:57<07:04,  2.73it/s]Evaluating:  46%|████▌     | 975/2132 [05:57<07:04,  2.73it/s]Evaluating:  46%|████▌     | 976/2132 [05:58<07:05,  2.72it/s]Evaluating:  46%|████▌     | 977/2132 [05:58<07:05,  2.71it/s]Evaluating:  46%|████▌     | 978/2132 [05:58<07:06,  2.71it/s]Evaluating:  46%|████▌     | 979/2132 [05:59<07:05,  2.71it/s]Evaluating:  46%|████▌     | 980/2132 [05:59<07:03,  2.72it/s]Evaluating:  46%|████▌     | 981/2132 [06:00<07:03,  2.72it/s]Evaluating:  46%|████▌     | 982/2132 [06:00<07:04,  2.71it/s]Evaluating:  46%|████▌     | 983/2132 [06:00<07:03,  2.71it/s]Evaluating:  46%|████▌     | 984/2132 [06:01<07:02,  2.72it/s]Evaluating:  46%|████▌     | 985/2132 [06:01<07:01,  2.72it/s]Evaluating:  46%|████▌     | 986/2132 [06:01<06:59,  2.73it/s]Evaluating:  46%|████▋     | 987/2132 [06:02<06:59,  2.73it/s]Evaluating:  46%|████▋     | 988/2132 [06:02<06:59,  2.73it/s]Evaluating:  46%|████▋     | 989/2132 [06:02<06:59,  2.73it/s]Evaluating:  46%|████▋     | 990/2132 [06:03<06:58,  2.73it/s]Evaluating:  46%|████▋     | 991/2132 [06:03<06:57,  2.73it/s]Evaluating:  47%|████▋     | 992/2132 [06:04<06:57,  2.73it/s]Evaluating:  47%|████▋     | 993/2132 [06:04<06:58,  2.72it/s]Evaluating:  47%|████▋     | 994/2132 [06:04<06:59,  2.71it/s]Evaluating:  47%|████▋     | 995/2132 [06:05<06:59,  2.71it/s]Evaluating:  47%|████▋     | 996/2132 [06:05<07:00,  2.70it/s]Evaluating:  47%|████▋     | 997/2132 [06:05<06:59,  2.71it/s]Evaluating:  47%|████▋     | 998/2132 [06:06<06:57,  2.72it/s]Evaluating:  47%|████▋     | 999/2132 [06:06<06:57,  2.71it/s]Evaluating:  47%|████▋     | 1000/2132 [06:07<06:56,  2.72it/s]Evaluating:  47%|████▋     | 1001/2132 [06:07<06:55,  2.72it/s]Evaluating:  47%|████▋     | 1002/2132 [06:07<06:55,  2.72it/s]Evaluating:  47%|████▋     | 1003/2132 [06:08<06:56,  2.71it/s]Evaluating:  47%|████▋     | 1004/2132 [06:08<06:55,  2.72it/s]Evaluating:  47%|████▋     | 1005/2132 [06:08<06:55,  2.71it/s]Evaluating:  47%|████▋     | 1006/2132 [06:09<06:54,  2.72it/s]Evaluating:  47%|████▋     | 1007/2132 [06:09<06:53,  2.72it/s]Evaluating:  47%|████▋     | 1008/2132 [06:09<06:51,  2.73it/s]Evaluating:  47%|████▋     | 1009/2132 [06:10<06:51,  2.73it/s]Evaluating:  47%|████▋     | 1010/2132 [06:10<06:50,  2.73it/s]Evaluating:  47%|████▋     | 1011/2132 [06:11<06:53,  2.71it/s]Evaluating:  47%|████▋     | 1012/2132 [06:11<06:52,  2.71it/s]Evaluating:  48%|████▊     | 1013/2132 [06:11<06:52,  2.72it/s]Evaluating:  48%|████▊     | 1014/2132 [06:12<06:53,  2.71it/s]Evaluating:  48%|████▊     | 1015/2132 [06:12<06:51,  2.71it/s]Evaluating:  48%|████▊     | 1016/2132 [06:12<06:50,  2.72it/s]Evaluating:  48%|████▊     | 1017/2132 [06:13<06:50,  2.71it/s]Evaluating:  48%|████▊     | 1018/2132 [06:13<06:51,  2.71it/s]Evaluating:  48%|████▊     | 1019/2132 [06:14<06:52,  2.70it/s]Evaluating:  48%|████▊     | 1020/2132 [06:14<06:50,  2.71it/s]Evaluating:  48%|████▊     | 1021/2132 [06:14<06:50,  2.70it/s]Evaluating:  48%|████▊     | 1022/2132 [06:15<06:50,  2.70it/s]Evaluating:  48%|████▊     | 1023/2132 [06:15<06:49,  2.71it/s]Evaluating:  48%|████▊     | 1024/2132 [06:15<06:48,  2.71it/s]Evaluating:  48%|████▊     | 1025/2132 [06:16<06:46,  2.72it/s]Evaluating:  48%|████▊     | 1026/2132 [06:16<06:45,  2.73it/s]Evaluating:  48%|████▊     | 1027/2132 [06:16<06:44,  2.73it/s]Evaluating:  48%|████▊     | 1028/2132 [06:17<06:44,  2.73it/s]Evaluating:  48%|████▊     | 1029/2132 [06:17<06:45,  2.72it/s]Evaluating:  48%|████▊     | 1030/2132 [06:18<06:43,  2.73it/s]Evaluating:  48%|████▊     | 1031/2132 [06:18<06:43,  2.73it/s]Evaluating:  48%|████▊     | 1032/2132 [06:18<06:42,  2.73it/s]Evaluating:  48%|████▊     | 1033/2132 [06:19<06:41,  2.74it/s]Evaluating:  48%|████▊     | 1034/2132 [06:19<06:41,  2.74it/s]Evaluating:  49%|████▊     | 1035/2132 [06:19<06:42,  2.73it/s]Evaluating:  49%|████▊     | 1036/2132 [06:20<06:41,  2.73it/s]Evaluating:  49%|████▊     | 1037/2132 [06:20<06:42,  2.72it/s]Evaluating:  49%|████▊     | 1038/2132 [06:21<06:42,  2.72it/s]Evaluating:  49%|████▊     | 1039/2132 [06:21<06:42,  2.72it/s]Evaluating:  49%|████▉     | 1040/2132 [06:21<06:40,  2.73it/s]Evaluating:  49%|████▉     | 1041/2132 [06:22<06:39,  2.73it/s]Evaluating:  49%|████▉     | 1042/2132 [06:22<06:40,  2.72it/s]Evaluating:  49%|████▉     | 1043/2132 [06:22<06:39,  2.73it/s]Evaluating:  49%|████▉     | 1044/2132 [06:23<06:39,  2.72it/s]Evaluating:  49%|████▉     | 1045/2132 [06:23<06:40,  2.71it/s]Evaluating:  49%|████▉     | 1046/2132 [06:23<06:41,  2.71it/s]Evaluating:  49%|████▉     | 1047/2132 [06:24<06:41,  2.70it/s]Evaluating:  49%|████▉     | 1048/2132 [06:24<06:42,  2.69it/s]Evaluating:  49%|████▉     | 1049/2132 [06:25<06:41,  2.70it/s]Evaluating:  49%|████▉     | 1050/2132 [06:25<06:39,  2.71it/s]Evaluating:  49%|████▉     | 1051/2132 [06:25<06:37,  2.72it/s]Evaluating:  49%|████▉     | 1052/2132 [06:26<06:37,  2.71it/s]Evaluating:  49%|████▉     | 1053/2132 [06:26<06:37,  2.71it/s]Evaluating:  49%|████▉     | 1054/2132 [06:26<06:37,  2.71it/s]Evaluating:  49%|████▉     | 1055/2132 [06:27<06:36,  2.71it/s]Evaluating:  50%|████▉     | 1056/2132 [06:27<06:35,  2.72it/s]Evaluating:  50%|████▉     | 1057/2132 [06:28<06:35,  2.72it/s]Evaluating:  50%|████▉     | 1058/2132 [06:28<06:35,  2.71it/s]Evaluating:  50%|████▉     | 1059/2132 [06:28<06:35,  2.71it/s]Evaluating:  50%|████▉     | 1060/2132 [06:29<06:38,  2.69it/s]Evaluating:  50%|████▉     | 1061/2132 [06:29<06:37,  2.70it/s]Evaluating:  50%|████▉     | 1062/2132 [06:29<06:35,  2.71it/s]Evaluating:  50%|████▉     | 1063/2132 [06:30<06:35,  2.71it/s]Evaluating:  50%|████▉     | 1064/2132 [06:30<06:34,  2.70it/s]Evaluating:  50%|████▉     | 1065/2132 [06:30<06:33,  2.71it/s]Evaluating:  50%|█████     | 1066/2132 [06:31<06:33,  2.71it/s]Evaluating:  50%|█████     | 1067/2132 [06:31<06:32,  2.72it/s]Evaluating:  50%|█████     | 1068/2132 [06:32<06:32,  2.71it/s]Evaluating:  50%|█████     | 1069/2132 [06:32<06:31,  2.71it/s]Evaluating:  50%|█████     | 1070/2132 [06:32<06:32,  2.71it/s]Evaluating:  50%|█████     | 1071/2132 [06:33<06:32,  2.70it/s]Evaluating:  50%|█████     | 1072/2132 [06:33<06:32,  2.70it/s]Evaluating:  50%|█████     | 1073/2132 [06:33<06:31,  2.70it/s]Evaluating:  50%|█████     | 1074/2132 [06:34<06:30,  2.71it/s]Evaluating:  50%|█████     | 1075/2132 [06:34<06:29,  2.71it/s]Evaluating:  50%|█████     | 1076/2132 [06:35<06:28,  2.72it/s]Evaluating:  51%|█████     | 1077/2132 [06:35<06:29,  2.71it/s]Evaluating:  51%|█████     | 1078/2132 [06:35<06:27,  2.72it/s]Evaluating:  51%|█████     | 1079/2132 [06:36<06:27,  2.72it/s]Evaluating:  51%|█████     | 1080/2132 [06:36<06:26,  2.72it/s]Evaluating:  51%|█████     | 1081/2132 [06:36<06:28,  2.70it/s]Evaluating:  51%|█████     | 1082/2132 [06:37<06:28,  2.70it/s]Evaluating:  51%|█████     | 1083/2132 [06:37<06:40,  2.62it/s]Evaluating:  51%|█████     | 1084/2132 [06:38<06:37,  2.64it/s]Evaluating:  51%|█████     | 1085/2132 [06:38<06:34,  2.66it/s]Evaluating:  51%|█████     | 1086/2132 [06:38<06:33,  2.66it/s]Evaluating:  51%|█████     | 1087/2132 [06:39<06:30,  2.68it/s]Evaluating:  51%|█████     | 1088/2132 [06:39<06:29,  2.68it/s]Evaluating:  51%|█████     | 1089/2132 [06:39<06:27,  2.69it/s]Evaluating:  51%|█████     | 1090/2132 [06:40<06:25,  2.70it/s]Evaluating:  51%|█████     | 1091/2132 [06:40<06:23,  2.72it/s]Evaluating:  51%|█████     | 1092/2132 [06:40<06:22,  2.72it/s]Evaluating:  51%|█████▏    | 1093/2132 [06:41<06:20,  2.73it/s]Evaluating:  51%|█████▏    | 1094/2132 [06:41<06:21,  2.72it/s]Evaluating:  51%|█████▏    | 1095/2132 [06:42<06:20,  2.72it/s]Evaluating:  51%|█████▏    | 1096/2132 [06:42<06:19,  2.73it/s]Evaluating:  51%|█████▏    | 1097/2132 [06:42<06:18,  2.73it/s]Evaluating:  52%|█████▏    | 1098/2132 [06:43<06:20,  2.72it/s]Evaluating:  52%|█████▏    | 1099/2132 [06:43<06:19,  2.73it/s]Evaluating:  52%|█████▏    | 1100/2132 [06:43<06:20,  2.71it/s]Evaluating:  52%|█████▏    | 1101/2132 [06:44<06:21,  2.71it/s]Evaluating:  52%|█████▏    | 1102/2132 [06:44<06:19,  2.71it/s]Evaluating:  52%|█████▏    | 1103/2132 [06:45<06:19,  2.71it/s]Evaluating:  52%|█████▏    | 1104/2132 [06:45<06:17,  2.72it/s]Evaluating:  52%|█████▏    | 1105/2132 [06:45<06:17,  2.72it/s]Evaluating:  52%|█████▏    | 1106/2132 [06:46<06:17,  2.72it/s]Evaluating:  52%|█████▏    | 1107/2132 [06:46<06:17,  2.72it/s]Evaluating:  52%|█████▏    | 1108/2132 [06:46<06:15,  2.73it/s]Evaluating:  52%|█████▏    | 1109/2132 [06:47<06:15,  2.73it/s]Evaluating:  52%|█████▏    | 1110/2132 [06:47<06:14,  2.73it/s]Evaluating:  52%|█████▏    | 1111/2132 [06:47<06:13,  2.74it/s]Evaluating:  52%|█████▏    | 1112/2132 [06:48<06:14,  2.72it/s]Evaluating:  52%|█████▏    | 1113/2132 [06:48<06:16,  2.71it/s]Evaluating:  52%|█████▏    | 1114/2132 [06:49<06:15,  2.71it/s]Evaluating:  52%|█████▏    | 1115/2132 [06:49<06:13,  2.72it/s]Evaluating:  52%|█████▏    | 1116/2132 [06:49<06:12,  2.72it/s]Evaluating:  52%|█████▏    | 1117/2132 [06:50<06:12,  2.73it/s]Evaluating:  52%|█████▏    | 1118/2132 [06:50<06:11,  2.73it/s]Evaluating:  52%|█████▏    | 1119/2132 [06:50<06:12,  2.72it/s]Evaluating:  53%|█████▎    | 1120/2132 [06:51<06:12,  2.72it/s]Evaluating:  53%|█████▎    | 1121/2132 [06:51<06:11,  2.72it/s]Evaluating:  53%|█████▎    | 1122/2132 [06:51<06:11,  2.72it/s]Evaluating:  53%|█████▎    | 1123/2132 [06:52<06:09,  2.73it/s]Evaluating:  53%|█████▎    | 1124/2132 [06:52<06:09,  2.73it/s]Evaluating:  53%|█████▎    | 1125/2132 [06:53<06:09,  2.73it/s]Evaluating:  53%|█████▎    | 1126/2132 [06:53<06:09,  2.73it/s]Evaluating:  53%|█████▎    | 1127/2132 [06:53<06:08,  2.73it/s]Evaluating:  53%|█████▎    | 1128/2132 [06:54<06:08,  2.73it/s]Evaluating:  53%|█████▎    | 1129/2132 [06:54<06:09,  2.72it/s]Evaluating:  53%|█████▎    | 1130/2132 [06:54<06:08,  2.72it/s]Evaluating:  53%|█████▎    | 1131/2132 [06:55<06:09,  2.71it/s]Evaluating:  53%|█████▎    | 1132/2132 [06:55<06:09,  2.71it/s]Evaluating:  53%|█████▎    | 1133/2132 [06:56<06:08,  2.71it/s]Evaluating:  53%|█████▎    | 1134/2132 [06:56<06:07,  2.71it/s]Evaluating:  53%|█████▎    | 1135/2132 [06:56<06:06,  2.72it/s]Evaluating:  53%|█████▎    | 1136/2132 [06:57<06:07,  2.71it/s]Evaluating:  53%|█████▎    | 1137/2132 [06:57<06:08,  2.70it/s]Evaluating:  53%|█████▎    | 1138/2132 [06:57<06:07,  2.70it/s]Evaluating:  53%|█████▎    | 1139/2132 [06:58<06:05,  2.71it/s]Evaluating:  53%|█████▎    | 1140/2132 [06:58<06:05,  2.72it/s]Evaluating:  54%|█████▎    | 1141/2132 [06:58<06:03,  2.73it/s]Evaluating:  54%|█████▎    | 1142/2132 [06:59<06:03,  2.73it/s]Evaluating:  54%|█████▎    | 1143/2132 [06:59<06:02,  2.73it/s]Evaluating:  54%|█████▎    | 1144/2132 [07:00<06:02,  2.72it/s]Evaluating:  54%|█████▎    | 1145/2132 [07:00<06:02,  2.72it/s]Evaluating:  54%|█████▍    | 1146/2132 [07:00<06:02,  2.72it/s]Evaluating:  54%|█████▍    | 1147/2132 [07:01<06:02,  2.72it/s]Evaluating:  54%|█████▍    | 1148/2132 [07:01<06:01,  2.72it/s]Evaluating:  54%|█████▍    | 1149/2132 [07:01<06:01,  2.72it/s]Evaluating:  54%|█████▍    | 1150/2132 [07:02<06:00,  2.73it/s]Evaluating:  54%|█████▍    | 1151/2132 [07:02<05:58,  2.73it/s]Evaluating:  54%|█████▍    | 1152/2132 [07:03<05:57,  2.74it/s]Evaluating:  54%|█████▍    | 1153/2132 [07:03<05:56,  2.74it/s]Evaluating:  54%|█████▍    | 1154/2132 [07:03<05:57,  2.74it/s]Evaluating:  54%|█████▍    | 1155/2132 [07:04<05:58,  2.72it/s]Evaluating:  54%|█████▍    | 1156/2132 [07:04<05:57,  2.73it/s]Evaluating:  54%|█████▍    | 1157/2132 [07:04<05:57,  2.73it/s]Evaluating:  54%|█████▍    | 1158/2132 [07:05<05:57,  2.72it/s]Evaluating:  54%|█████▍    | 1159/2132 [07:05<05:57,  2.72it/s]Evaluating:  54%|█████▍    | 1160/2132 [07:05<05:56,  2.73it/s]Evaluating:  54%|█████▍    | 1161/2132 [07:06<05:56,  2.73it/s]Evaluating:  55%|█████▍    | 1162/2132 [07:06<05:56,  2.72it/s]Evaluating:  55%|█████▍    | 1163/2132 [07:07<05:55,  2.72it/s]Evaluating:  55%|█████▍    | 1164/2132 [07:07<05:55,  2.72it/s]Evaluating:  55%|█████▍    | 1165/2132 [07:07<05:55,  2.72it/s]Evaluating:  55%|█████▍    | 1166/2132 [07:08<05:53,  2.73it/s]Evaluating:  55%|█████▍    | 1167/2132 [07:08<05:53,  2.73it/s]Evaluating:  55%|█████▍    | 1168/2132 [07:08<05:54,  2.72it/s]Evaluating:  55%|█████▍    | 1169/2132 [07:09<05:55,  2.71it/s]Evaluating:  55%|█████▍    | 1170/2132 [07:09<05:54,  2.71it/s]Evaluating:  55%|█████▍    | 1171/2132 [07:09<05:54,  2.71it/s]Evaluating:  55%|█████▍    | 1172/2132 [07:10<05:53,  2.71it/s]Evaluating:  55%|█████▌    | 1173/2132 [07:10<05:53,  2.71it/s]Evaluating:  55%|█████▌    | 1174/2132 [07:11<05:52,  2.72it/s]Evaluating:  55%|█████▌    | 1175/2132 [07:11<05:50,  2.73it/s]Evaluating:  55%|█████▌    | 1176/2132 [07:11<05:50,  2.73it/s]Evaluating:  55%|█████▌    | 1177/2132 [07:12<05:50,  2.72it/s]Evaluating:  55%|█████▌    | 1178/2132 [07:12<05:49,  2.73it/s]Evaluating:  55%|█████▌    | 1179/2132 [07:12<05:48,  2.74it/s]Evaluating:  55%|█████▌    | 1180/2132 [07:13<05:47,  2.74it/s]Evaluating:  55%|█████▌    | 1181/2132 [07:13<05:47,  2.74it/s]Evaluating:  55%|█████▌    | 1182/2132 [07:14<05:47,  2.73it/s]Evaluating:  55%|█████▌    | 1183/2132 [07:14<05:48,  2.72it/s]Evaluating:  56%|█████▌    | 1184/2132 [07:14<05:47,  2.73it/s]Evaluating:  56%|█████▌    | 1185/2132 [07:15<05:46,  2.74it/s]Evaluating:  56%|█████▌    | 1186/2132 [07:15<05:45,  2.74it/s]Evaluating:  56%|█████▌    | 1187/2132 [07:15<05:46,  2.73it/s]Evaluating:  56%|█████▌    | 1188/2132 [07:16<05:45,  2.73it/s]Evaluating:  56%|█████▌    | 1189/2132 [07:16<05:44,  2.74it/s]Evaluating:  56%|█████▌    | 1190/2132 [07:16<05:45,  2.73it/s]Evaluating:  56%|█████▌    | 1191/2132 [07:17<05:46,  2.72it/s]Evaluating:  56%|█████▌    | 1192/2132 [07:17<05:45,  2.72it/s]Evaluating:  56%|█████▌    | 1193/2132 [07:18<05:46,  2.71it/s]Evaluating:  56%|█████▌    | 1194/2132 [07:18<05:46,  2.70it/s]Evaluating:  56%|█████▌    | 1195/2132 [07:18<05:46,  2.71it/s]Evaluating:  56%|█████▌    | 1196/2132 [07:19<05:44,  2.72it/s]Evaluating:  56%|█████▌    | 1197/2132 [07:19<05:45,  2.71it/s]Evaluating:  56%|█████▌    | 1198/2132 [07:19<05:44,  2.71it/s]Evaluating:  56%|█████▌    | 1199/2132 [07:20<05:43,  2.72it/s]Evaluating:  56%|█████▋    | 1200/2132 [07:20<05:43,  2.72it/s]Evaluating:  56%|█████▋    | 1201/2132 [07:21<05:41,  2.72it/s]Evaluating:  56%|█████▋    | 1202/2132 [07:21<05:40,  2.73it/s]Evaluating:  56%|█████▋    | 1203/2132 [07:21<05:40,  2.73it/s]Evaluating:  56%|█████▋    | 1204/2132 [07:22<05:40,  2.72it/s]Evaluating:  57%|█████▋    | 1205/2132 [07:22<05:40,  2.72it/s]Evaluating:  57%|█████▋    | 1206/2132 [07:22<05:39,  2.73it/s]Evaluating:  57%|█████▋    | 1207/2132 [07:23<05:38,  2.73it/s]Evaluating:  57%|█████▋    | 1208/2132 [07:23<05:38,  2.73it/s]Evaluating:  57%|█████▋    | 1209/2132 [07:23<05:38,  2.73it/s]Evaluating:  57%|█████▋    | 1210/2132 [07:24<05:38,  2.73it/s]Evaluating:  57%|█████▋    | 1211/2132 [07:24<05:38,  2.72it/s]Evaluating:  57%|█████▋    | 1212/2132 [07:25<05:37,  2.72it/s]Evaluating:  57%|█████▋    | 1213/2132 [07:25<05:36,  2.73it/s]Evaluating:  57%|█████▋    | 1214/2132 [07:25<05:36,  2.73it/s]Evaluating:  57%|█████▋    | 1215/2132 [07:26<05:36,  2.72it/s]Evaluating:  57%|█████▋    | 1216/2132 [07:26<05:37,  2.72it/s]Evaluating:  57%|█████▋    | 1217/2132 [07:26<05:36,  2.72it/s]Evaluating:  57%|█████▋    | 1218/2132 [07:27<05:35,  2.72it/s]Evaluating:  57%|█████▋    | 1219/2132 [07:27<05:34,  2.73it/s]Evaluating:  57%|█████▋    | 1220/2132 [07:27<05:34,  2.72it/s]Evaluating:  57%|█████▋    | 1221/2132 [07:28<05:34,  2.72it/s]Evaluating:  57%|█████▋    | 1222/2132 [07:28<05:34,  2.72it/s]Evaluating:  57%|█████▋    | 1223/2132 [07:29<05:34,  2.72it/s]Evaluating:  57%|█████▋    | 1224/2132 [07:29<05:34,  2.72it/s]Evaluating:  57%|█████▋    | 1225/2132 [07:29<05:33,  2.72it/s]Evaluating:  58%|█████▊    | 1226/2132 [07:30<05:32,  2.73it/s]Evaluating:  58%|█████▊    | 1227/2132 [07:30<05:32,  2.72it/s]Evaluating:  58%|█████▊    | 1228/2132 [07:30<05:32,  2.72it/s]Evaluating:  58%|█████▊    | 1229/2132 [07:31<05:32,  2.71it/s]Evaluating:  58%|█████▊    | 1230/2132 [07:31<05:31,  2.72it/s]Evaluating:  58%|█████▊    | 1231/2132 [07:32<05:31,  2.72it/s]Evaluating:  58%|█████▊    | 1232/2132 [07:32<05:30,  2.72it/s]Evaluating:  58%|█████▊    | 1233/2132 [07:32<05:30,  2.72it/s]Evaluating:  58%|█████▊    | 1234/2132 [07:33<05:29,  2.72it/s]Evaluating:  58%|█████▊    | 1235/2132 [07:33<05:28,  2.73it/s]Evaluating:  58%|█████▊    | 1236/2132 [07:33<05:28,  2.73it/s]Evaluating:  58%|█████▊    | 1237/2132 [07:34<05:29,  2.72it/s]Evaluating:  58%|█████▊    | 1238/2132 [07:34<05:28,  2.72it/s]Evaluating:  58%|█████▊    | 1239/2132 [07:34<05:28,  2.72it/s]Evaluating:  58%|█████▊    | 1240/2132 [07:35<05:28,  2.72it/s]Evaluating:  58%|█████▊    | 1241/2132 [07:35<05:27,  2.72it/s]Evaluating:  58%|█████▊    | 1242/2132 [07:36<05:26,  2.73it/s]Evaluating:  58%|█████▊    | 1243/2132 [07:36<05:26,  2.73it/s]Evaluating:  58%|█████▊    | 1244/2132 [07:36<05:26,  2.72it/s]Evaluating:  58%|█████▊    | 1245/2132 [07:37<05:26,  2.72it/s]Evaluating:  58%|█████▊    | 1246/2132 [07:37<05:27,  2.71it/s]Evaluating:  58%|█████▊    | 1247/2132 [07:37<05:27,  2.70it/s]Evaluating:  59%|█████▊    | 1248/2132 [07:38<05:27,  2.70it/s]Evaluating:  59%|█████▊    | 1249/2132 [07:38<05:25,  2.71it/s]Evaluating:  59%|█████▊    | 1250/2132 [07:39<05:24,  2.72it/s]Evaluating:  59%|█████▊    | 1251/2132 [07:39<05:23,  2.72it/s]Evaluating:  59%|█████▊    | 1252/2132 [07:39<05:21,  2.73it/s]Evaluating:  59%|█████▉    | 1253/2132 [07:40<05:21,  2.73it/s]Evaluating:  59%|█████▉    | 1254/2132 [07:40<05:20,  2.74it/s]Evaluating:  59%|█████▉    | 1255/2132 [07:40<05:19,  2.74it/s]Evaluating:  59%|█████▉    | 1256/2132 [07:41<05:19,  2.74it/s]Evaluating:  59%|█████▉    | 1257/2132 [07:41<05:20,  2.73it/s]Evaluating:  59%|█████▉    | 1258/2132 [07:41<05:20,  2.73it/s]Evaluating:  59%|█████▉    | 1259/2132 [07:42<05:21,  2.72it/s]Evaluating:  59%|█████▉    | 1260/2132 [07:42<05:19,  2.73it/s]Evaluating:  59%|█████▉    | 1261/2132 [07:43<05:18,  2.74it/s]Evaluating:  59%|█████▉    | 1262/2132 [07:43<05:18,  2.73it/s]Evaluating:  59%|█████▉    | 1263/2132 [07:43<05:20,  2.71it/s]Evaluating:  59%|█████▉    | 1264/2132 [07:44<05:20,  2.71it/s]Evaluating:  59%|█████▉    | 1265/2132 [07:44<05:20,  2.70it/s]Evaluating:  59%|█████▉    | 1266/2132 [07:44<05:20,  2.70it/s]Evaluating:  59%|█████▉    | 1267/2132 [07:45<05:20,  2.70it/s]Evaluating:  59%|█████▉    | 1268/2132 [07:45<05:20,  2.69it/s]Evaluating:  60%|█████▉    | 1269/2132 [07:45<05:18,  2.71it/s]Evaluating:  60%|█████▉    | 1270/2132 [07:46<05:18,  2.71it/s]Evaluating:  60%|█████▉    | 1271/2132 [07:46<05:19,  2.70it/s]Evaluating:  60%|█████▉    | 1272/2132 [07:47<05:17,  2.71it/s]Evaluating:  60%|█████▉    | 1273/2132 [07:47<05:17,  2.71it/s]Evaluating:  60%|█████▉    | 1274/2132 [07:47<05:18,  2.69it/s]Evaluating:  60%|█████▉    | 1275/2132 [07:48<05:18,  2.69it/s]Evaluating:  60%|█████▉    | 1276/2132 [07:48<05:17,  2.69it/s]Evaluating:  60%|█████▉    | 1277/2132 [07:48<05:19,  2.68it/s]Evaluating:  60%|█████▉    | 1278/2132 [07:49<05:18,  2.68it/s]Evaluating:  60%|█████▉    | 1279/2132 [07:49<05:16,  2.70it/s]Evaluating:  60%|██████    | 1280/2132 [07:50<05:15,  2.70it/s]Evaluating:  60%|██████    | 1281/2132 [07:50<05:16,  2.69it/s]Evaluating:  60%|██████    | 1282/2132 [07:50<05:15,  2.69it/s]Evaluating:  60%|██████    | 1283/2132 [07:51<05:14,  2.70it/s]Evaluating:  60%|██████    | 1284/2132 [07:51<05:14,  2.69it/s]Evaluating:  60%|██████    | 1285/2132 [07:51<05:14,  2.69it/s]Evaluating:  60%|██████    | 1286/2132 [07:52<05:13,  2.70it/s]Evaluating:  60%|██████    | 1287/2132 [07:52<05:12,  2.70it/s]Evaluating:  60%|██████    | 1288/2132 [07:53<05:12,  2.70it/s]Evaluating:  60%|██████    | 1289/2132 [07:53<05:12,  2.70it/s]Evaluating:  61%|██████    | 1290/2132 [07:53<05:11,  2.70it/s]Evaluating:  61%|██████    | 1291/2132 [07:54<05:09,  2.71it/s]Evaluating:  61%|██████    | 1292/2132 [07:54<05:08,  2.73it/s]Evaluating:  61%|██████    | 1293/2132 [07:54<05:08,  2.72it/s]Evaluating:  61%|██████    | 1294/2132 [07:55<05:09,  2.71it/s]Evaluating:  61%|██████    | 1295/2132 [07:55<05:09,  2.70it/s]Evaluating:  61%|██████    | 1296/2132 [07:56<05:10,  2.70it/s]Evaluating:  61%|██████    | 1297/2132 [07:56<05:08,  2.71it/s]Evaluating:  61%|██████    | 1298/2132 [07:56<05:08,  2.71it/s]Evaluating:  61%|██████    | 1299/2132 [07:57<05:15,  2.64it/s]Evaluating:  61%|██████    | 1300/2132 [07:57<05:11,  2.67it/s]Evaluating:  61%|██████    | 1301/2132 [07:57<05:09,  2.69it/s]Evaluating:  61%|██████    | 1302/2132 [07:58<05:07,  2.70it/s]Evaluating:  61%|██████    | 1303/2132 [07:58<05:06,  2.71it/s]Evaluating:  61%|██████    | 1304/2132 [07:58<05:05,  2.71it/s]Evaluating:  61%|██████    | 1305/2132 [07:59<05:05,  2.71it/s]Evaluating:  61%|██████▏   | 1306/2132 [07:59<05:05,  2.70it/s]Evaluating:  61%|██████▏   | 1307/2132 [08:00<05:04,  2.71it/s]Evaluating:  61%|██████▏   | 1308/2132 [08:00<05:03,  2.72it/s]Evaluating:  61%|██████▏   | 1309/2132 [08:00<05:02,  2.72it/s]Evaluating:  61%|██████▏   | 1310/2132 [08:01<05:02,  2.72it/s]Evaluating:  61%|██████▏   | 1311/2132 [08:01<05:02,  2.71it/s]Evaluating:  62%|██████▏   | 1312/2132 [08:01<05:01,  2.72it/s]Evaluating:  62%|██████▏   | 1313/2132 [08:02<05:00,  2.73it/s]Evaluating:  62%|██████▏   | 1314/2132 [08:02<04:59,  2.73it/s]Evaluating:  62%|██████▏   | 1315/2132 [08:03<04:59,  2.73it/s]Evaluating:  62%|██████▏   | 1316/2132 [08:03<04:59,  2.73it/s]Evaluating:  62%|██████▏   | 1317/2132 [08:03<04:59,  2.72it/s]Evaluating:  62%|██████▏   | 1318/2132 [08:04<04:57,  2.73it/s]Evaluating:  62%|██████▏   | 1319/2132 [08:04<04:58,  2.72it/s]Evaluating:  62%|██████▏   | 1320/2132 [08:04<04:58,  2.72it/s]Evaluating:  62%|██████▏   | 1321/2132 [08:05<04:59,  2.71it/s]Evaluating:  62%|██████▏   | 1322/2132 [08:05<04:59,  2.70it/s]Evaluating:  62%|██████▏   | 1323/2132 [08:05<04:58,  2.71it/s]Evaluating:  62%|██████▏   | 1324/2132 [08:06<04:57,  2.72it/s]Evaluating:  62%|██████▏   | 1325/2132 [08:06<04:57,  2.72it/s]Evaluating:  62%|██████▏   | 1326/2132 [08:07<04:56,  2.72it/s]Evaluating:  62%|██████▏   | 1327/2132 [08:07<04:57,  2.71it/s]Evaluating:  62%|██████▏   | 1328/2132 [08:07<04:56,  2.71it/s]Evaluating:  62%|██████▏   | 1329/2132 [08:08<04:56,  2.71it/s]Evaluating:  62%|██████▏   | 1330/2132 [08:08<04:56,  2.71it/s]Evaluating:  62%|██████▏   | 1331/2132 [08:08<04:55,  2.71it/s]Evaluating:  62%|██████▏   | 1332/2132 [08:09<04:55,  2.71it/s]Evaluating:  63%|██████▎   | 1333/2132 [08:09<04:55,  2.71it/s]Evaluating:  63%|██████▎   | 1334/2132 [08:10<04:54,  2.71it/s]Evaluating:  63%|██████▎   | 1335/2132 [08:10<04:53,  2.71it/s]Evaluating:  63%|██████▎   | 1336/2132 [08:10<04:53,  2.71it/s]Evaluating:  63%|██████▎   | 1337/2132 [08:11<04:53,  2.71it/s]Evaluating:  63%|██████▎   | 1338/2132 [08:11<04:54,  2.70it/s]Evaluating:  63%|██████▎   | 1339/2132 [08:11<04:53,  2.70it/s]Evaluating:  63%|██████▎   | 1340/2132 [08:12<04:51,  2.71it/s]Evaluating:  63%|██████▎   | 1341/2132 [08:12<04:51,  2.71it/s]Evaluating:  63%|██████▎   | 1342/2132 [08:12<04:50,  2.72it/s]Evaluating:  63%|██████▎   | 1343/2132 [08:13<04:50,  2.72it/s]Evaluating:  63%|██████▎   | 1344/2132 [08:13<04:48,  2.73it/s]Evaluating:  63%|██████▎   | 1345/2132 [08:14<04:49,  2.72it/s]Evaluating:  63%|██████▎   | 1346/2132 [08:14<04:48,  2.72it/s]Evaluating:  63%|██████▎   | 1347/2132 [08:14<04:48,  2.72it/s]Evaluating:  63%|██████▎   | 1348/2132 [08:15<04:48,  2.72it/s]Evaluating:  63%|██████▎   | 1349/2132 [08:15<04:48,  2.72it/s]Evaluating:  63%|██████▎   | 1350/2132 [08:15<04:47,  2.72it/s]Evaluating:  63%|██████▎   | 1351/2132 [08:16<04:45,  2.73it/s]Evaluating:  63%|██████▎   | 1352/2132 [08:16<04:44,  2.74it/s]Evaluating:  63%|██████▎   | 1353/2132 [08:17<04:44,  2.74it/s]Evaluating:  64%|██████▎   | 1354/2132 [08:17<04:44,  2.73it/s]Evaluating:  64%|██████▎   | 1355/2132 [08:17<04:45,  2.73it/s]Evaluating:  64%|██████▎   | 1356/2132 [08:18<04:44,  2.73it/s]Evaluating:  64%|██████▎   | 1357/2132 [08:18<04:43,  2.73it/s]Evaluating:  64%|██████▎   | 1358/2132 [08:18<04:44,  2.72it/s]Evaluating:  64%|██████▎   | 1359/2132 [08:19<04:44,  2.71it/s]Evaluating:  64%|██████▍   | 1360/2132 [08:19<04:44,  2.72it/s]Evaluating:  64%|██████▍   | 1361/2132 [08:19<04:43,  2.72it/s]Evaluating:  64%|██████▍   | 1362/2132 [08:20<04:44,  2.71it/s]Evaluating:  64%|██████▍   | 1363/2132 [08:20<04:45,  2.70it/s]Evaluating:  64%|██████▍   | 1364/2132 [08:21<04:44,  2.70it/s]Evaluating:  64%|██████▍   | 1365/2132 [08:21<04:43,  2.70it/s]Evaluating:  64%|██████▍   | 1366/2132 [08:21<04:42,  2.71it/s]Evaluating:  64%|██████▍   | 1367/2132 [08:22<04:42,  2.71it/s]Evaluating:  64%|██████▍   | 1368/2132 [08:22<04:42,  2.71it/s]Evaluating:  64%|██████▍   | 1369/2132 [08:22<04:42,  2.70it/s]Evaluating:  64%|██████▍   | 1370/2132 [08:23<04:41,  2.71it/s]Evaluating:  64%|██████▍   | 1371/2132 [08:23<04:41,  2.71it/s]Evaluating:  64%|██████▍   | 1372/2132 [08:24<04:41,  2.70it/s]Evaluating:  64%|██████▍   | 1373/2132 [08:24<04:41,  2.70it/s]Evaluating:  64%|██████▍   | 1374/2132 [08:24<04:41,  2.69it/s]Evaluating:  64%|██████▍   | 1375/2132 [08:25<04:38,  2.71it/s]Evaluating:  65%|██████▍   | 1376/2132 [08:25<04:37,  2.72it/s]Evaluating:  65%|██████▍   | 1377/2132 [08:25<04:37,  2.72it/s]Evaluating:  65%|██████▍   | 1378/2132 [08:26<04:36,  2.72it/s]Evaluating:  65%|██████▍   | 1379/2132 [08:26<04:36,  2.72it/s]Evaluating:  65%|██████▍   | 1380/2132 [08:26<04:36,  2.72it/s]Evaluating:  65%|██████▍   | 1381/2132 [08:27<04:35,  2.72it/s]Evaluating:  65%|██████▍   | 1382/2132 [08:27<04:35,  2.72it/s]Evaluating:  65%|██████▍   | 1383/2132 [08:28<04:34,  2.73it/s]Evaluating:  65%|██████▍   | 1384/2132 [08:28<04:33,  2.73it/s]Evaluating:  65%|██████▍   | 1385/2132 [08:28<04:34,  2.72it/s]Evaluating:  65%|██████▌   | 1386/2132 [08:29<04:35,  2.71it/s]Evaluating:  65%|██████▌   | 1387/2132 [08:29<04:34,  2.71it/s]Evaluating:  65%|██████▌   | 1388/2132 [08:29<04:34,  2.71it/s]Evaluating:  65%|██████▌   | 1389/2132 [08:30<04:34,  2.71it/s]Evaluating:  65%|██████▌   | 1390/2132 [08:30<04:33,  2.72it/s]Evaluating:  65%|██████▌   | 1391/2132 [08:31<04:33,  2.71it/s]Evaluating:  65%|██████▌   | 1392/2132 [08:31<04:33,  2.71it/s]Evaluating:  65%|██████▌   | 1393/2132 [08:31<04:32,  2.71it/s]Evaluating:  65%|██████▌   | 1394/2132 [08:32<04:33,  2.70it/s]Evaluating:  65%|██████▌   | 1395/2132 [08:32<04:33,  2.69it/s]Evaluating:  65%|██████▌   | 1396/2132 [08:32<04:33,  2.69it/s]Evaluating:  66%|██████▌   | 1397/2132 [08:33<04:32,  2.70it/s]Evaluating:  66%|██████▌   | 1398/2132 [08:33<04:32,  2.69it/s]Evaluating:  66%|██████▌   | 1399/2132 [08:33<04:31,  2.70it/s]Evaluating:  66%|██████▌   | 1400/2132 [08:34<04:30,  2.70it/s]Evaluating:  66%|██████▌   | 1401/2132 [08:34<04:29,  2.71it/s]Evaluating:  66%|██████▌   | 1402/2132 [08:35<04:28,  2.72it/s]Evaluating:  66%|██████▌   | 1403/2132 [08:35<04:28,  2.72it/s]Evaluating:  66%|██████▌   | 1404/2132 [08:35<04:27,  2.72it/s]Evaluating:  66%|██████▌   | 1405/2132 [08:36<04:26,  2.73it/s]Evaluating:  66%|██████▌   | 1406/2132 [08:36<04:26,  2.73it/s]Evaluating:  66%|██████▌   | 1407/2132 [08:36<04:25,  2.73it/s]Evaluating:  66%|██████▌   | 1408/2132 [08:37<04:25,  2.73it/s]Evaluating:  66%|██████▌   | 1409/2132 [08:37<04:24,  2.73it/s]Evaluating:  66%|██████▌   | 1410/2132 [08:38<04:24,  2.73it/s]Evaluating:  66%|██████▌   | 1411/2132 [08:38<04:26,  2.71it/s]Evaluating:  66%|██████▌   | 1412/2132 [08:38<04:26,  2.70it/s]Evaluating:  66%|██████▋   | 1413/2132 [08:39<04:24,  2.72it/s]Evaluating:  66%|██████▋   | 1414/2132 [08:39<04:23,  2.72it/s]Evaluating:  66%|██████▋   | 1415/2132 [08:39<04:24,  2.72it/s]Evaluating:  66%|██████▋   | 1416/2132 [08:40<04:24,  2.70it/s]Evaluating:  66%|██████▋   | 1417/2132 [08:40<04:25,  2.70it/s]Evaluating:  67%|██████▋   | 1418/2132 [08:40<04:23,  2.71it/s]Evaluating:  67%|██████▋   | 1419/2132 [08:41<04:24,  2.70it/s]Evaluating:  67%|██████▋   | 1420/2132 [08:41<04:23,  2.70it/s]Evaluating:  67%|██████▋   | 1421/2132 [08:42<04:23,  2.70it/s]Evaluating:  67%|██████▋   | 1422/2132 [08:42<04:23,  2.70it/s]Evaluating:  67%|██████▋   | 1423/2132 [08:42<04:22,  2.70it/s]Evaluating:  67%|██████▋   | 1424/2132 [08:43<04:21,  2.70it/s]Evaluating:  67%|██████▋   | 1425/2132 [08:43<04:21,  2.71it/s]Evaluating:  67%|██████▋   | 1426/2132 [08:43<04:20,  2.71it/s]Evaluating:  67%|██████▋   | 1427/2132 [08:44<04:20,  2.71it/s]Evaluating:  67%|██████▋   | 1428/2132 [08:44<04:18,  2.72it/s]Evaluating:  67%|██████▋   | 1429/2132 [08:45<04:18,  2.72it/s]Evaluating:  67%|██████▋   | 1430/2132 [08:45<04:18,  2.72it/s]Evaluating:  67%|██████▋   | 1431/2132 [08:45<04:18,  2.71it/s]Evaluating:  67%|██████▋   | 1432/2132 [08:46<04:17,  2.72it/s]Evaluating:  67%|██████▋   | 1433/2132 [08:46<04:16,  2.72it/s]Evaluating:  67%|██████▋   | 1434/2132 [08:46<04:16,  2.73it/s]Evaluating:  67%|██████▋   | 1435/2132 [08:47<04:16,  2.72it/s]Evaluating:  67%|██████▋   | 1436/2132 [08:47<04:15,  2.72it/s]Evaluating:  67%|██████▋   | 1437/2132 [08:47<04:15,  2.72it/s]Evaluating:  67%|██████▋   | 1438/2132 [08:48<04:15,  2.72it/s]Evaluating:  67%|██████▋   | 1439/2132 [08:48<04:14,  2.72it/s]Evaluating:  68%|██████▊   | 1440/2132 [08:49<04:14,  2.72it/s]Evaluating:  68%|██████▊   | 1441/2132 [08:49<04:14,  2.72it/s]Evaluating:  68%|██████▊   | 1442/2132 [08:49<04:13,  2.72it/s]Evaluating:  68%|██████▊   | 1443/2132 [08:50<04:13,  2.71it/s]Evaluating:  68%|██████▊   | 1444/2132 [08:50<04:12,  2.72it/s]Evaluating:  68%|██████▊   | 1445/2132 [08:50<04:12,  2.72it/s]Evaluating:  68%|██████▊   | 1446/2132 [08:51<04:13,  2.71it/s]Evaluating:  68%|██████▊   | 1447/2132 [08:51<04:12,  2.71it/s]Evaluating:  68%|██████▊   | 1448/2132 [08:52<04:12,  2.71it/s]Evaluating:  68%|██████▊   | 1449/2132 [08:52<04:10,  2.73it/s]Evaluating:  68%|██████▊   | 1450/2132 [08:52<04:09,  2.73it/s]Evaluating:  68%|██████▊   | 1451/2132 [08:53<04:09,  2.73it/s]Evaluating:  68%|██████▊   | 1452/2132 [08:53<04:09,  2.73it/s]Evaluating:  68%|██████▊   | 1453/2132 [08:53<04:09,  2.72it/s]Evaluating:  68%|██████▊   | 1454/2132 [08:54<04:09,  2.72it/s]Evaluating:  68%|██████▊   | 1455/2132 [08:54<04:08,  2.72it/s]Evaluating:  68%|██████▊   | 1456/2132 [08:54<04:09,  2.71it/s]Evaluating:  68%|██████▊   | 1457/2132 [08:55<04:07,  2.72it/s]Evaluating:  68%|██████▊   | 1458/2132 [08:55<04:07,  2.72it/s]Evaluating:  68%|██████▊   | 1459/2132 [08:56<04:07,  2.72it/s]Evaluating:  68%|██████▊   | 1460/2132 [08:56<04:07,  2.72it/s]Evaluating:  69%|██████▊   | 1461/2132 [08:56<04:08,  2.70it/s]Evaluating:  69%|██████▊   | 1462/2132 [08:57<04:07,  2.71it/s]Evaluating:  69%|██████▊   | 1463/2132 [08:57<04:06,  2.71it/s]Evaluating:  69%|██████▊   | 1464/2132 [08:57<04:06,  2.71it/s]Evaluating:  69%|██████▊   | 1465/2132 [08:58<04:06,  2.71it/s]Evaluating:  69%|██████▉   | 1466/2132 [08:58<04:05,  2.71it/s]Evaluating:  69%|██████▉   | 1467/2132 [08:59<04:05,  2.71it/s]Evaluating:  69%|██████▉   | 1468/2132 [08:59<04:04,  2.71it/s]Evaluating:  69%|██████▉   | 1469/2132 [08:59<04:04,  2.71it/s]Evaluating:  69%|██████▉   | 1470/2132 [09:00<04:03,  2.71it/s]Evaluating:  69%|██████▉   | 1471/2132 [09:00<04:03,  2.72it/s]Evaluating:  69%|██████▉   | 1472/2132 [09:00<04:02,  2.72it/s]Evaluating:  69%|██████▉   | 1473/2132 [09:01<04:02,  2.72it/s]Evaluating:  69%|██████▉   | 1474/2132 [09:01<04:03,  2.70it/s]Evaluating:  69%|██████▉   | 1475/2132 [09:01<04:02,  2.71it/s]Evaluating:  69%|██████▉   | 1476/2132 [09:02<04:02,  2.71it/s]Evaluating:  69%|██████▉   | 1477/2132 [09:02<04:01,  2.71it/s]Evaluating:  69%|██████▉   | 1478/2132 [09:03<04:00,  2.72it/s]Evaluating:  69%|██████▉   | 1479/2132 [09:03<04:00,  2.71it/s]Evaluating:  69%|██████▉   | 1480/2132 [09:03<04:01,  2.70it/s]Evaluating:  69%|██████▉   | 1481/2132 [09:04<04:00,  2.71it/s]Evaluating:  70%|██████▉   | 1482/2132 [09:04<03:59,  2.71it/s]Evaluating:  70%|██████▉   | 1483/2132 [09:04<03:58,  2.72it/s]Evaluating:  70%|██████▉   | 1484/2132 [09:05<03:58,  2.71it/s]Evaluating:  70%|██████▉   | 1485/2132 [09:05<03:58,  2.71it/s]Evaluating:  70%|██████▉   | 1486/2132 [09:06<03:57,  2.72it/s]Evaluating:  70%|██████▉   | 1487/2132 [09:06<03:56,  2.72it/s]Evaluating:  70%|██████▉   | 1488/2132 [09:06<03:56,  2.72it/s]Evaluating:  70%|██████▉   | 1489/2132 [09:07<03:55,  2.73it/s]Evaluating:  70%|██████▉   | 1490/2132 [09:07<03:55,  2.73it/s]Evaluating:  70%|██████▉   | 1491/2132 [09:07<03:56,  2.71it/s]Evaluating:  70%|██████▉   | 1492/2132 [09:08<03:56,  2.71it/s]Evaluating:  70%|███████   | 1493/2132 [09:08<03:56,  2.70it/s]Evaluating:  70%|███████   | 1494/2132 [09:08<03:55,  2.70it/s]Evaluating:  70%|███████   | 1495/2132 [09:09<03:55,  2.70it/s]Evaluating:  70%|███████   | 1496/2132 [09:09<03:55,  2.70it/s]Evaluating:  70%|███████   | 1497/2132 [09:10<03:54,  2.71it/s]Evaluating:  70%|███████   | 1498/2132 [09:10<03:54,  2.71it/s]Evaluating:  70%|███████   | 1499/2132 [09:10<03:53,  2.71it/s]Evaluating:  70%|███████   | 1500/2132 [09:11<03:52,  2.72it/s]Evaluating:  70%|███████   | 1501/2132 [09:11<03:52,  2.71it/s]Evaluating:  70%|███████   | 1502/2132 [09:11<03:53,  2.70it/s]Evaluating:  70%|███████   | 1503/2132 [09:12<03:52,  2.70it/s]Evaluating:  71%|███████   | 1504/2132 [09:12<03:52,  2.70it/s]Evaluating:  71%|███████   | 1505/2132 [09:13<03:52,  2.70it/s]Evaluating:  71%|███████   | 1506/2132 [09:13<03:51,  2.71it/s]Evaluating:  71%|███████   | 1507/2132 [09:13<03:50,  2.71it/s]Evaluating:  71%|███████   | 1508/2132 [09:14<03:50,  2.71it/s]Evaluating:  71%|███████   | 1509/2132 [09:14<03:49,  2.71it/s]Evaluating:  71%|███████   | 1510/2132 [09:14<03:48,  2.72it/s]Evaluating:  71%|███████   | 1511/2132 [09:15<03:49,  2.71it/s]Evaluating:  71%|███████   | 1512/2132 [09:15<03:48,  2.72it/s]Evaluating:  71%|███████   | 1513/2132 [09:15<03:47,  2.72it/s]Evaluating:  71%|███████   | 1514/2132 [09:16<03:47,  2.72it/s]Evaluating:  71%|███████   | 1515/2132 [09:16<03:51,  2.67it/s]Evaluating:  71%|███████   | 1516/2132 [09:17<03:49,  2.69it/s]Evaluating:  71%|███████   | 1517/2132 [09:17<03:48,  2.69it/s]Evaluating:  71%|███████   | 1518/2132 [09:17<03:47,  2.69it/s]Evaluating:  71%|███████   | 1519/2132 [09:18<03:48,  2.69it/s]Evaluating:  71%|███████▏  | 1520/2132 [09:18<03:47,  2.68it/s]Evaluating:  71%|███████▏  | 1521/2132 [09:18<03:47,  2.68it/s]Evaluating:  71%|███████▏  | 1522/2132 [09:19<03:46,  2.69it/s]Evaluating:  71%|███████▏  | 1523/2132 [09:19<03:45,  2.70it/s]Evaluating:  71%|███████▏  | 1524/2132 [09:20<03:44,  2.71it/s]Evaluating:  72%|███████▏  | 1525/2132 [09:20<03:44,  2.70it/s]Evaluating:  72%|███████▏  | 1526/2132 [09:20<03:44,  2.70it/s]Evaluating:  72%|███████▏  | 1527/2132 [09:21<03:46,  2.68it/s]Evaluating:  72%|███████▏  | 1528/2132 [09:21<03:44,  2.69it/s]Evaluating:  72%|███████▏  | 1529/2132 [09:21<03:44,  2.69it/s]Evaluating:  72%|███████▏  | 1530/2132 [09:22<03:43,  2.70it/s]Evaluating:  72%|███████▏  | 1531/2132 [09:22<03:42,  2.70it/s]Evaluating:  72%|███████▏  | 1532/2132 [09:23<03:42,  2.70it/s]Evaluating:  72%|███████▏  | 1533/2132 [09:23<03:42,  2.70it/s]Evaluating:  72%|███████▏  | 1534/2132 [09:23<03:41,  2.70it/s]Evaluating:  72%|███████▏  | 1535/2132 [09:24<03:41,  2.69it/s]Evaluating:  72%|███████▏  | 1536/2132 [09:24<03:41,  2.69it/s]Evaluating:  72%|███████▏  | 1537/2132 [09:24<03:40,  2.70it/s]Evaluating:  72%|███████▏  | 1538/2132 [09:25<03:40,  2.70it/s]Evaluating:  72%|███████▏  | 1539/2132 [09:25<03:40,  2.69it/s]Evaluating:  72%|███████▏  | 1540/2132 [09:26<03:39,  2.69it/s]Evaluating:  72%|███████▏  | 1541/2132 [09:26<03:38,  2.70it/s]Evaluating:  72%|███████▏  | 1542/2132 [09:26<03:38,  2.70it/s]Evaluating:  72%|███████▏  | 1543/2132 [09:27<03:39,  2.69it/s]Evaluating:  72%|███████▏  | 1544/2132 [09:27<03:38,  2.69it/s]Evaluating:  72%|███████▏  | 1545/2132 [09:27<03:38,  2.69it/s]Evaluating:  73%|███████▎  | 1546/2132 [09:28<03:37,  2.69it/s]Evaluating:  73%|███████▎  | 1547/2132 [09:28<03:35,  2.71it/s]Evaluating:  73%|███████▎  | 1548/2132 [09:28<03:35,  2.71it/s]Evaluating:  73%|███████▎  | 1549/2132 [09:29<03:35,  2.70it/s]Evaluating:  73%|███████▎  | 1550/2132 [09:29<03:34,  2.71it/s]Evaluating:  73%|███████▎  | 1551/2132 [09:30<03:34,  2.71it/s]Evaluating:  73%|███████▎  | 1552/2132 [09:30<03:33,  2.71it/s]Evaluating:  73%|███████▎  | 1553/2132 [09:30<03:33,  2.72it/s]Evaluating:  73%|███████▎  | 1554/2132 [09:31<03:32,  2.72it/s]Evaluating:  73%|███████▎  | 1555/2132 [09:31<03:32,  2.72it/s]Evaluating:  73%|███████▎  | 1556/2132 [09:31<03:31,  2.72it/s]Evaluating:  73%|███████▎  | 1557/2132 [09:32<03:32,  2.71it/s]Evaluating:  73%|███████▎  | 1558/2132 [09:32<03:32,  2.70it/s]Evaluating:  73%|███████▎  | 1559/2132 [09:33<03:32,  2.70it/s]Evaluating:  73%|███████▎  | 1560/2132 [09:33<03:31,  2.70it/s]Evaluating:  73%|███████▎  | 1561/2132 [09:33<03:31,  2.70it/s]Evaluating:  73%|███████▎  | 1562/2132 [09:34<03:30,  2.71it/s]Evaluating:  73%|███████▎  | 1563/2132 [09:34<03:29,  2.71it/s]Evaluating:  73%|███████▎  | 1564/2132 [09:34<03:29,  2.71it/s]Evaluating:  73%|███████▎  | 1565/2132 [09:35<03:29,  2.71it/s]Evaluating:  73%|███████▎  | 1566/2132 [09:35<03:28,  2.71it/s]Evaluating:  73%|███████▎  | 1567/2132 [09:35<03:28,  2.71it/s]Evaluating:  74%|███████▎  | 1568/2132 [09:36<03:28,  2.70it/s]Evaluating:  74%|███████▎  | 1569/2132 [09:36<03:27,  2.71it/s]Evaluating:  74%|███████▎  | 1570/2132 [09:37<03:27,  2.71it/s]Evaluating:  74%|███████▎  | 1571/2132 [09:37<03:27,  2.70it/s]Evaluating:  74%|███████▎  | 1572/2132 [09:37<03:26,  2.71it/s]Evaluating:  74%|███████▍  | 1573/2132 [09:38<03:26,  2.71it/s]Evaluating:  74%|███████▍  | 1574/2132 [09:38<03:25,  2.72it/s]Evaluating:  74%|███████▍  | 1575/2132 [09:38<03:24,  2.72it/s]Evaluating:  74%|███████▍  | 1576/2132 [09:39<03:24,  2.72it/s]Evaluating:  74%|███████▍  | 1577/2132 [09:39<03:24,  2.72it/s]Evaluating:  74%|███████▍  | 1578/2132 [09:40<03:23,  2.72it/s]Evaluating:  74%|███████▍  | 1579/2132 [09:40<03:23,  2.72it/s]Evaluating:  74%|███████▍  | 1580/2132 [09:40<03:22,  2.72it/s]Evaluating:  74%|███████▍  | 1581/2132 [09:41<03:22,  2.72it/s]Evaluating:  74%|███████▍  | 1582/2132 [09:41<03:22,  2.72it/s]Evaluating:  74%|███████▍  | 1583/2132 [09:41<03:22,  2.71it/s]Evaluating:  74%|███████▍  | 1584/2132 [09:42<03:21,  2.72it/s]Evaluating:  74%|███████▍  | 1585/2132 [09:42<03:20,  2.73it/s]Evaluating:  74%|███████▍  | 1586/2132 [09:42<03:19,  2.74it/s]Evaluating:  74%|███████▍  | 1587/2132 [09:43<03:19,  2.73it/s]Evaluating:  74%|███████▍  | 1588/2132 [09:43<03:19,  2.73it/s]Evaluating:  75%|███████▍  | 1589/2132 [09:44<03:19,  2.73it/s]Evaluating:  75%|███████▍  | 1590/2132 [09:44<03:19,  2.72it/s]Evaluating:  75%|███████▍  | 1591/2132 [09:44<03:18,  2.72it/s]Evaluating:  75%|███████▍  | 1592/2132 [09:45<03:17,  2.73it/s]Evaluating:  75%|███████▍  | 1593/2132 [09:45<03:17,  2.73it/s]Evaluating:  75%|███████▍  | 1594/2132 [09:45<03:16,  2.74it/s]Evaluating:  75%|███████▍  | 1595/2132 [09:46<03:17,  2.72it/s]Evaluating:  75%|███████▍  | 1596/2132 [09:46<03:16,  2.72it/s]Evaluating:  75%|███████▍  | 1597/2132 [09:47<03:15,  2.73it/s]Evaluating:  75%|███████▍  | 1598/2132 [09:47<03:15,  2.73it/s]Evaluating:  75%|███████▌  | 1599/2132 [09:47<03:15,  2.73it/s]Evaluating:  75%|███████▌  | 1600/2132 [09:48<03:14,  2.73it/s]Evaluating:  75%|███████▌  | 1601/2132 [09:48<03:15,  2.72it/s]Evaluating:  75%|███████▌  | 1602/2132 [09:48<03:15,  2.71it/s]Evaluating:  75%|███████▌  | 1603/2132 [09:49<03:14,  2.72it/s]Evaluating:  75%|███████▌  | 1604/2132 [09:49<03:15,  2.71it/s]Evaluating:  75%|███████▌  | 1605/2132 [09:49<03:14,  2.71it/s]Evaluating:  75%|███████▌  | 1606/2132 [09:50<03:14,  2.71it/s]Evaluating:  75%|███████▌  | 1607/2132 [09:50<03:13,  2.71it/s]Evaluating:  75%|███████▌  | 1608/2132 [09:51<03:13,  2.70it/s]Evaluating:  75%|███████▌  | 1609/2132 [09:51<03:13,  2.70it/s]Evaluating:  76%|███████▌  | 1610/2132 [09:51<03:12,  2.71it/s]Evaluating:  76%|███████▌  | 1611/2132 [09:52<03:12,  2.70it/s]Evaluating:  76%|███████▌  | 1612/2132 [09:52<03:12,  2.71it/s]Evaluating:  76%|███████▌  | 1613/2132 [09:52<03:11,  2.71it/s]Evaluating:  76%|███████▌  | 1614/2132 [09:53<03:11,  2.70it/s]Evaluating:  76%|███████▌  | 1615/2132 [09:53<03:10,  2.71it/s]Evaluating:  76%|███████▌  | 1616/2132 [09:54<03:10,  2.70it/s]Evaluating:  76%|███████▌  | 1617/2132 [09:54<03:10,  2.71it/s]Evaluating:  76%|███████▌  | 1618/2132 [09:54<03:09,  2.71it/s]Evaluating:  76%|███████▌  | 1619/2132 [09:55<03:09,  2.71it/s]Evaluating:  76%|███████▌  | 1620/2132 [09:55<03:09,  2.71it/s]Evaluating:  76%|███████▌  | 1621/2132 [09:55<03:08,  2.71it/s]Evaluating:  76%|███████▌  | 1622/2132 [09:56<03:08,  2.71it/s]Evaluating:  76%|███████▌  | 1623/2132 [09:56<03:08,  2.70it/s]Evaluating:  76%|███████▌  | 1624/2132 [09:56<03:08,  2.70it/s]Evaluating:  76%|███████▌  | 1625/2132 [09:57<03:07,  2.71it/s]Evaluating:  76%|███████▋  | 1626/2132 [09:57<03:06,  2.72it/s]Evaluating:  76%|███████▋  | 1627/2132 [09:58<03:05,  2.72it/s]Evaluating:  76%|███████▋  | 1628/2132 [09:58<03:04,  2.73it/s]Evaluating:  76%|███████▋  | 1629/2132 [09:58<03:04,  2.72it/s]Evaluating:  76%|███████▋  | 1630/2132 [09:59<03:04,  2.73it/s]Evaluating:  77%|███████▋  | 1631/2132 [09:59<03:03,  2.73it/s]Evaluating:  77%|███████▋  | 1632/2132 [09:59<03:04,  2.71it/s]Evaluating:  77%|███████▋  | 1633/2132 [10:00<03:05,  2.70it/s]Evaluating:  77%|███████▋  | 1634/2132 [10:00<03:04,  2.70it/s]Evaluating:  77%|███████▋  | 1635/2132 [10:01<03:03,  2.70it/s]Evaluating:  77%|███████▋  | 1636/2132 [10:01<03:02,  2.71it/s]Evaluating:  77%|███████▋  | 1637/2132 [10:01<03:01,  2.72it/s]Evaluating:  77%|███████▋  | 1638/2132 [10:02<03:01,  2.73it/s]Evaluating:  77%|███████▋  | 1639/2132 [10:02<03:00,  2.73it/s]Evaluating:  77%|███████▋  | 1640/2132 [10:02<02:59,  2.74it/s]Evaluating:  77%|███████▋  | 1641/2132 [10:03<02:59,  2.73it/s]Evaluating:  77%|███████▋  | 1642/2132 [10:03<02:58,  2.74it/s]Evaluating:  77%|███████▋  | 1643/2132 [10:03<02:59,  2.73it/s]Evaluating:  77%|███████▋  | 1644/2132 [10:04<02:59,  2.72it/s]Evaluating:  77%|███████▋  | 1645/2132 [10:04<02:58,  2.73it/s]Evaluating:  77%|███████▋  | 1646/2132 [10:05<02:57,  2.74it/s]Evaluating:  77%|███████▋  | 1647/2132 [10:05<02:57,  2.74it/s]Evaluating:  77%|███████▋  | 1648/2132 [10:05<02:56,  2.74it/s]Evaluating:  77%|███████▋  | 1649/2132 [10:06<02:56,  2.73it/s]Evaluating:  77%|███████▋  | 1650/2132 [10:06<02:56,  2.73it/s]Evaluating:  77%|███████▋  | 1651/2132 [10:06<02:56,  2.73it/s]Evaluating:  77%|███████▋  | 1652/2132 [10:07<02:56,  2.73it/s]Evaluating:  78%|███████▊  | 1653/2132 [10:07<02:55,  2.73it/s]Evaluating:  78%|███████▊  | 1654/2132 [10:07<02:55,  2.73it/s]Evaluating:  78%|███████▊  | 1655/2132 [10:08<02:54,  2.74it/s]Evaluating:  78%|███████▊  | 1656/2132 [10:08<02:53,  2.74it/s]Evaluating:  78%|███████▊  | 1657/2132 [10:09<02:53,  2.74it/s]Evaluating:  78%|███████▊  | 1658/2132 [10:09<02:53,  2.73it/s]Evaluating:  78%|███████▊  | 1659/2132 [10:09<02:53,  2.72it/s]Evaluating:  78%|███████▊  | 1660/2132 [10:10<02:53,  2.72it/s]Evaluating:  78%|███████▊  | 1661/2132 [10:10<02:53,  2.72it/s]Evaluating:  78%|███████▊  | 1662/2132 [10:10<02:52,  2.72it/s]Evaluating:  78%|███████▊  | 1663/2132 [10:11<02:52,  2.73it/s]Evaluating:  78%|███████▊  | 1664/2132 [10:11<02:52,  2.72it/s]Evaluating:  78%|███████▊  | 1665/2132 [10:12<02:51,  2.72it/s]Evaluating:  78%|███████▊  | 1666/2132 [10:12<02:51,  2.71it/s]Evaluating:  78%|███████▊  | 1667/2132 [10:12<02:51,  2.71it/s]Evaluating:  78%|███████▊  | 1668/2132 [10:13<02:52,  2.69it/s]Evaluating:  78%|███████▊  | 1669/2132 [10:13<02:51,  2.70it/s]Evaluating:  78%|███████▊  | 1670/2132 [10:13<02:50,  2.71it/s]Evaluating:  78%|███████▊  | 1671/2132 [10:14<02:50,  2.71it/s]Evaluating:  78%|███████▊  | 1672/2132 [10:14<02:49,  2.71it/s]Evaluating:  78%|███████▊  | 1673/2132 [10:14<02:48,  2.72it/s]Evaluating:  79%|███████▊  | 1674/2132 [10:15<02:48,  2.71it/s]Evaluating:  79%|███████▊  | 1675/2132 [10:15<02:48,  2.71it/s]Evaluating:  79%|███████▊  | 1676/2132 [10:16<02:48,  2.71it/s]Evaluating:  79%|███████▊  | 1677/2132 [10:16<02:47,  2.72it/s]Evaluating:  79%|███████▊  | 1678/2132 [10:16<02:46,  2.73it/s]Evaluating:  79%|███████▉  | 1679/2132 [10:17<02:45,  2.73it/s]Evaluating:  79%|███████▉  | 1680/2132 [10:17<02:46,  2.72it/s]Evaluating:  79%|███████▉  | 1681/2132 [10:17<02:46,  2.72it/s]Evaluating:  79%|███████▉  | 1682/2132 [10:18<02:45,  2.71it/s]Evaluating:  79%|███████▉  | 1683/2132 [10:18<02:45,  2.71it/s]Evaluating:  79%|███████▉  | 1684/2132 [10:19<02:45,  2.70it/s]Evaluating:  79%|███████▉  | 1685/2132 [10:19<02:44,  2.72it/s]Evaluating:  79%|███████▉  | 1686/2132 [10:19<02:43,  2.72it/s]Evaluating:  79%|███████▉  | 1687/2132 [10:20<02:43,  2.73it/s]Evaluating:  79%|███████▉  | 1688/2132 [10:20<02:42,  2.73it/s]Evaluating:  79%|███████▉  | 1689/2132 [10:20<02:42,  2.73it/s]Evaluating:  79%|███████▉  | 1690/2132 [10:21<02:42,  2.72it/s]Evaluating:  79%|███████▉  | 1691/2132 [10:21<02:42,  2.71it/s]Evaluating:  79%|███████▉  | 1692/2132 [10:21<02:42,  2.71it/s]Evaluating:  79%|███████▉  | 1693/2132 [10:22<02:41,  2.71it/s]Evaluating:  79%|███████▉  | 1694/2132 [10:22<02:41,  2.72it/s]Evaluating:  80%|███████▉  | 1695/2132 [10:23<02:40,  2.72it/s]Evaluating:  80%|███████▉  | 1696/2132 [10:23<02:41,  2.70it/s]Evaluating:  80%|███████▉  | 1697/2132 [10:23<02:41,  2.69it/s]Evaluating:  80%|███████▉  | 1698/2132 [10:24<02:41,  2.69it/s]Evaluating:  80%|███████▉  | 1699/2132 [10:24<02:40,  2.69it/s]Evaluating:  80%|███████▉  | 1700/2132 [10:24<02:39,  2.70it/s]Evaluating:  80%|███████▉  | 1701/2132 [10:25<02:39,  2.70it/s]Evaluating:  80%|███████▉  | 1702/2132 [10:25<02:38,  2.71it/s]Evaluating:  80%|███████▉  | 1703/2132 [10:26<02:38,  2.71it/s]Evaluating:  80%|███████▉  | 1704/2132 [10:26<02:37,  2.72it/s]Evaluating:  80%|███████▉  | 1705/2132 [10:26<02:38,  2.70it/s]Evaluating:  80%|████████  | 1706/2132 [10:27<02:37,  2.71it/s]Evaluating:  80%|████████  | 1707/2132 [10:27<02:36,  2.72it/s]Evaluating:  80%|████████  | 1708/2132 [10:27<02:35,  2.72it/s]Evaluating:  80%|████████  | 1709/2132 [10:28<02:36,  2.71it/s]Evaluating:  80%|████████  | 1710/2132 [10:28<02:35,  2.71it/s]Evaluating:  80%|████████  | 1711/2132 [10:28<02:34,  2.72it/s]Evaluating:  80%|████████  | 1712/2132 [10:29<02:33,  2.73it/s]Evaluating:  80%|████████  | 1713/2132 [10:29<02:33,  2.73it/s]Evaluating:  80%|████████  | 1714/2132 [10:30<02:33,  2.71it/s]Evaluating:  80%|████████  | 1715/2132 [10:30<02:33,  2.72it/s]Evaluating:  80%|████████  | 1716/2132 [10:30<02:33,  2.71it/s]Evaluating:  81%|████████  | 1717/2132 [10:31<02:32,  2.72it/s]Evaluating:  81%|████████  | 1718/2132 [10:31<02:32,  2.71it/s]Evaluating:  81%|████████  | 1719/2132 [10:31<02:32,  2.71it/s]Evaluating:  81%|████████  | 1720/2132 [10:32<02:31,  2.71it/s]Evaluating:  81%|████████  | 1721/2132 [10:32<02:31,  2.71it/s]Evaluating:  81%|████████  | 1722/2132 [10:33<02:31,  2.70it/s]Evaluating:  81%|████████  | 1723/2132 [10:33<02:30,  2.72it/s]Evaluating:  81%|████████  | 1724/2132 [10:33<02:30,  2.72it/s]Evaluating:  81%|████████  | 1725/2132 [10:34<02:29,  2.72it/s]Evaluating:  81%|████████  | 1726/2132 [10:34<02:30,  2.70it/s]Evaluating:  81%|████████  | 1727/2132 [10:34<02:29,  2.70it/s]Evaluating:  81%|████████  | 1728/2132 [10:35<02:29,  2.71it/s]Evaluating:  81%|████████  | 1729/2132 [10:35<02:28,  2.72it/s]Evaluating:  81%|████████  | 1730/2132 [10:35<02:28,  2.71it/s]Evaluating:  81%|████████  | 1731/2132 [10:36<02:32,  2.63it/s]Evaluating:  81%|████████  | 1732/2132 [10:36<02:31,  2.65it/s]Evaluating:  81%|████████▏ | 1733/2132 [10:37<02:29,  2.67it/s]Evaluating:  81%|████████▏ | 1734/2132 [10:37<02:28,  2.69it/s]Evaluating:  81%|████████▏ | 1735/2132 [10:37<02:27,  2.69it/s]Evaluating:  81%|████████▏ | 1736/2132 [10:38<02:27,  2.69it/s]Evaluating:  81%|████████▏ | 1737/2132 [10:38<02:26,  2.70it/s]Evaluating:  82%|████████▏ | 1738/2132 [10:38<02:27,  2.68it/s]Evaluating:  82%|████████▏ | 1739/2132 [10:39<02:26,  2.69it/s]Evaluating:  82%|████████▏ | 1740/2132 [10:39<02:25,  2.69it/s]Evaluating:  82%|████████▏ | 1741/2132 [10:40<02:25,  2.68it/s]Evaluating:  82%|████████▏ | 1742/2132 [10:40<02:25,  2.69it/s]Evaluating:  82%|████████▏ | 1743/2132 [10:40<02:24,  2.70it/s]Evaluating:  82%|████████▏ | 1744/2132 [10:41<02:23,  2.70it/s]Evaluating:  82%|████████▏ | 1745/2132 [10:41<02:23,  2.69it/s]Evaluating:  82%|████████▏ | 1746/2132 [10:41<02:23,  2.69it/s]Evaluating:  82%|████████▏ | 1747/2132 [10:42<02:23,  2.69it/s]Evaluating:  82%|████████▏ | 1748/2132 [10:42<02:23,  2.68it/s]Evaluating:  82%|████████▏ | 1749/2132 [10:43<02:23,  2.68it/s]Evaluating:  82%|████████▏ | 1750/2132 [10:43<02:22,  2.68it/s]Evaluating:  82%|████████▏ | 1751/2132 [10:43<02:22,  2.68it/s]Evaluating:  82%|████████▏ | 1752/2132 [10:44<02:21,  2.69it/s]Evaluating:  82%|████████▏ | 1753/2132 [10:44<02:20,  2.70it/s]Evaluating:  82%|████████▏ | 1754/2132 [10:44<02:20,  2.70it/s]Evaluating:  82%|████████▏ | 1755/2132 [10:45<02:19,  2.70it/s]Evaluating:  82%|████████▏ | 1756/2132 [10:45<02:19,  2.70it/s]Evaluating:  82%|████████▏ | 1757/2132 [10:46<02:18,  2.70it/s]Evaluating:  82%|████████▏ | 1758/2132 [10:46<02:19,  2.68it/s]Evaluating:  83%|████████▎ | 1759/2132 [10:46<02:19,  2.68it/s]Evaluating:  83%|████████▎ | 1760/2132 [10:47<02:18,  2.69it/s]Evaluating:  83%|████████▎ | 1761/2132 [10:47<02:18,  2.68it/s]Evaluating:  83%|████████▎ | 1762/2132 [10:47<02:19,  2.66it/s]Evaluating:  83%|████████▎ | 1763/2132 [10:48<02:17,  2.67it/s]Evaluating:  83%|████████▎ | 1764/2132 [10:48<02:16,  2.69it/s]Evaluating:  83%|████████▎ | 1765/2132 [10:49<02:15,  2.70it/s]Evaluating:  83%|████████▎ | 1766/2132 [10:49<02:15,  2.71it/s]Evaluating:  83%|████████▎ | 1767/2132 [10:49<02:14,  2.71it/s]Evaluating:  83%|████████▎ | 1768/2132 [10:50<02:14,  2.72it/s]Evaluating:  83%|████████▎ | 1769/2132 [10:50<02:13,  2.73it/s]Evaluating:  83%|████████▎ | 1770/2132 [10:50<02:13,  2.71it/s]Evaluating:  83%|████████▎ | 1771/2132 [10:51<02:12,  2.72it/s]Evaluating:  83%|████████▎ | 1772/2132 [10:51<02:11,  2.73it/s]Evaluating:  83%|████████▎ | 1773/2132 [10:51<02:11,  2.73it/s]Evaluating:  83%|████████▎ | 1774/2132 [10:52<02:11,  2.72it/s]Evaluating:  83%|████████▎ | 1775/2132 [10:52<02:10,  2.73it/s]Evaluating:  83%|████████▎ | 1776/2132 [10:53<02:10,  2.73it/s]Evaluating:  83%|████████▎ | 1777/2132 [10:53<02:10,  2.73it/s]Evaluating:  83%|████████▎ | 1778/2132 [10:53<02:09,  2.72it/s]Evaluating:  83%|████████▎ | 1779/2132 [10:54<02:09,  2.73it/s]Evaluating:  83%|████████▎ | 1780/2132 [10:54<02:08,  2.74it/s]Evaluating:  84%|████████▎ | 1781/2132 [10:54<02:07,  2.74it/s]Evaluating:  84%|████████▎ | 1782/2132 [10:55<02:08,  2.73it/s]Evaluating:  84%|████████▎ | 1783/2132 [10:55<02:07,  2.73it/s]Evaluating:  84%|████████▎ | 1784/2132 [10:55<02:07,  2.73it/s]Evaluating:  84%|████████▎ | 1785/2132 [10:56<02:08,  2.71it/s]Evaluating:  84%|████████▍ | 1786/2132 [10:56<02:07,  2.72it/s]Evaluating:  84%|████████▍ | 1787/2132 [10:57<02:06,  2.72it/s]Evaluating:  84%|████████▍ | 1788/2132 [10:57<02:06,  2.72it/s]Evaluating:  84%|████████▍ | 1789/2132 [10:57<02:05,  2.73it/s]Evaluating:  84%|████████▍ | 1790/2132 [10:58<02:05,  2.73it/s]Evaluating:  84%|████████▍ | 1791/2132 [10:58<02:05,  2.73it/s]Evaluating:  84%|████████▍ | 1792/2132 [10:58<02:05,  2.72it/s]Evaluating:  84%|████████▍ | 1793/2132 [10:59<02:04,  2.72it/s]Evaluating:  84%|████████▍ | 1794/2132 [10:59<02:04,  2.72it/s]Evaluating:  84%|████████▍ | 1795/2132 [11:00<02:04,  2.72it/s]Evaluating:  84%|████████▍ | 1796/2132 [11:00<02:03,  2.72it/s]Evaluating:  84%|████████▍ | 1797/2132 [11:00<02:03,  2.70it/s]Evaluating:  84%|████████▍ | 1798/2132 [11:01<02:03,  2.70it/s]Evaluating:  84%|████████▍ | 1799/2132 [11:01<02:03,  2.71it/s]Evaluating:  84%|████████▍ | 1800/2132 [11:01<02:02,  2.71it/s]Evaluating:  84%|████████▍ | 1801/2132 [11:02<02:01,  2.72it/s]Evaluating:  85%|████████▍ | 1802/2132 [11:02<02:01,  2.72it/s]Evaluating:  85%|████████▍ | 1803/2132 [11:02<02:02,  2.70it/s]Evaluating:  85%|████████▍ | 1804/2132 [11:03<02:01,  2.69it/s]Evaluating:  85%|████████▍ | 1805/2132 [11:03<02:01,  2.69it/s]Evaluating:  85%|████████▍ | 1806/2132 [11:04<02:01,  2.69it/s]Evaluating:  85%|████████▍ | 1807/2132 [11:04<02:00,  2.69it/s]Evaluating:  85%|████████▍ | 1808/2132 [11:04<02:00,  2.69it/s]Evaluating:  85%|████████▍ | 1809/2132 [11:05<01:59,  2.70it/s]Evaluating:  85%|████████▍ | 1810/2132 [11:05<01:59,  2.70it/s]Evaluating:  85%|████████▍ | 1811/2132 [11:05<01:58,  2.71it/s]Evaluating:  85%|████████▍ | 1812/2132 [11:06<01:57,  2.71it/s]Evaluating:  85%|████████▌ | 1813/2132 [11:06<01:57,  2.72it/s]Evaluating:  85%|████████▌ | 1814/2132 [11:07<01:57,  2.70it/s]Evaluating:  85%|████████▌ | 1815/2132 [11:07<01:57,  2.70it/s]Evaluating:  85%|████████▌ | 1816/2132 [11:07<01:57,  2.70it/s]Evaluating:  85%|████████▌ | 1817/2132 [11:08<01:56,  2.70it/s]Evaluating:  85%|████████▌ | 1818/2132 [11:08<01:56,  2.70it/s]Evaluating:  85%|████████▌ | 1819/2132 [11:08<01:55,  2.71it/s]Evaluating:  85%|████████▌ | 1820/2132 [11:09<01:55,  2.70it/s]Evaluating:  85%|████████▌ | 1821/2132 [11:09<01:55,  2.69it/s]Evaluating:  85%|████████▌ | 1822/2132 [11:10<01:55,  2.69it/s]Evaluating:  86%|████████▌ | 1823/2132 [11:10<01:54,  2.69it/s]Evaluating:  86%|████████▌ | 1824/2132 [11:10<01:54,  2.70it/s]Evaluating:  86%|████████▌ | 1825/2132 [11:11<01:53,  2.71it/s]Evaluating:  86%|████████▌ | 1826/2132 [11:11<01:53,  2.71it/s]Evaluating:  86%|████████▌ | 1827/2132 [11:11<01:52,  2.71it/s]Evaluating:  86%|████████▌ | 1828/2132 [11:12<01:52,  2.71it/s]Evaluating:  86%|████████▌ | 1829/2132 [11:12<01:52,  2.70it/s]Evaluating:  86%|████████▌ | 1830/2132 [11:12<01:52,  2.69it/s]Evaluating:  86%|████████▌ | 1831/2132 [11:13<01:51,  2.69it/s]Evaluating:  86%|████████▌ | 1832/2132 [11:13<01:51,  2.68it/s]Evaluating:  86%|████████▌ | 1833/2132 [11:14<01:51,  2.69it/s]Evaluating:  86%|████████▌ | 1834/2132 [11:14<01:50,  2.69it/s]Evaluating:  86%|████████▌ | 1835/2132 [11:14<01:50,  2.68it/s]Evaluating:  86%|████████▌ | 1836/2132 [11:15<01:50,  2.68it/s]Evaluating:  86%|████████▌ | 1837/2132 [11:15<01:49,  2.69it/s]Evaluating:  86%|████████▌ | 1838/2132 [11:15<01:48,  2.70it/s]Evaluating:  86%|████████▋ | 1839/2132 [11:16<01:48,  2.70it/s]Evaluating:  86%|████████▋ | 1840/2132 [11:16<01:48,  2.70it/s]Evaluating:  86%|████████▋ | 1841/2132 [11:17<01:48,  2.69it/s]Evaluating:  86%|████████▋ | 1842/2132 [11:17<01:48,  2.68it/s]Evaluating:  86%|████████▋ | 1843/2132 [11:17<01:47,  2.69it/s]Evaluating:  86%|████████▋ | 1844/2132 [11:18<01:46,  2.70it/s]Evaluating:  87%|████████▋ | 1845/2132 [11:18<01:46,  2.69it/s]Evaluating:  87%|████████▋ | 1846/2132 [11:18<01:46,  2.70it/s]Evaluating:  87%|████████▋ | 1847/2132 [11:19<01:45,  2.69it/s]Evaluating:  87%|████████▋ | 1848/2132 [11:19<01:45,  2.69it/s]Evaluating:  87%|████████▋ | 1849/2132 [11:20<01:44,  2.70it/s]Evaluating:  87%|████████▋ | 1850/2132 [11:20<01:43,  2.71it/s]Evaluating:  87%|████████▋ | 1851/2132 [11:20<01:43,  2.71it/s]Evaluating:  87%|████████▋ | 1852/2132 [11:21<01:43,  2.70it/s]Evaluating:  87%|████████▋ | 1853/2132 [11:21<01:43,  2.69it/s]Evaluating:  87%|████████▋ | 1854/2132 [11:21<01:42,  2.70it/s]Evaluating:  87%|████████▋ | 1855/2132 [11:22<01:42,  2.72it/s]Evaluating:  87%|████████▋ | 1856/2132 [11:22<01:41,  2.71it/s]Evaluating:  87%|████████▋ | 1857/2132 [11:23<01:42,  2.69it/s]Evaluating:  87%|████████▋ | 1858/2132 [11:23<01:41,  2.70it/s]Evaluating:  87%|████████▋ | 1859/2132 [11:23<01:41,  2.70it/s]Evaluating:  87%|████████▋ | 1860/2132 [11:24<01:40,  2.71it/s]Evaluating:  87%|████████▋ | 1861/2132 [11:24<01:40,  2.69it/s]Evaluating:  87%|████████▋ | 1862/2132 [11:24<01:39,  2.71it/s]Evaluating:  87%|████████▋ | 1863/2132 [11:25<01:39,  2.71it/s]Evaluating:  87%|████████▋ | 1864/2132 [11:25<01:38,  2.71it/s]Evaluating:  87%|████████▋ | 1865/2132 [11:25<01:39,  2.69it/s]Evaluating:  88%|████████▊ | 1866/2132 [11:26<01:38,  2.70it/s]Evaluating:  88%|████████▊ | 1867/2132 [11:26<01:38,  2.69it/s]Evaluating:  88%|████████▊ | 1868/2132 [11:27<01:37,  2.70it/s]Evaluating:  88%|████████▊ | 1869/2132 [11:27<01:37,  2.70it/s]Evaluating:  88%|████████▊ | 1870/2132 [11:27<01:37,  2.70it/s]Evaluating:  88%|████████▊ | 1871/2132 [11:28<01:36,  2.71it/s]Evaluating:  88%|████████▊ | 1872/2132 [11:28<01:35,  2.71it/s]Evaluating:  88%|████████▊ | 1873/2132 [11:28<01:35,  2.71it/s]Evaluating:  88%|████████▊ | 1874/2132 [11:29<01:35,  2.71it/s]Evaluating:  88%|████████▊ | 1875/2132 [11:29<01:35,  2.70it/s]Evaluating:  88%|████████▊ | 1876/2132 [11:30<01:34,  2.70it/s]Evaluating:  88%|████████▊ | 1877/2132 [11:30<01:34,  2.69it/s]Evaluating:  88%|████████▊ | 1878/2132 [11:30<01:34,  2.69it/s]Evaluating:  88%|████████▊ | 1879/2132 [11:31<01:33,  2.70it/s]Evaluating:  88%|████████▊ | 1880/2132 [11:31<01:33,  2.70it/s]Evaluating:  88%|████████▊ | 1881/2132 [11:31<01:32,  2.70it/s]Evaluating:  88%|████████▊ | 1882/2132 [11:32<01:32,  2.71it/s]Evaluating:  88%|████████▊ | 1883/2132 [11:32<01:31,  2.71it/s]Evaluating:  88%|████████▊ | 1884/2132 [11:32<01:31,  2.71it/s]Evaluating:  88%|████████▊ | 1885/2132 [11:33<01:31,  2.71it/s]Evaluating:  88%|████████▊ | 1886/2132 [11:33<01:30,  2.71it/s]Evaluating:  89%|████████▊ | 1887/2132 [11:34<01:30,  2.70it/s]Evaluating:  89%|████████▊ | 1888/2132 [11:34<01:30,  2.70it/s]Evaluating:  89%|████████▊ | 1889/2132 [11:34<01:29,  2.70it/s]Evaluating:  89%|████████▊ | 1890/2132 [11:35<01:29,  2.70it/s]Evaluating:  89%|████████▊ | 1891/2132 [11:35<01:29,  2.71it/s]Evaluating:  89%|████████▊ | 1892/2132 [11:35<01:28,  2.72it/s]Evaluating:  89%|████████▉ | 1893/2132 [11:36<01:28,  2.70it/s]Evaluating:  89%|████████▉ | 1894/2132 [11:36<01:28,  2.69it/s]Evaluating:  89%|████████▉ | 1895/2132 [11:37<01:28,  2.69it/s]Evaluating:  89%|████████▉ | 1896/2132 [11:37<01:27,  2.69it/s]Evaluating:  89%|████████▉ | 1897/2132 [11:37<01:27,  2.68it/s]Evaluating:  89%|████████▉ | 1898/2132 [11:38<01:27,  2.68it/s]Evaluating:  89%|████████▉ | 1899/2132 [11:38<01:26,  2.69it/s]Evaluating:  89%|████████▉ | 1900/2132 [11:38<01:25,  2.71it/s]Evaluating:  89%|████████▉ | 1901/2132 [11:39<01:25,  2.69it/s]Evaluating:  89%|████████▉ | 1902/2132 [11:39<01:25,  2.69it/s]Evaluating:  89%|████████▉ | 1903/2132 [11:40<01:24,  2.70it/s]Evaluating:  89%|████████▉ | 1904/2132 [11:40<01:24,  2.69it/s]Evaluating:  89%|████████▉ | 1905/2132 [11:40<01:24,  2.70it/s]Evaluating:  89%|████████▉ | 1906/2132 [11:41<01:23,  2.69it/s]Evaluating:  89%|████████▉ | 1907/2132 [11:41<01:23,  2.69it/s]Evaluating:  89%|████████▉ | 1908/2132 [11:41<01:23,  2.69it/s]Evaluating:  90%|████████▉ | 1909/2132 [11:42<01:22,  2.69it/s]Evaluating:  90%|████████▉ | 1910/2132 [11:42<01:22,  2.68it/s]Evaluating:  90%|████████▉ | 1911/2132 [11:43<01:22,  2.68it/s]Evaluating:  90%|████████▉ | 1912/2132 [11:43<01:22,  2.68it/s]Evaluating:  90%|████████▉ | 1913/2132 [11:43<01:21,  2.68it/s]Evaluating:  90%|████████▉ | 1914/2132 [11:44<01:21,  2.68it/s]Evaluating:  90%|████████▉ | 1915/2132 [11:44<01:20,  2.69it/s]Evaluating:  90%|████████▉ | 1916/2132 [11:44<01:20,  2.69it/s]Evaluating:  90%|████████▉ | 1917/2132 [11:45<01:20,  2.68it/s]Evaluating:  90%|████████▉ | 1918/2132 [11:45<01:19,  2.69it/s]Evaluating:  90%|█████████ | 1919/2132 [11:46<01:19,  2.69it/s]Evaluating:  90%|█████████ | 1920/2132 [11:46<01:18,  2.69it/s]Evaluating:  90%|█████████ | 1921/2132 [11:46<01:18,  2.69it/s]Evaluating:  90%|█████████ | 1922/2132 [11:47<01:17,  2.70it/s]Evaluating:  90%|█████████ | 1923/2132 [11:47<01:17,  2.69it/s]Evaluating:  90%|█████████ | 1924/2132 [11:47<01:18,  2.67it/s]Evaluating:  90%|█████████ | 1925/2132 [11:48<01:17,  2.67it/s]Evaluating:  90%|█████████ | 1926/2132 [11:48<01:17,  2.67it/s]Evaluating:  90%|█████████ | 1927/2132 [11:48<01:16,  2.66it/s]Evaluating:  90%|█████████ | 1928/2132 [11:49<01:16,  2.67it/s]Evaluating:  90%|█████████ | 1929/2132 [11:49<01:15,  2.68it/s]Evaluating:  91%|█████████ | 1930/2132 [11:50<01:15,  2.67it/s]Evaluating:  91%|█████████ | 1931/2132 [11:50<01:14,  2.69it/s]Evaluating:  91%|█████████ | 1932/2132 [11:50<01:14,  2.70it/s]Evaluating:  91%|█████████ | 1933/2132 [11:51<01:13,  2.69it/s]Evaluating:  91%|█████████ | 1934/2132 [11:51<01:13,  2.69it/s]Evaluating:  91%|█████████ | 1935/2132 [11:51<01:13,  2.69it/s]Evaluating:  91%|█████████ | 1936/2132 [11:52<01:12,  2.69it/s]Evaluating:  91%|█████████ | 1937/2132 [11:52<01:12,  2.70it/s]Evaluating:  91%|█████████ | 1938/2132 [11:53<01:11,  2.70it/s]Evaluating:  91%|█████████ | 1939/2132 [11:53<01:11,  2.69it/s]Evaluating:  91%|█████████ | 1940/2132 [11:53<01:11,  2.69it/s]Evaluating:  91%|█████████ | 1941/2132 [11:54<01:10,  2.70it/s]Evaluating:  91%|█████████ | 1942/2132 [11:54<01:10,  2.71it/s]Evaluating:  91%|█████████ | 1943/2132 [11:54<01:09,  2.71it/s]Evaluating:  91%|█████████ | 1944/2132 [11:55<01:09,  2.70it/s]Evaluating:  91%|█████████ | 1945/2132 [11:55<01:09,  2.67it/s]Evaluating:  91%|█████████▏| 1946/2132 [11:56<01:09,  2.68it/s]Evaluating:  91%|█████████▏| 1947/2132 [11:56<01:10,  2.62it/s]Evaluating:  91%|█████████▏| 1948/2132 [11:56<01:09,  2.66it/s]Evaluating:  91%|█████████▏| 1949/2132 [11:57<01:08,  2.68it/s]Evaluating:  91%|█████████▏| 1950/2132 [11:57<01:07,  2.69it/s]Evaluating:  92%|█████████▏| 1951/2132 [11:57<01:06,  2.70it/s]Evaluating:  92%|█████████▏| 1952/2132 [11:58<01:06,  2.70it/s]Evaluating:  92%|█████████▏| 1953/2132 [11:58<01:05,  2.72it/s]Evaluating:  92%|█████████▏| 1954/2132 [11:59<01:05,  2.71it/s]Evaluating:  92%|█████████▏| 1955/2132 [11:59<01:05,  2.72it/s]Evaluating:  92%|█████████▏| 1956/2132 [11:59<01:04,  2.71it/s]Evaluating:  92%|█████████▏| 1957/2132 [12:00<01:04,  2.70it/s]Evaluating:  92%|█████████▏| 1958/2132 [12:00<01:04,  2.70it/s]Evaluating:  92%|█████████▏| 1959/2132 [12:00<01:04,  2.70it/s]Evaluating:  92%|█████████▏| 1960/2132 [12:01<01:03,  2.69it/s]Evaluating:  92%|█████████▏| 1961/2132 [12:01<01:03,  2.68it/s]Evaluating:  92%|█████████▏| 1962/2132 [12:02<01:03,  2.68it/s]Evaluating:  92%|█████████▏| 1963/2132 [12:02<01:02,  2.69it/s]Evaluating:  92%|█████████▏| 1964/2132 [12:02<01:02,  2.69it/s]Evaluating:  92%|█████████▏| 1965/2132 [12:03<01:02,  2.69it/s]Evaluating:  92%|█████████▏| 1966/2132 [12:03<01:01,  2.70it/s]Evaluating:  92%|█████████▏| 1967/2132 [12:03<01:01,  2.69it/s]Evaluating:  92%|█████████▏| 1968/2132 [12:04<01:00,  2.69it/s]Evaluating:  92%|█████████▏| 1969/2132 [12:04<01:00,  2.70it/s]Evaluating:  92%|█████████▏| 1970/2132 [12:04<00:59,  2.70it/s]Evaluating:  92%|█████████▏| 1971/2132 [12:05<00:59,  2.70it/s]Evaluating:  92%|█████████▏| 1972/2132 [12:05<00:59,  2.69it/s]Evaluating:  93%|█████████▎| 1973/2132 [12:06<00:58,  2.70it/s]Evaluating:  93%|█████████▎| 1974/2132 [12:06<00:58,  2.70it/s]Evaluating:  93%|█████████▎| 1975/2132 [12:06<00:58,  2.70it/s]Evaluating:  93%|█████████▎| 1976/2132 [12:07<00:57,  2.70it/s]Evaluating:  93%|█████████▎| 1977/2132 [12:07<00:57,  2.70it/s]Evaluating:  93%|█████████▎| 1978/2132 [12:07<00:57,  2.70it/s]Evaluating:  93%|█████████▎| 1979/2132 [12:08<00:56,  2.70it/s]Evaluating:  93%|█████████▎| 1980/2132 [12:08<00:56,  2.70it/s]Evaluating:  93%|█████████▎| 1981/2132 [12:09<00:55,  2.70it/s]Evaluating:  93%|█████████▎| 1982/2132 [12:09<00:55,  2.70it/s]Evaluating:  93%|█████████▎| 1983/2132 [12:09<00:55,  2.69it/s]Evaluating:  93%|█████████▎| 1984/2132 [12:10<00:55,  2.67it/s]Evaluating:  93%|█████████▎| 1985/2132 [12:10<00:55,  2.67it/s]Evaluating:  93%|█████████▎| 1986/2132 [12:10<00:54,  2.69it/s]Evaluating:  93%|█████████▎| 1987/2132 [12:11<00:53,  2.70it/s]Evaluating:  93%|█████████▎| 1988/2132 [12:11<00:53,  2.71it/s]Evaluating:  93%|█████████▎| 1989/2132 [12:12<00:52,  2.71it/s]Evaluating:  93%|█████████▎| 1990/2132 [12:12<00:52,  2.71it/s]Evaluating:  93%|█████████▎| 1991/2132 [12:12<00:51,  2.71it/s]Evaluating:  93%|█████████▎| 1992/2132 [12:13<00:51,  2.71it/s]Evaluating:  93%|█████████▎| 1993/2132 [12:13<00:51,  2.70it/s]Evaluating:  94%|█████████▎| 1994/2132 [12:13<00:50,  2.71it/s]Evaluating:  94%|█████████▎| 1995/2132 [12:14<00:50,  2.69it/s]Evaluating:  94%|█████████▎| 1996/2132 [12:14<00:50,  2.69it/s]Evaluating:  94%|█████████▎| 1997/2132 [12:14<00:49,  2.70it/s]Evaluating:  94%|█████████▎| 1998/2132 [12:15<00:49,  2.70it/s]Evaluating:  94%|█████████▍| 1999/2132 [12:15<00:49,  2.70it/s]Evaluating:  94%|█████████▍| 2000/2132 [12:16<00:49,  2.68it/s]Evaluating:  94%|█████████▍| 2001/2132 [12:16<00:48,  2.68it/s]Evaluating:  94%|█████████▍| 2002/2132 [12:16<00:48,  2.68it/s]Evaluating:  94%|█████████▍| 2003/2132 [12:17<00:48,  2.67it/s]Evaluating:  94%|█████████▍| 2004/2132 [12:17<00:47,  2.68it/s]Evaluating:  94%|█████████▍| 2005/2132 [12:17<00:47,  2.69it/s]Evaluating:  94%|█████████▍| 2006/2132 [12:18<00:46,  2.70it/s]Evaluating:  94%|█████████▍| 2007/2132 [12:18<00:46,  2.71it/s]Evaluating:  94%|█████████▍| 2008/2132 [12:19<00:45,  2.70it/s]Evaluating:  94%|█████████▍| 2009/2132 [12:19<00:45,  2.71it/s]Evaluating:  94%|█████████▍| 2010/2132 [12:19<00:45,  2.70it/s]Evaluating:  94%|█████████▍| 2011/2132 [12:20<00:44,  2.69it/s]Evaluating:  94%|█████████▍| 2012/2132 [12:20<00:44,  2.70it/s]Evaluating:  94%|█████████▍| 2013/2132 [12:20<00:44,  2.70it/s]Evaluating:  94%|█████████▍| 2014/2132 [12:21<00:43,  2.70it/s]Evaluating:  95%|█████████▍| 2015/2132 [12:21<00:43,  2.68it/s]Evaluating:  95%|█████████▍| 2016/2132 [12:22<00:43,  2.68it/s]Evaluating:  95%|█████████▍| 2017/2132 [12:22<00:42,  2.68it/s]Evaluating:  95%|█████████▍| 2018/2132 [12:22<00:42,  2.69it/s]Evaluating:  95%|█████████▍| 2019/2132 [12:23<00:42,  2.69it/s]Evaluating:  95%|█████████▍| 2020/2132 [12:23<00:41,  2.70it/s]Evaluating:  95%|█████████▍| 2021/2132 [12:23<00:41,  2.70it/s]Evaluating:  95%|█████████▍| 2022/2132 [12:24<00:40,  2.69it/s]Evaluating:  95%|█████████▍| 2023/2132 [12:24<00:40,  2.70it/s]Evaluating:  95%|█████████▍| 2024/2132 [12:24<00:39,  2.70it/s]Evaluating:  95%|█████████▍| 2025/2132 [12:25<00:39,  2.71it/s]Evaluating:  95%|█████████▌| 2026/2132 [12:25<00:39,  2.70it/s]Evaluating:  95%|█████████▌| 2027/2132 [12:26<00:38,  2.70it/s]Evaluating:  95%|█████████▌| 2028/2132 [12:26<00:38,  2.69it/s]Evaluating:  95%|█████████▌| 2029/2132 [12:26<00:38,  2.70it/s]Evaluating:  95%|█████████▌| 2030/2132 [12:27<00:37,  2.70it/s]Evaluating:  95%|█████████▌| 2031/2132 [12:27<00:37,  2.68it/s]Evaluating:  95%|█████████▌| 2032/2132 [12:27<00:37,  2.69it/s]Evaluating:  95%|█████████▌| 2033/2132 [12:28<00:36,  2.70it/s]Evaluating:  95%|█████████▌| 2034/2132 [12:28<00:36,  2.69it/s]Evaluating:  95%|█████████▌| 2035/2132 [12:29<00:35,  2.70it/s]Evaluating:  95%|█████████▌| 2036/2132 [12:29<00:35,  2.70it/s]Evaluating:  96%|█████████▌| 2037/2132 [12:29<00:35,  2.70it/s]Evaluating:  96%|█████████▌| 2038/2132 [12:30<00:34,  2.71it/s]Evaluating:  96%|█████████▌| 2039/2132 [12:30<00:34,  2.71it/s]Evaluating:  96%|█████████▌| 2040/2132 [12:30<00:33,  2.71it/s]Evaluating:  96%|█████████▌| 2041/2132 [12:31<00:33,  2.71it/s]Evaluating:  96%|█████████▌| 2042/2132 [12:31<00:33,  2.72it/s]Evaluating:  96%|█████████▌| 2043/2132 [12:32<00:32,  2.72it/s]Evaluating:  96%|█████████▌| 2044/2132 [12:32<00:32,  2.72it/s]Evaluating:  96%|█████████▌| 2045/2132 [12:32<00:32,  2.71it/s]Evaluating:  96%|█████████▌| 2046/2132 [12:33<00:31,  2.70it/s]Evaluating:  96%|█████████▌| 2047/2132 [12:33<00:31,  2.71it/s]Evaluating:  96%|█████████▌| 2048/2132 [12:33<00:31,  2.71it/s]Evaluating:  96%|█████████▌| 2049/2132 [12:34<00:30,  2.70it/s]Evaluating:  96%|█████████▌| 2050/2132 [12:34<00:30,  2.71it/s]Evaluating:  96%|█████████▌| 2051/2132 [12:34<00:30,  2.69it/s]Evaluating:  96%|█████████▌| 2052/2132 [12:35<00:29,  2.69it/s]Evaluating:  96%|█████████▋| 2053/2132 [12:35<00:29,  2.69it/s]Evaluating:  96%|█████████▋| 2054/2132 [12:36<00:28,  2.69it/s]Evaluating:  96%|█████████▋| 2055/2132 [12:36<00:28,  2.71it/s]Evaluating:  96%|█████████▋| 2056/2132 [12:36<00:27,  2.71it/s]Evaluating:  96%|█████████▋| 2057/2132 [12:37<00:27,  2.73it/s]Evaluating:  97%|█████████▋| 2058/2132 [12:37<00:27,  2.73it/s]Evaluating:  97%|█████████▋| 2059/2132 [12:37<00:26,  2.73it/s]Evaluating:  97%|█████████▋| 2060/2132 [12:38<00:26,  2.73it/s]Evaluating:  97%|█████████▋| 2061/2132 [12:38<00:26,  2.73it/s]Evaluating:  97%|█████████▋| 2062/2132 [12:39<00:25,  2.72it/s]Evaluating:  97%|█████████▋| 2063/2132 [12:39<00:25,  2.72it/s]Evaluating:  97%|█████████▋| 2064/2132 [12:39<00:25,  2.71it/s]Evaluating:  97%|█████████▋| 2065/2132 [12:40<00:24,  2.72it/s]Evaluating:  97%|█████████▋| 2066/2132 [12:40<00:24,  2.71it/s]Evaluating:  97%|█████████▋| 2067/2132 [12:40<00:23,  2.72it/s]Evaluating:  97%|█████████▋| 2068/2132 [12:41<00:23,  2.71it/s]Evaluating:  97%|█████████▋| 2069/2132 [12:41<00:23,  2.71it/s]Evaluating:  97%|█████████▋| 2070/2132 [12:41<00:22,  2.70it/s]Evaluating:  97%|█████████▋| 2071/2132 [12:42<00:22,  2.71it/s]Evaluating:  97%|█████████▋| 2072/2132 [12:42<00:22,  2.71it/s]Evaluating:  97%|█████████▋| 2073/2132 [12:43<00:21,  2.72it/s]Evaluating:  97%|█████████▋| 2074/2132 [12:43<00:21,  2.71it/s]Evaluating:  97%|█████████▋| 2075/2132 [12:43<00:20,  2.72it/s]Evaluating:  97%|█████████▋| 2076/2132 [12:44<00:20,  2.71it/s]Evaluating:  97%|█████████▋| 2077/2132 [12:44<00:20,  2.71it/s]Evaluating:  97%|█████████▋| 2078/2132 [12:44<00:19,  2.71it/s]Evaluating:  98%|█████████▊| 2079/2132 [12:45<00:19,  2.72it/s]Evaluating:  98%|█████████▊| 2080/2132 [12:45<00:19,  2.71it/s]Evaluating:  98%|█████████▊| 2081/2132 [12:46<00:18,  2.71it/s]Evaluating:  98%|█████████▊| 2082/2132 [12:46<00:18,  2.70it/s]Evaluating:  98%|█████████▊| 2083/2132 [12:46<00:18,  2.70it/s]Evaluating:  98%|█████████▊| 2084/2132 [12:47<00:17,  2.70it/s]Evaluating:  98%|█████████▊| 2085/2132 [12:47<00:17,  2.70it/s]Evaluating:  98%|█████████▊| 2086/2132 [12:47<00:17,  2.70it/s]Evaluating:  98%|█████████▊| 2087/2132 [12:48<00:16,  2.69it/s]Evaluating:  98%|█████████▊| 2088/2132 [12:48<00:16,  2.69it/s]Evaluating:  98%|█████████▊| 2089/2132 [12:49<00:16,  2.68it/s]Evaluating:  98%|█████████▊| 2090/2132 [12:49<00:15,  2.69it/s]Evaluating:  98%|█████████▊| 2091/2132 [12:49<00:15,  2.70it/s]Evaluating:  98%|█████████▊| 2092/2132 [12:50<00:14,  2.70it/s]Evaluating:  98%|█████████▊| 2093/2132 [12:50<00:14,  2.70it/s]Evaluating:  98%|█████████▊| 2094/2132 [12:50<00:14,  2.70it/s]Evaluating:  98%|█████████▊| 2095/2132 [12:51<00:13,  2.69it/s]Evaluating:  98%|█████████▊| 2096/2132 [12:51<00:13,  2.69it/s]Evaluating:  98%|█████████▊| 2097/2132 [12:51<00:12,  2.69it/s]Evaluating:  98%|█████████▊| 2098/2132 [12:52<00:12,  2.70it/s]Evaluating:  98%|█████████▊| 2099/2132 [12:52<00:12,  2.69it/s]Evaluating:  98%|█████████▊| 2100/2132 [12:53<00:11,  2.70it/s]Evaluating:  99%|█████████▊| 2101/2132 [12:53<00:11,  2.70it/s]Evaluating:  99%|█████████▊| 2102/2132 [12:53<00:11,  2.70it/s]Evaluating:  99%|█████████▊| 2103/2132 [12:54<00:10,  2.71it/s]Evaluating:  99%|█████████▊| 2104/2132 [12:54<00:10,  2.71it/s]Evaluating:  99%|█████████▊| 2105/2132 [12:54<00:09,  2.71it/s]Evaluating:  99%|█████████▉| 2106/2132 [12:55<00:09,  2.71it/s]Evaluating:  99%|█████████▉| 2107/2132 [12:55<00:09,  2.71it/s]Evaluating:  99%|█████████▉| 2108/2132 [12:56<00:08,  2.72it/s]Evaluating:  99%|█████████▉| 2109/2132 [12:56<00:08,  2.71it/s]Evaluating:  99%|█████████▉| 2110/2132 [12:56<00:08,  2.70it/s]Evaluating:  99%|█████████▉| 2111/2132 [12:57<00:07,  2.71it/s]Evaluating:  99%|█████████▉| 2112/2132 [12:57<00:07,  2.71it/s]Evaluating:  99%|█████████▉| 2113/2132 [12:57<00:07,  2.71it/s]Evaluating:  99%|█████████▉| 2114/2132 [12:58<00:06,  2.71it/s]Evaluating:  99%|█████████▉| 2115/2132 [12:58<00:06,  2.71it/s]Evaluating:  99%|█████████▉| 2116/2132 [12:58<00:05,  2.71it/s]Evaluating:  99%|█████████▉| 2117/2132 [12:59<00:05,  2.71it/s]Evaluating:  99%|█████████▉| 2118/2132 [12:59<00:05,  2.71it/s]Evaluating:  99%|█████████▉| 2119/2132 [13:00<00:04,  2.70it/s]Evaluating:  99%|█████████▉| 2120/2132 [13:00<00:04,  2.71it/s]Evaluating:  99%|█████████▉| 2121/2132 [13:00<00:04,  2.72it/s]Evaluating: 100%|█████████▉| 2122/2132 [13:01<00:03,  2.72it/s]Evaluating: 100%|█████████▉| 2123/2132 [13:01<00:03,  2.72it/s]Evaluating: 100%|█████████▉| 2124/2132 [13:01<00:02,  2.69it/s]Evaluating: 100%|█████████▉| 2125/2132 [13:02<00:02,  2.69it/s]Evaluating: 100%|█████████▉| 2126/2132 [13:02<00:02,  2.71it/s]Evaluating: 100%|█████████▉| 2127/2132 [13:03<00:01,  2.70it/s]Evaluating: 100%|█████████▉| 2128/2132 [13:03<00:01,  2.70it/s]Evaluating: 100%|█████████▉| 2129/2132 [13:03<00:01,  2.70it/s]Evaluating: 100%|█████████▉| 2130/2132 [13:04<00:00,  2.71it/s]Evaluating: 100%|█████████▉| 2131/2132 [13:04<00:00,  2.71it/s]Evaluating: 100%|██████████| 2132/2132 [13:04<00:00,  2.94it/s]Evaluating: 100%|██████████| 2132/2132 [13:04<00:00,  2.72it/s]
05/10/2022 00:02:17 - INFO - __main__ -     Evaluation done in total 784.811014 secs (0.046019 sec per example)
05/10/2022 00:03:10 - INFO - __main__ -   Results: {'exact': 70.75927523727351, 'f1': 83.61750440610864, 'total': 11590, 'HasAns_exact': 70.75927523727351, 'HasAns_f1': 83.61750440610864, 'HasAns_total': 11590, 'best_exact': 70.75927523727351, 'best_exact_thresh': 0.0, 'best_f1': 83.61750440610864, 'best_f1_thresh': 0.0}
  es 
2022-05-10 00:03:14.177096: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/10/2022 00:03:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'qa_outputs.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'qa_outputs.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.18.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.21.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.14.attention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:03:41 - INFO - __main__ -   lang2id = None
05/10/2022 00:03:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='es', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-es-question-es.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/10/2022 00:03:45 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/10/2022 00:03:45 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'qa_outputs.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'qa_outputs.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.18.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.21.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.14.attention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:04:13 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/2503 [00:00<?, ?it/s]  2%|▏         | 62/2503 [00:00<00:04, 609.40it/s]  9%|▉         | 225/2503 [00:00<00:01, 1205.16it/s] 14%|█▍        | 346/2503 [00:00<00:02, 851.28it/s]  20%|█▉        | 498/2503 [00:00<00:01, 1056.17it/s] 30%|██▉       | 741/2503 [00:00<00:01, 1479.67it/s] 36%|███▌      | 903/2503 [00:00<00:01, 1379.64it/s] 42%|████▏     | 1051/2503 [00:00<00:01, 1215.60it/s] 47%|████▋     | 1182/2503 [00:01<00:01, 1138.30it/s] 55%|█████▌    | 1388/2503 [00:01<00:00, 1370.95it/s] 61%|██████▏   | 1535/2503 [00:01<00:00, 1297.40it/s] 67%|██████▋   | 1672/2503 [00:01<00:00, 1082.83it/s] 72%|███████▏  | 1790/2503 [00:01<00:00, 1083.13it/s] 76%|███████▌  | 1905/2503 [00:01<00:00, 1078.23it/s] 81%|████████  | 2018/2503 [00:01<00:00, 959.23it/s]  86%|████████▌ | 2142/2503 [00:01<00:00, 1026.63it/s] 96%|█████████▌| 2402/2503 [00:02<00:00, 1428.38it/s]100%|██████████| 2503/2503 [00:02<00:00, 1220.75it/s]
convert squad examples to features:   0%|          | 0/5253 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/5253 [00:00<15:51,  5.52it/s]convert squad examples to features:   1%|          | 33/5253 [00:00<00:43, 121.13it/s]convert squad examples to features:   1%|          | 65/5253 [00:00<00:36, 142.16it/s]convert squad examples to features:   2%|▏         | 97/5253 [00:00<00:32, 157.32it/s]convert squad examples to features:   2%|▏         | 129/5253 [00:00<00:29, 175.48it/s]convert squad examples to features:   3%|▎         | 161/5253 [00:01<00:29, 172.10it/s]convert squad examples to features:   4%|▎         | 193/5253 [00:01<00:30, 167.74it/s]convert squad examples to features:   4%|▍         | 225/5253 [00:01<00:26, 187.77it/s]convert squad examples to features:   5%|▍         | 257/5253 [00:01<00:28, 175.73it/s]convert squad examples to features:   6%|▌         | 289/5253 [00:01<00:24, 200.82it/s]convert squad examples to features:   6%|▌         | 321/5253 [00:01<00:24, 203.41it/s]convert squad examples to features:   7%|▋         | 385/5253 [00:02<00:20, 235.40it/s]convert squad examples to features:   8%|▊         | 417/5253 [00:02<00:19, 247.69it/s]convert squad examples to features:   9%|▊         | 449/5253 [00:02<00:18, 257.92it/s]convert squad examples to features:   9%|▉         | 481/5253 [00:02<00:19, 247.73it/s]convert squad examples to features:  10%|▉         | 513/5253 [00:02<00:18, 256.96it/s]convert squad examples to features:  11%|█         | 577/5253 [00:02<00:19, 245.68it/s]convert squad examples to features:  12%|█▏        | 609/5253 [00:02<00:19, 233.85it/s]convert squad examples to features:  12%|█▏        | 641/5253 [00:03<00:21, 218.52it/s]convert squad examples to features:  13%|█▎        | 673/5253 [00:03<00:21, 211.86it/s]convert squad examples to features:  13%|█▎        | 705/5253 [00:03<00:20, 221.06it/s]convert squad examples to features:  14%|█▍        | 737/5253 [00:03<00:23, 196.19it/s]convert squad examples to features:  15%|█▌        | 801/5253 [00:03<00:19, 233.12it/s]convert squad examples to features:  16%|█▌        | 833/5253 [00:04<00:22, 200.43it/s]convert squad examples to features:  16%|█▋        | 865/5253 [00:04<00:23, 185.23it/s]convert squad examples to features:  17%|█▋        | 897/5253 [00:04<00:21, 202.47it/s]convert squad examples to features:  18%|█▊        | 929/5253 [00:04<00:21, 204.46it/s]convert squad examples to features:  18%|█▊        | 961/5253 [00:04<00:20, 212.95it/s]convert squad examples to features:  19%|█▉        | 993/5253 [00:04<00:20, 207.69it/s]convert squad examples to features:  20%|█▉        | 1025/5253 [00:05<00:21, 195.49it/s]convert squad examples to features:  20%|██        | 1057/5253 [00:05<00:21, 198.12it/s]convert squad examples to features:  21%|██        | 1089/5253 [00:05<00:20, 201.09it/s]convert squad examples to features:  21%|██▏       | 1121/5253 [00:05<00:20, 203.30it/s]convert squad examples to features:  22%|██▏       | 1153/5253 [00:05<00:17, 227.92it/s]convert squad examples to features:  23%|██▎       | 1185/5253 [00:05<00:20, 201.25it/s]convert squad examples to features:  23%|██▎       | 1217/5253 [00:05<00:19, 203.96it/s]convert squad examples to features:  24%|██▍       | 1249/5253 [00:06<00:19, 207.51it/s]convert squad examples to features:  24%|██▍       | 1281/5253 [00:06<00:18, 209.89it/s]convert squad examples to features:  25%|██▍       | 1313/5253 [00:06<00:19, 200.49it/s]convert squad examples to features:  26%|██▌       | 1345/5253 [00:06<00:19, 205.36it/s]convert squad examples to features:  26%|██▌       | 1377/5253 [00:06<00:17, 227.31it/s]convert squad examples to features:  27%|██▋       | 1409/5253 [00:06<00:17, 225.28it/s]convert squad examples to features:  27%|██▋       | 1441/5253 [00:06<00:16, 231.26it/s]convert squad examples to features:  28%|██▊       | 1473/5253 [00:07<00:17, 218.73it/s]convert squad examples to features:  29%|██▊       | 1505/5253 [00:07<00:17, 211.49it/s]convert squad examples to features:  29%|██▉       | 1537/5253 [00:07<00:16, 221.43it/s]convert squad examples to features:  30%|██▉       | 1569/5253 [00:07<00:15, 233.49it/s]convert squad examples to features:  30%|███       | 1601/5253 [00:07<00:15, 237.51it/s]convert squad examples to features:  31%|███       | 1633/5253 [00:07<00:14, 242.49it/s]convert squad examples to features:  32%|███▏      | 1697/5253 [00:08<00:13, 264.23it/s]convert squad examples to features:  33%|███▎      | 1729/5253 [00:08<00:17, 200.51it/s]convert squad examples to features:  34%|███▎      | 1761/5253 [00:08<00:19, 175.74it/s]convert squad examples to features:  34%|███▍      | 1793/5253 [00:08<00:18, 189.30it/s]convert squad examples to features:  35%|███▍      | 1825/5253 [00:08<00:19, 179.86it/s]convert squad examples to features:  35%|███▌      | 1857/5253 [00:09<00:18, 186.69it/s]convert squad examples to features:  36%|███▌      | 1889/5253 [00:09<00:24, 137.11it/s]convert squad examples to features:  37%|███▋      | 1921/5253 [00:09<00:23, 143.14it/s]convert squad examples to features:  38%|███▊      | 1985/5253 [00:10<00:24, 134.73it/s]convert squad examples to features:  39%|███▉      | 2049/5253 [00:10<00:18, 168.95it/s]convert squad examples to features:  40%|███▉      | 2081/5253 [00:10<00:18, 171.17it/s]convert squad examples to features:  40%|████      | 2113/5253 [00:10<00:18, 171.38it/s]convert squad examples to features:  41%|████      | 2145/5253 [00:10<00:17, 175.81it/s]convert squad examples to features:  41%|████▏     | 2177/5253 [00:11<00:16, 191.36it/s]convert squad examples to features:  42%|████▏     | 2209/5253 [00:11<00:15, 190.79it/s]convert squad examples to features:  43%|████▎     | 2241/5253 [00:11<00:15, 198.97it/s]convert squad examples to features:  43%|████▎     | 2273/5253 [00:11<00:13, 214.41it/s]convert squad examples to features:  44%|████▍     | 2305/5253 [00:11<00:14, 202.44it/s]convert squad examples to features:  44%|████▍     | 2337/5253 [00:11<00:15, 184.94it/s]convert squad examples to features:  45%|████▌     | 2369/5253 [00:11<00:14, 201.35it/s]convert squad examples to features:  46%|████▌     | 2401/5253 [00:12<00:12, 224.10it/s]convert squad examples to features:  46%|████▋     | 2433/5253 [00:12<00:13, 214.96it/s]convert squad examples to features:  47%|████▋     | 2465/5253 [00:12<00:15, 184.44it/s]convert squad examples to features:  48%|████▊     | 2497/5253 [00:12<00:14, 194.22it/s]convert squad examples to features:  48%|████▊     | 2529/5253 [00:12<00:14, 184.59it/s]convert squad examples to features:  49%|████▉     | 2561/5253 [00:12<00:14, 182.44it/s]convert squad examples to features:  49%|████▉     | 2593/5253 [00:13<00:13, 190.52it/s]convert squad examples to features:  50%|████▉     | 2625/5253 [00:13<00:12, 208.56it/s]convert squad examples to features:  51%|█████     | 2657/5253 [00:13<00:13, 197.86it/s]convert squad examples to features:  51%|█████     | 2689/5253 [00:13<00:12, 212.19it/s]convert squad examples to features:  52%|█████▏    | 2721/5253 [00:13<00:13, 192.68it/s]convert squad examples to features:  52%|█████▏    | 2753/5253 [00:13<00:12, 200.32it/s]convert squad examples to features:  53%|█████▎    | 2785/5253 [00:14<00:11, 217.85it/s]convert squad examples to features:  54%|█████▎    | 2817/5253 [00:14<00:11, 220.67it/s]convert squad examples to features:  54%|█████▍    | 2849/5253 [00:14<00:11, 201.31it/s]convert squad examples to features:  55%|█████▍    | 2881/5253 [00:14<00:10, 218.10it/s]convert squad examples to features:  55%|█████▌    | 2913/5253 [00:14<00:10, 228.15it/s]convert squad examples to features:  57%|█████▋    | 2977/5253 [00:14<00:09, 249.82it/s]convert squad examples to features:  57%|█████▋    | 3009/5253 [00:14<00:08, 257.45it/s]convert squad examples to features:  58%|█████▊    | 3041/5253 [00:15<00:08, 270.14it/s]convert squad examples to features:  58%|█████▊    | 3073/5253 [00:15<00:08, 246.35it/s]convert squad examples to features:  59%|█████▉    | 3105/5253 [00:15<00:09, 231.63it/s]convert squad examples to features:  60%|█████▉    | 3137/5253 [00:15<00:09, 233.40it/s]convert squad examples to features:  60%|██████    | 3169/5253 [00:15<00:09, 217.40it/s]convert squad examples to features:  62%|██████▏   | 3233/5253 [00:15<00:09, 220.02it/s]convert squad examples to features:  62%|██████▏   | 3265/5253 [00:16<00:09, 219.92it/s]convert squad examples to features:  63%|██████▎   | 3297/5253 [00:16<00:09, 213.48it/s]convert squad examples to features:  63%|██████▎   | 3329/5253 [00:16<00:08, 213.85it/s]convert squad examples to features:  64%|██████▍   | 3361/5253 [00:16<00:09, 204.50it/s]convert squad examples to features:  65%|██████▍   | 3393/5253 [00:16<00:08, 226.64it/s]convert squad examples to features:  65%|██████▌   | 3425/5253 [00:16<00:08, 225.13it/s]convert squad examples to features:  66%|██████▌   | 3457/5253 [00:16<00:07, 236.11it/s]convert squad examples to features:  66%|██████▋   | 3489/5253 [00:17<00:07, 228.73it/s]convert squad examples to features:  67%|██████▋   | 3521/5253 [00:17<00:08, 211.05it/s]convert squad examples to features:  68%|██████▊   | 3585/5253 [00:17<00:07, 228.16it/s]convert squad examples to features:  69%|██████▉   | 3617/5253 [00:17<00:07, 228.31it/s]convert squad examples to features:  69%|██████▉   | 3649/5253 [00:17<00:08, 186.18it/s]convert squad examples to features:  70%|███████   | 3681/5253 [00:18<00:08, 192.46it/s]convert squad examples to features:  71%|███████   | 3713/5253 [00:18<00:07, 201.57it/s]convert squad examples to features:  72%|███████▏  | 3777/5253 [00:18<00:07, 192.68it/s]convert squad examples to features:  73%|███████▎  | 3809/5253 [00:18<00:07, 202.62it/s]convert squad examples to features:  73%|███████▎  | 3841/5253 [00:18<00:07, 197.09it/s]convert squad examples to features:  74%|███████▎  | 3873/5253 [00:19<00:07, 192.13it/s]convert squad examples to features:  74%|███████▍  | 3905/5253 [00:19<00:07, 192.20it/s]convert squad examples to features:  75%|███████▍  | 3937/5253 [00:19<00:06, 202.24it/s]convert squad examples to features:  76%|███████▌  | 3969/5253 [00:19<00:05, 218.04it/s]convert squad examples to features:  76%|███████▌  | 4001/5253 [00:19<00:05, 213.89it/s]convert squad examples to features:  77%|███████▋  | 4033/5253 [00:19<00:05, 227.64it/s]convert squad examples to features:  77%|███████▋  | 4065/5253 [00:19<00:05, 226.79it/s]convert squad examples to features:  78%|███████▊  | 4097/5253 [00:20<00:04, 233.14it/s]convert squad examples to features:  79%|███████▉  | 4161/5253 [00:20<00:04, 233.78it/s]convert squad examples to features:  80%|███████▉  | 4193/5253 [00:20<00:05, 208.98it/s]convert squad examples to features:  81%|████████  | 4257/5253 [00:20<00:04, 236.71it/s]convert squad examples to features:  82%|████████▏ | 4289/5253 [00:20<00:04, 222.48it/s]convert squad examples to features:  82%|████████▏ | 4321/5253 [00:21<00:03, 236.12it/s]convert squad examples to features:  83%|████████▎ | 4353/5253 [00:21<00:03, 245.05it/s]convert squad examples to features:  84%|████████▍ | 4417/5253 [00:21<00:03, 268.27it/s]convert squad examples to features:  85%|████████▍ | 4449/5253 [00:21<00:03, 248.07it/s]convert squad examples to features:  85%|████████▌ | 4481/5253 [00:21<00:03, 219.58it/s]convert squad examples to features:  86%|████████▌ | 4513/5253 [00:21<00:03, 194.80it/s]convert squad examples to features:  87%|████████▋ | 4545/5253 [00:22<00:03, 205.36it/s]convert squad examples to features:  87%|████████▋ | 4577/5253 [00:22<00:03, 186.29it/s]convert squad examples to features:  88%|████████▊ | 4609/5253 [00:22<00:03, 183.53it/s]convert squad examples to features:  88%|████████▊ | 4641/5253 [00:22<00:03, 175.75it/s]convert squad examples to features:  89%|████████▉ | 4673/5253 [00:22<00:03, 189.11it/s]convert squad examples to features:  90%|████████▉ | 4705/5253 [00:22<00:02, 192.55it/s]convert squad examples to features:  90%|█████████ | 4737/5253 [00:23<00:02, 203.61it/s]convert squad examples to features:  91%|█████████ | 4769/5253 [00:23<00:02, 227.55it/s]convert squad examples to features:  91%|█████████▏| 4801/5253 [00:23<00:01, 232.83it/s]convert squad examples to features:  92%|█████████▏| 4833/5253 [00:23<00:01, 243.36it/s]convert squad examples to features:  93%|█████████▎| 4865/5253 [00:23<00:01, 247.48it/s]convert squad examples to features:  93%|█████████▎| 4897/5253 [00:23<00:01, 238.92it/s]convert squad examples to features:  94%|█████████▍| 4929/5253 [00:23<00:01, 240.89it/s]convert squad examples to features:  95%|█████████▌| 4993/5253 [00:23<00:00, 283.58it/s]convert squad examples to features:  96%|█████████▌| 5025/5253 [00:24<00:00, 251.83it/s]convert squad examples to features:  96%|█████████▋| 5057/5253 [00:24<00:00, 252.44it/s]convert squad examples to features:  97%|█████████▋| 5121/5253 [00:24<00:00, 288.23it/s]convert squad examples to features:  98%|█████████▊| 5153/5253 [00:24<00:00, 283.74it/s]convert squad examples to features:  99%|█████████▊| 5185/5253 [00:24<00:00, 286.18it/s]convert squad examples to features:  99%|█████████▉| 5217/5253 [00:24<00:00, 279.69it/s]convert squad examples to features: 100%|██████████| 5253/5253 [00:24<00:00, 211.51it/s]
add example index and unique id:   0%|          | 0/5253 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 5253/5253 [00:00<00:00, 545985.01it/s]
05/10/2022 00:04:40 - INFO - __main__ -   Saving features into cached file ./cached_test-context-es-question-es.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_es
05/10/2022 00:04:46 - INFO - __main__ -   ***** Running evaluation  *****
05/10/2022 00:04:46 - INFO - __main__ -     Num examples = 5532
05/10/2022 00:04:46 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/692 [00:00<?, ?it/s]Evaluating:   0%|          | 1/692 [00:00<10:48,  1.06it/s]Evaluating:   0%|          | 2/692 [00:01<06:52,  1.67it/s]Evaluating:   0%|          | 3/692 [00:01<05:37,  2.04it/s]Evaluating:   1%|          | 4/692 [00:02<05:02,  2.27it/s]Evaluating:   1%|          | 5/692 [00:02<04:42,  2.43it/s]Evaluating:   1%|          | 6/692 [00:02<04:29,  2.54it/s]Evaluating:   1%|          | 7/692 [00:03<04:22,  2.61it/s]Evaluating:   1%|          | 8/692 [00:03<04:17,  2.66it/s]Evaluating:   1%|▏         | 9/692 [00:03<04:14,  2.68it/s]Evaluating:   1%|▏         | 10/692 [00:04<04:12,  2.70it/s]Evaluating:   2%|▏         | 11/692 [00:04<04:10,  2.72it/s]Evaluating:   2%|▏         | 12/692 [00:04<04:08,  2.74it/s]Evaluating:   2%|▏         | 13/692 [00:05<04:07,  2.74it/s]Evaluating:   2%|▏         | 14/692 [00:05<04:07,  2.74it/s]Evaluating:   2%|▏         | 15/692 [00:06<04:06,  2.75it/s]Evaluating:   2%|▏         | 16/692 [00:06<04:05,  2.76it/s]Evaluating:   2%|▏         | 17/692 [00:06<04:04,  2.76it/s]Evaluating:   3%|▎         | 18/692 [00:07<04:04,  2.75it/s]Evaluating:   3%|▎         | 19/692 [00:07<04:03,  2.76it/s]Evaluating:   3%|▎         | 20/692 [00:07<04:04,  2.75it/s]Evaluating:   3%|▎         | 21/692 [00:08<04:04,  2.75it/s]Evaluating:   3%|▎         | 22/692 [00:08<04:03,  2.75it/s]Evaluating:   3%|▎         | 23/692 [00:08<04:03,  2.75it/s]Evaluating:   3%|▎         | 24/692 [00:09<04:02,  2.75it/s]Evaluating:   4%|▎         | 25/692 [00:09<04:03,  2.74it/s]Evaluating:   4%|▍         | 26/692 [00:10<04:02,  2.75it/s]Evaluating:   4%|▍         | 27/692 [00:10<04:01,  2.75it/s]Evaluating:   4%|▍         | 28/692 [00:10<04:01,  2.74it/s]Evaluating:   4%|▍         | 29/692 [00:11<04:01,  2.74it/s]Evaluating:   4%|▍         | 30/692 [00:11<04:01,  2.75it/s]Evaluating:   4%|▍         | 31/692 [00:11<04:01,  2.74it/s]Evaluating:   5%|▍         | 32/692 [00:12<04:00,  2.75it/s]Evaluating:   5%|▍         | 33/692 [00:12<04:00,  2.75it/s]Evaluating:   5%|▍         | 34/692 [00:12<04:00,  2.74it/s]Evaluating:   5%|▌         | 35/692 [00:13<03:59,  2.74it/s]Evaluating:   5%|▌         | 36/692 [00:13<03:59,  2.74it/s]Evaluating:   5%|▌         | 37/692 [00:14<03:59,  2.73it/s]Evaluating:   5%|▌         | 38/692 [00:14<03:58,  2.74it/s]Evaluating:   6%|▌         | 39/692 [00:14<03:57,  2.75it/s]Evaluating:   6%|▌         | 40/692 [00:15<03:56,  2.76it/s]Evaluating:   6%|▌         | 41/692 [00:15<03:55,  2.76it/s]Evaluating:   6%|▌         | 42/692 [00:15<03:54,  2.77it/s]Evaluating:   6%|▌         | 43/692 [00:16<03:53,  2.77it/s]Evaluating:   6%|▋         | 44/692 [00:16<03:53,  2.77it/s]Evaluating:   7%|▋         | 45/692 [00:16<03:54,  2.76it/s]Evaluating:   7%|▋         | 46/692 [00:17<03:54,  2.76it/s]Evaluating:   7%|▋         | 47/692 [00:17<03:54,  2.75it/s]Evaluating:   7%|▋         | 48/692 [00:18<03:54,  2.75it/s]Evaluating:   7%|▋         | 49/692 [00:18<03:54,  2.74it/s]Evaluating:   7%|▋         | 50/692 [00:18<03:53,  2.74it/s]Evaluating:   7%|▋         | 51/692 [00:19<03:53,  2.75it/s]Evaluating:   8%|▊         | 52/692 [00:19<03:52,  2.75it/s]Evaluating:   8%|▊         | 53/692 [00:19<03:51,  2.76it/s]Evaluating:   8%|▊         | 54/692 [00:20<03:51,  2.76it/s]Evaluating:   8%|▊         | 55/692 [00:20<03:50,  2.76it/s]Evaluating:   8%|▊         | 56/692 [00:20<03:50,  2.76it/s]Evaluating:   8%|▊         | 57/692 [00:21<03:49,  2.77it/s]Evaluating:   8%|▊         | 58/692 [00:21<03:49,  2.76it/s]Evaluating:   9%|▊         | 59/692 [00:21<03:48,  2.77it/s]Evaluating:   9%|▊         | 60/692 [00:22<03:48,  2.76it/s]Evaluating:   9%|▉         | 61/692 [00:22<03:48,  2.76it/s]Evaluating:   9%|▉         | 62/692 [00:23<03:49,  2.75it/s]Evaluating:   9%|▉         | 63/692 [00:23<03:48,  2.75it/s]Evaluating:   9%|▉         | 64/692 [00:23<03:47,  2.76it/s]Evaluating:   9%|▉         | 65/692 [00:24<03:47,  2.75it/s]Evaluating:  10%|▉         | 66/692 [00:24<03:47,  2.76it/s]Evaluating:  10%|▉         | 67/692 [00:24<03:46,  2.76it/s]Evaluating:  10%|▉         | 68/692 [00:25<03:47,  2.75it/s]Evaluating:  10%|▉         | 69/692 [00:25<03:47,  2.74it/s]Evaluating:  10%|█         | 70/692 [00:25<03:46,  2.74it/s]Evaluating:  10%|█         | 71/692 [00:26<03:46,  2.75it/s]Evaluating:  10%|█         | 72/692 [00:26<03:45,  2.75it/s]Evaluating:  11%|█         | 73/692 [00:27<03:45,  2.74it/s]Evaluating:  11%|█         | 74/692 [00:27<03:44,  2.75it/s]Evaluating:  11%|█         | 75/692 [00:27<03:44,  2.75it/s]Evaluating:  11%|█         | 76/692 [00:28<03:44,  2.74it/s]Evaluating:  11%|█         | 77/692 [00:28<03:44,  2.74it/s]Evaluating:  11%|█▏        | 78/692 [00:28<03:44,  2.73it/s]Evaluating:  11%|█▏        | 79/692 [00:29<03:43,  2.74it/s]Evaluating:  12%|█▏        | 80/692 [00:29<03:44,  2.73it/s]Evaluating:  12%|█▏        | 81/692 [00:30<03:44,  2.73it/s]Evaluating:  12%|█▏        | 82/692 [00:30<03:44,  2.72it/s]Evaluating:  12%|█▏        | 83/692 [00:30<03:43,  2.72it/s]Evaluating:  12%|█▏        | 84/692 [00:31<03:44,  2.71it/s]Evaluating:  12%|█▏        | 85/692 [00:31<03:43,  2.71it/s]Evaluating:  12%|█▏        | 86/692 [00:31<03:41,  2.73it/s]Evaluating:  13%|█▎        | 87/692 [00:32<03:41,  2.73it/s]Evaluating:  13%|█▎        | 88/692 [00:32<03:41,  2.72it/s]Evaluating:  13%|█▎        | 89/692 [00:32<03:41,  2.72it/s]Evaluating:  13%|█▎        | 90/692 [00:33<03:40,  2.72it/s]Evaluating:  13%|█▎        | 91/692 [00:33<03:40,  2.72it/s]Evaluating:  13%|█▎        | 92/692 [00:34<03:40,  2.73it/s]Evaluating:  13%|█▎        | 93/692 [00:34<03:38,  2.74it/s]Evaluating:  14%|█▎        | 94/692 [00:34<03:38,  2.74it/s]Evaluating:  14%|█▎        | 95/692 [00:35<03:37,  2.74it/s]Evaluating:  14%|█▍        | 96/692 [00:35<03:36,  2.75it/s]Evaluating:  14%|█▍        | 97/692 [00:35<03:36,  2.74it/s]Evaluating:  14%|█▍        | 98/692 [00:36<03:36,  2.74it/s]Evaluating:  14%|█▍        | 99/692 [00:36<03:37,  2.73it/s]Evaluating:  14%|█▍        | 100/692 [00:36<03:37,  2.73it/s]Evaluating:  15%|█▍        | 101/692 [00:37<03:37,  2.72it/s]Evaluating:  15%|█▍        | 102/692 [00:37<03:36,  2.73it/s]Evaluating:  15%|█▍        | 103/692 [00:38<03:35,  2.74it/s]Evaluating:  15%|█▌        | 104/692 [00:38<03:34,  2.75it/s]Evaluating:  15%|█▌        | 105/692 [00:38<03:34,  2.74it/s]Evaluating:  15%|█▌        | 106/692 [00:39<03:33,  2.74it/s]Evaluating:  15%|█▌        | 107/692 [00:39<03:33,  2.74it/s]Evaluating:  16%|█▌        | 108/692 [00:39<03:33,  2.74it/s]Evaluating:  16%|█▌        | 109/692 [00:40<03:32,  2.74it/s]Evaluating:  16%|█▌        | 110/692 [00:40<03:32,  2.74it/s]Evaluating:  16%|█▌        | 111/692 [00:40<03:32,  2.74it/s]Evaluating:  16%|█▌        | 112/692 [00:41<03:31,  2.74it/s]Evaluating:  16%|█▋        | 113/692 [00:41<03:32,  2.73it/s]Evaluating:  16%|█▋        | 114/692 [00:42<03:32,  2.73it/s]Evaluating:  17%|█▋        | 115/692 [00:42<03:37,  2.65it/s]Evaluating:  17%|█▋        | 116/692 [00:42<03:35,  2.68it/s]Evaluating:  17%|█▋        | 117/692 [00:43<03:34,  2.69it/s]Evaluating:  17%|█▋        | 118/692 [00:43<03:33,  2.69it/s]Evaluating:  17%|█▋        | 119/692 [00:43<03:31,  2.72it/s]Evaluating:  17%|█▋        | 120/692 [00:44<03:29,  2.73it/s]Evaluating:  17%|█▋        | 121/692 [00:44<03:29,  2.73it/s]Evaluating:  18%|█▊        | 122/692 [00:45<03:28,  2.73it/s]Evaluating:  18%|█▊        | 123/692 [00:45<03:27,  2.74it/s]Evaluating:  18%|█▊        | 124/692 [00:45<03:26,  2.75it/s]Evaluating:  18%|█▊        | 125/692 [00:46<03:26,  2.75it/s]Evaluating:  18%|█▊        | 126/692 [00:46<03:26,  2.75it/s]Evaluating:  18%|█▊        | 127/692 [00:46<03:26,  2.74it/s]Evaluating:  18%|█▊        | 128/692 [00:47<03:27,  2.72it/s]Evaluating:  19%|█▊        | 129/692 [00:47<03:26,  2.73it/s]Evaluating:  19%|█▉        | 130/692 [00:47<03:26,  2.72it/s]Evaluating:  19%|█▉        | 131/692 [00:48<03:25,  2.73it/s]Evaluating:  19%|█▉        | 132/692 [00:48<03:24,  2.74it/s]Evaluating:  19%|█▉        | 133/692 [00:49<03:24,  2.74it/s]Evaluating:  19%|█▉        | 134/692 [00:49<03:23,  2.74it/s]Evaluating:  20%|█▉        | 135/692 [00:49<03:22,  2.75it/s]Evaluating:  20%|█▉        | 136/692 [00:50<03:22,  2.75it/s]Evaluating:  20%|█▉        | 137/692 [00:50<03:22,  2.74it/s]Evaluating:  20%|█▉        | 138/692 [00:50<03:22,  2.73it/s]Evaluating:  20%|██        | 139/692 [00:51<03:21,  2.74it/s]Evaluating:  20%|██        | 140/692 [00:51<03:21,  2.74it/s]Evaluating:  20%|██        | 141/692 [00:51<03:21,  2.74it/s]Evaluating:  21%|██        | 142/692 [00:52<03:21,  2.73it/s]Evaluating:  21%|██        | 143/692 [00:52<03:21,  2.73it/s]Evaluating:  21%|██        | 144/692 [00:53<03:21,  2.73it/s]Evaluating:  21%|██        | 145/692 [00:53<03:20,  2.72it/s]Evaluating:  21%|██        | 146/692 [00:53<03:20,  2.72it/s]Evaluating:  21%|██        | 147/692 [00:54<03:19,  2.73it/s]Evaluating:  21%|██▏       | 148/692 [00:54<03:19,  2.73it/s]Evaluating:  22%|██▏       | 149/692 [00:54<03:18,  2.73it/s]Evaluating:  22%|██▏       | 150/692 [00:55<03:18,  2.73it/s]Evaluating:  22%|██▏       | 151/692 [00:55<03:17,  2.75it/s]Evaluating:  22%|██▏       | 152/692 [00:56<03:16,  2.75it/s]Evaluating:  22%|██▏       | 153/692 [00:56<03:16,  2.74it/s]Evaluating:  22%|██▏       | 154/692 [00:56<03:16,  2.74it/s]Evaluating:  22%|██▏       | 155/692 [00:57<03:16,  2.73it/s]Evaluating:  23%|██▎       | 156/692 [00:57<03:15,  2.74it/s]Evaluating:  23%|██▎       | 157/692 [00:57<03:15,  2.74it/s]Evaluating:  23%|██▎       | 158/692 [00:58<03:14,  2.74it/s]Evaluating:  23%|██▎       | 159/692 [00:58<03:13,  2.75it/s]Evaluating:  23%|██▎       | 160/692 [00:58<03:13,  2.74it/s]Evaluating:  23%|██▎       | 161/692 [00:59<03:14,  2.73it/s]Evaluating:  23%|██▎       | 162/692 [00:59<03:14,  2.73it/s]Evaluating:  24%|██▎       | 163/692 [01:00<03:14,  2.73it/s]Evaluating:  24%|██▎       | 164/692 [01:00<03:13,  2.74it/s]Evaluating:  24%|██▍       | 165/692 [01:00<03:12,  2.74it/s]Evaluating:  24%|██▍       | 166/692 [01:01<03:11,  2.74it/s]Evaluating:  24%|██▍       | 167/692 [01:01<03:12,  2.73it/s]Evaluating:  24%|██▍       | 168/692 [01:01<03:11,  2.73it/s]Evaluating:  24%|██▍       | 169/692 [01:02<03:11,  2.73it/s]Evaluating:  25%|██▍       | 170/692 [01:02<03:10,  2.74it/s]Evaluating:  25%|██▍       | 171/692 [01:02<03:09,  2.75it/s]Evaluating:  25%|██▍       | 172/692 [01:03<03:09,  2.74it/s]Evaluating:  25%|██▌       | 173/692 [01:03<03:09,  2.74it/s]Evaluating:  25%|██▌       | 174/692 [01:04<03:09,  2.73it/s]Evaluating:  25%|██▌       | 175/692 [01:04<03:08,  2.74it/s]Evaluating:  25%|██▌       | 176/692 [01:04<03:08,  2.74it/s]Evaluating:  26%|██▌       | 177/692 [01:05<03:07,  2.74it/s]Evaluating:  26%|██▌       | 178/692 [01:05<03:07,  2.74it/s]Evaluating:  26%|██▌       | 179/692 [01:05<03:07,  2.73it/s]Evaluating:  26%|██▌       | 180/692 [01:06<03:07,  2.74it/s]Evaluating:  26%|██▌       | 181/692 [01:06<03:06,  2.74it/s]Evaluating:  26%|██▋       | 182/692 [01:06<03:06,  2.73it/s]Evaluating:  26%|██▋       | 183/692 [01:07<03:06,  2.73it/s]Evaluating:  27%|██▋       | 184/692 [01:07<03:06,  2.73it/s]Evaluating:  27%|██▋       | 185/692 [01:08<03:06,  2.72it/s]Evaluating:  27%|██▋       | 186/692 [01:08<03:05,  2.73it/s]Evaluating:  27%|██▋       | 187/692 [01:08<03:05,  2.72it/s]Evaluating:  27%|██▋       | 188/692 [01:09<03:04,  2.74it/s]Evaluating:  27%|██▋       | 189/692 [01:09<03:04,  2.73it/s]Evaluating:  27%|██▋       | 190/692 [01:09<03:04,  2.72it/s]Evaluating:  28%|██▊       | 191/692 [01:10<03:03,  2.73it/s]Evaluating:  28%|██▊       | 192/692 [01:10<03:02,  2.74it/s]Evaluating:  28%|██▊       | 193/692 [01:10<03:01,  2.75it/s]Evaluating:  28%|██▊       | 194/692 [01:11<03:01,  2.75it/s]Evaluating:  28%|██▊       | 195/692 [01:11<03:00,  2.75it/s]Evaluating:  28%|██▊       | 196/692 [01:12<03:01,  2.73it/s]Evaluating:  28%|██▊       | 197/692 [01:12<03:00,  2.74it/s]Evaluating:  29%|██▊       | 198/692 [01:12<03:00,  2.74it/s]Evaluating:  29%|██▉       | 199/692 [01:13<03:00,  2.74it/s]Evaluating:  29%|██▉       | 200/692 [01:13<03:00,  2.73it/s]Evaluating:  29%|██▉       | 201/692 [01:13<02:59,  2.74it/s]Evaluating:  29%|██▉       | 202/692 [01:14<03:00,  2.72it/s]Evaluating:  29%|██▉       | 203/692 [01:14<02:59,  2.73it/s]Evaluating:  29%|██▉       | 204/692 [01:15<02:58,  2.73it/s]Evaluating:  30%|██▉       | 205/692 [01:15<02:58,  2.72it/s]Evaluating:  30%|██▉       | 206/692 [01:15<02:58,  2.73it/s]Evaluating:  30%|██▉       | 207/692 [01:16<02:57,  2.73it/s]Evaluating:  30%|███       | 208/692 [01:16<02:57,  2.73it/s]Evaluating:  30%|███       | 209/692 [01:16<02:56,  2.73it/s]Evaluating:  30%|███       | 210/692 [01:17<02:55,  2.74it/s]Evaluating:  30%|███       | 211/692 [01:17<02:55,  2.75it/s]Evaluating:  31%|███       | 212/692 [01:17<02:55,  2.74it/s]Evaluating:  31%|███       | 213/692 [01:18<02:55,  2.73it/s]Evaluating:  31%|███       | 214/692 [01:18<02:54,  2.74it/s]Evaluating:  31%|███       | 215/692 [01:19<02:54,  2.74it/s]Evaluating:  31%|███       | 216/692 [01:19<02:54,  2.73it/s]Evaluating:  31%|███▏      | 217/692 [01:19<02:54,  2.72it/s]Evaluating:  32%|███▏      | 218/692 [01:20<02:54,  2.72it/s]Evaluating:  32%|███▏      | 219/692 [01:20<02:53,  2.72it/s]Evaluating:  32%|███▏      | 220/692 [01:20<02:53,  2.72it/s]Evaluating:  32%|███▏      | 221/692 [01:21<02:52,  2.73it/s]Evaluating:  32%|███▏      | 222/692 [01:21<02:52,  2.73it/s]Evaluating:  32%|███▏      | 223/692 [01:21<02:51,  2.73it/s]Evaluating:  32%|███▏      | 224/692 [01:22<02:51,  2.74it/s]Evaluating:  33%|███▎      | 225/692 [01:22<02:50,  2.73it/s]Evaluating:  33%|███▎      | 226/692 [01:23<02:50,  2.74it/s]Evaluating:  33%|███▎      | 227/692 [01:23<02:50,  2.73it/s]Evaluating:  33%|███▎      | 228/692 [01:23<02:49,  2.74it/s]Evaluating:  33%|███▎      | 229/692 [01:24<02:48,  2.75it/s]Evaluating:  33%|███▎      | 230/692 [01:24<02:47,  2.75it/s]Evaluating:  33%|███▎      | 231/692 [01:24<02:47,  2.76it/s]Evaluating:  34%|███▎      | 232/692 [01:25<02:48,  2.74it/s]Evaluating:  34%|███▎      | 233/692 [01:25<02:47,  2.73it/s]Evaluating:  34%|███▍      | 234/692 [01:25<02:47,  2.74it/s]Evaluating:  34%|███▍      | 235/692 [01:26<02:46,  2.74it/s]Evaluating:  34%|███▍      | 236/692 [01:26<02:46,  2.74it/s]Evaluating:  34%|███▍      | 237/692 [01:27<02:46,  2.74it/s]Evaluating:  34%|███▍      | 238/692 [01:27<02:46,  2.72it/s]Evaluating:  35%|███▍      | 239/692 [01:27<02:46,  2.72it/s]Evaluating:  35%|███▍      | 240/692 [01:28<02:45,  2.73it/s]Evaluating:  35%|███▍      | 241/692 [01:28<02:45,  2.73it/s]Evaluating:  35%|███▍      | 242/692 [01:28<02:44,  2.73it/s]Evaluating:  35%|███▌      | 243/692 [01:29<02:44,  2.73it/s]Evaluating:  35%|███▌      | 244/692 [01:29<02:44,  2.73it/s]Evaluating:  35%|███▌      | 245/692 [01:30<02:44,  2.72it/s]Evaluating:  36%|███▌      | 246/692 [01:30<02:43,  2.73it/s]Evaluating:  36%|███▌      | 247/692 [01:30<02:42,  2.73it/s]Evaluating:  36%|███▌      | 248/692 [01:31<02:42,  2.73it/s]Evaluating:  36%|███▌      | 249/692 [01:31<02:42,  2.73it/s]Evaluating:  36%|███▌      | 250/692 [01:31<02:41,  2.74it/s]Evaluating:  36%|███▋      | 251/692 [01:32<02:40,  2.75it/s]Evaluating:  36%|███▋      | 252/692 [01:32<02:41,  2.73it/s]Evaluating:  37%|███▋      | 253/692 [01:32<02:40,  2.74it/s]Evaluating:  37%|███▋      | 254/692 [01:33<02:40,  2.73it/s]Evaluating:  37%|███▋      | 255/692 [01:33<02:41,  2.71it/s]Evaluating:  37%|███▋      | 256/692 [01:34<02:40,  2.71it/s]Evaluating:  37%|███▋      | 257/692 [01:34<02:40,  2.71it/s]Evaluating:  37%|███▋      | 258/692 [01:34<02:40,  2.71it/s]Evaluating:  37%|███▋      | 259/692 [01:35<02:39,  2.71it/s]Evaluating:  38%|███▊      | 260/692 [01:35<02:39,  2.71it/s]Evaluating:  38%|███▊      | 261/692 [01:35<02:38,  2.72it/s]Evaluating:  38%|███▊      | 262/692 [01:36<02:38,  2.72it/s]Evaluating:  38%|███▊      | 263/692 [01:36<02:38,  2.71it/s]Evaluating:  38%|███▊      | 264/692 [01:37<02:37,  2.71it/s]Evaluating:  38%|███▊      | 265/692 [01:37<02:37,  2.72it/s]Evaluating:  38%|███▊      | 266/692 [01:37<02:37,  2.71it/s]Evaluating:  39%|███▊      | 267/692 [01:38<02:35,  2.73it/s]Evaluating:  39%|███▊      | 268/692 [01:38<02:34,  2.74it/s]Evaluating:  39%|███▉      | 269/692 [01:38<02:34,  2.74it/s]Evaluating:  39%|███▉      | 270/692 [01:39<02:33,  2.74it/s]Evaluating:  39%|███▉      | 271/692 [01:39<02:33,  2.74it/s]Evaluating:  39%|███▉      | 272/692 [01:39<02:33,  2.74it/s]Evaluating:  39%|███▉      | 273/692 [01:40<02:33,  2.73it/s]Evaluating:  40%|███▉      | 274/692 [01:40<02:33,  2.73it/s]Evaluating:  40%|███▉      | 275/692 [01:41<02:32,  2.73it/s]Evaluating:  40%|███▉      | 276/692 [01:41<02:32,  2.73it/s]Evaluating:  40%|████      | 277/692 [01:41<02:32,  2.72it/s]Evaluating:  40%|████      | 278/692 [01:42<02:32,  2.72it/s]Evaluating:  40%|████      | 279/692 [01:42<02:31,  2.72it/s]Evaluating:  40%|████      | 280/692 [01:42<02:31,  2.71it/s]Evaluating:  41%|████      | 281/692 [01:43<02:31,  2.72it/s]Evaluating:  41%|████      | 282/692 [01:43<02:30,  2.73it/s]Evaluating:  41%|████      | 283/692 [01:43<02:29,  2.73it/s]Evaluating:  41%|████      | 284/692 [01:44<02:29,  2.72it/s]Evaluating:  41%|████      | 285/692 [01:44<02:29,  2.72it/s]Evaluating:  41%|████▏     | 286/692 [01:45<02:28,  2.73it/s]Evaluating:  41%|████▏     | 287/692 [01:45<02:27,  2.74it/s]Evaluating:  42%|████▏     | 288/692 [01:45<02:27,  2.74it/s]Evaluating:  42%|████▏     | 289/692 [01:46<02:27,  2.74it/s]Evaluating:  42%|████▏     | 290/692 [01:46<02:26,  2.74it/s]Evaluating:  42%|████▏     | 291/692 [01:46<02:26,  2.73it/s]Evaluating:  42%|████▏     | 292/692 [01:47<02:26,  2.74it/s]Evaluating:  42%|████▏     | 293/692 [01:47<02:25,  2.74it/s]Evaluating:  42%|████▏     | 294/692 [01:47<02:25,  2.74it/s]Evaluating:  43%|████▎     | 295/692 [01:48<02:24,  2.75it/s]Evaluating:  43%|████▎     | 296/692 [01:48<02:24,  2.73it/s]Evaluating:  43%|████▎     | 297/692 [01:49<02:24,  2.73it/s]Evaluating:  43%|████▎     | 298/692 [01:49<02:24,  2.72it/s]Evaluating:  43%|████▎     | 299/692 [01:49<02:24,  2.72it/s]Evaluating:  43%|████▎     | 300/692 [01:50<02:23,  2.73it/s]Evaluating:  43%|████▎     | 301/692 [01:50<02:23,  2.73it/s]Evaluating:  44%|████▎     | 302/692 [01:50<02:23,  2.72it/s]Evaluating:  44%|████▍     | 303/692 [01:51<02:22,  2.73it/s]Evaluating:  44%|████▍     | 304/692 [01:51<02:21,  2.73it/s]Evaluating:  44%|████▍     | 305/692 [01:52<02:21,  2.74it/s]Evaluating:  44%|████▍     | 306/692 [01:52<02:21,  2.73it/s]Evaluating:  44%|████▍     | 307/692 [01:52<02:21,  2.73it/s]Evaluating:  45%|████▍     | 308/692 [01:53<02:20,  2.74it/s]Evaluating:  45%|████▍     | 309/692 [01:53<02:24,  2.65it/s]Evaluating:  45%|████▍     | 310/692 [01:53<02:22,  2.68it/s]Evaluating:  45%|████▍     | 311/692 [01:54<02:21,  2.70it/s]Evaluating:  45%|████▌     | 312/692 [01:54<02:20,  2.71it/s]Evaluating:  45%|████▌     | 313/692 [01:54<02:19,  2.71it/s]Evaluating:  45%|████▌     | 314/692 [01:55<02:19,  2.71it/s]Evaluating:  46%|████▌     | 315/692 [01:55<02:18,  2.72it/s]Evaluating:  46%|████▌     | 316/692 [01:56<02:18,  2.72it/s]Evaluating:  46%|████▌     | 317/692 [01:56<02:17,  2.72it/s]Evaluating:  46%|████▌     | 318/692 [01:56<02:17,  2.73it/s]Evaluating:  46%|████▌     | 319/692 [01:57<02:16,  2.73it/s]Evaluating:  46%|████▌     | 320/692 [01:57<02:16,  2.73it/s]Evaluating:  46%|████▋     | 321/692 [01:57<02:15,  2.73it/s]Evaluating:  47%|████▋     | 322/692 [01:58<02:15,  2.73it/s]Evaluating:  47%|████▋     | 323/692 [01:58<02:15,  2.73it/s]Evaluating:  47%|████▋     | 324/692 [01:59<02:15,  2.72it/s]Evaluating:  47%|████▋     | 325/692 [01:59<02:14,  2.73it/s]Evaluating:  47%|████▋     | 326/692 [01:59<02:14,  2.73it/s]Evaluating:  47%|████▋     | 327/692 [02:00<02:13,  2.73it/s]Evaluating:  47%|████▋     | 328/692 [02:00<02:13,  2.73it/s]Evaluating:  48%|████▊     | 329/692 [02:00<02:12,  2.73it/s]Evaluating:  48%|████▊     | 330/692 [02:01<02:12,  2.73it/s]Evaluating:  48%|████▊     | 331/692 [02:01<02:12,  2.73it/s]Evaluating:  48%|████▊     | 332/692 [02:01<02:12,  2.71it/s]Evaluating:  48%|████▊     | 333/692 [02:02<02:12,  2.72it/s]Evaluating:  48%|████▊     | 334/692 [02:02<02:11,  2.72it/s]Evaluating:  48%|████▊     | 335/692 [02:03<02:11,  2.72it/s]Evaluating:  49%|████▊     | 336/692 [02:03<02:10,  2.72it/s]Evaluating:  49%|████▊     | 337/692 [02:03<02:10,  2.73it/s]Evaluating:  49%|████▉     | 338/692 [02:04<02:09,  2.73it/s]Evaluating:  49%|████▉     | 339/692 [02:04<02:09,  2.73it/s]Evaluating:  49%|████▉     | 340/692 [02:04<02:08,  2.74it/s]Evaluating:  49%|████▉     | 341/692 [02:05<02:08,  2.74it/s]Evaluating:  49%|████▉     | 342/692 [02:05<02:07,  2.74it/s]Evaluating:  50%|████▉     | 343/692 [02:05<02:07,  2.73it/s]Evaluating:  50%|████▉     | 344/692 [02:06<02:07,  2.73it/s]Evaluating:  50%|████▉     | 345/692 [02:06<02:07,  2.72it/s]Evaluating:  50%|█████     | 346/692 [02:07<02:07,  2.71it/s]Evaluating:  50%|█████     | 347/692 [02:07<02:06,  2.72it/s]Evaluating:  50%|█████     | 348/692 [02:07<02:06,  2.73it/s]Evaluating:  50%|█████     | 349/692 [02:08<02:06,  2.72it/s]Evaluating:  51%|█████     | 350/692 [02:08<02:05,  2.72it/s]Evaluating:  51%|█████     | 351/692 [02:08<02:05,  2.72it/s]Evaluating:  51%|█████     | 352/692 [02:09<02:04,  2.72it/s]Evaluating:  51%|█████     | 353/692 [02:09<02:04,  2.72it/s]Evaluating:  51%|█████     | 354/692 [02:10<02:04,  2.71it/s]Evaluating:  51%|█████▏    | 355/692 [02:10<02:04,  2.72it/s]Evaluating:  51%|█████▏    | 356/692 [02:10<02:04,  2.71it/s]Evaluating:  52%|█████▏    | 357/692 [02:11<02:03,  2.71it/s]Evaluating:  52%|█████▏    | 358/692 [02:11<02:03,  2.71it/s]Evaluating:  52%|█████▏    | 359/692 [02:11<02:02,  2.71it/s]Evaluating:  52%|█████▏    | 360/692 [02:12<02:02,  2.71it/s]Evaluating:  52%|█████▏    | 361/692 [02:12<02:01,  2.72it/s]Evaluating:  52%|█████▏    | 362/692 [02:12<02:02,  2.70it/s]Evaluating:  52%|█████▏    | 363/692 [02:13<02:01,  2.71it/s]Evaluating:  53%|█████▎    | 364/692 [02:13<02:00,  2.71it/s]Evaluating:  53%|█████▎    | 365/692 [02:14<02:00,  2.72it/s]Evaluating:  53%|█████▎    | 366/692 [02:14<01:59,  2.73it/s]Evaluating:  53%|█████▎    | 367/692 [02:14<01:59,  2.72it/s]Evaluating:  53%|█████▎    | 368/692 [02:15<01:59,  2.72it/s]Evaluating:  53%|█████▎    | 369/692 [02:15<01:58,  2.73it/s]Evaluating:  53%|█████▎    | 370/692 [02:15<01:58,  2.73it/s]Evaluating:  54%|█████▎    | 371/692 [02:16<01:57,  2.73it/s]Evaluating:  54%|█████▍    | 372/692 [02:16<01:57,  2.73it/s]Evaluating:  54%|█████▍    | 373/692 [02:17<01:56,  2.74it/s]Evaluating:  54%|█████▍    | 374/692 [02:17<01:56,  2.74it/s]Evaluating:  54%|█████▍    | 375/692 [02:17<01:56,  2.73it/s]Evaluating:  54%|█████▍    | 376/692 [02:18<01:55,  2.73it/s]Evaluating:  54%|█████▍    | 377/692 [02:18<01:55,  2.73it/s]Evaluating:  55%|█████▍    | 378/692 [02:18<01:55,  2.73it/s]Evaluating:  55%|█████▍    | 379/692 [02:19<01:54,  2.74it/s]Evaluating:  55%|█████▍    | 380/692 [02:19<01:54,  2.73it/s]Evaluating:  55%|█████▌    | 381/692 [02:19<01:53,  2.73it/s]Evaluating:  55%|█████▌    | 382/692 [02:20<01:54,  2.72it/s]Evaluating:  55%|█████▌    | 383/692 [02:20<01:53,  2.72it/s]Evaluating:  55%|█████▌    | 384/692 [02:21<01:53,  2.72it/s]Evaluating:  56%|█████▌    | 385/692 [02:21<01:53,  2.71it/s]Evaluating:  56%|█████▌    | 386/692 [02:21<01:52,  2.71it/s]Evaluating:  56%|█████▌    | 387/692 [02:22<01:52,  2.72it/s]Evaluating:  56%|█████▌    | 388/692 [02:22<01:51,  2.72it/s]Evaluating:  56%|█████▌    | 389/692 [02:22<01:51,  2.72it/s]Evaluating:  56%|█████▋    | 390/692 [02:23<01:50,  2.73it/s]Evaluating:  57%|█████▋    | 391/692 [02:23<01:50,  2.72it/s]Evaluating:  57%|█████▋    | 392/692 [02:23<01:50,  2.72it/s]Evaluating:  57%|█████▋    | 393/692 [02:24<01:50,  2.71it/s]Evaluating:  57%|█████▋    | 394/692 [02:24<01:49,  2.72it/s]Evaluating:  57%|█████▋    | 395/692 [02:25<01:49,  2.72it/s]Evaluating:  57%|█████▋    | 396/692 [02:25<01:48,  2.73it/s]Evaluating:  57%|█████▋    | 397/692 [02:25<01:48,  2.72it/s]Evaluating:  58%|█████▊    | 398/692 [02:26<01:47,  2.72it/s]Evaluating:  58%|█████▊    | 399/692 [02:26<01:48,  2.71it/s]Evaluating:  58%|█████▊    | 400/692 [02:26<01:47,  2.71it/s]Evaluating:  58%|█████▊    | 401/692 [02:27<01:46,  2.72it/s]Evaluating:  58%|█████▊    | 402/692 [02:27<01:46,  2.73it/s]Evaluating:  58%|█████▊    | 403/692 [02:28<01:46,  2.72it/s]Evaluating:  58%|█████▊    | 404/692 [02:28<01:46,  2.72it/s]Evaluating:  59%|█████▊    | 405/692 [02:28<01:45,  2.72it/s]Evaluating:  59%|█████▊    | 406/692 [02:29<01:45,  2.72it/s]Evaluating:  59%|█████▉    | 407/692 [02:29<01:44,  2.72it/s]Evaluating:  59%|█████▉    | 408/692 [02:29<01:44,  2.72it/s]Evaluating:  59%|█████▉    | 409/692 [02:30<01:44,  2.72it/s]Evaluating:  59%|█████▉    | 410/692 [02:30<01:43,  2.72it/s]Evaluating:  59%|█████▉    | 411/692 [02:30<01:43,  2.72it/s]Evaluating:  60%|█████▉    | 412/692 [02:31<01:42,  2.73it/s]Evaluating:  60%|█████▉    | 413/692 [02:31<01:42,  2.72it/s]Evaluating:  60%|█████▉    | 414/692 [02:32<01:42,  2.72it/s]Evaluating:  60%|█████▉    | 415/692 [02:32<01:41,  2.73it/s]Evaluating:  60%|██████    | 416/692 [02:32<01:41,  2.73it/s]Evaluating:  60%|██████    | 417/692 [02:33<01:41,  2.72it/s]Evaluating:  60%|██████    | 418/692 [02:33<01:40,  2.73it/s]Evaluating:  61%|██████    | 419/692 [02:33<01:40,  2.73it/s]Evaluating:  61%|██████    | 420/692 [02:34<01:39,  2.73it/s]Evaluating:  61%|██████    | 421/692 [02:34<01:39,  2.73it/s]Evaluating:  61%|██████    | 422/692 [02:35<01:39,  2.72it/s]Evaluating:  61%|██████    | 423/692 [02:35<01:38,  2.72it/s]Evaluating:  61%|██████▏   | 424/692 [02:35<01:38,  2.72it/s]Evaluating:  61%|██████▏   | 425/692 [02:36<01:38,  2.72it/s]Evaluating:  62%|██████▏   | 426/692 [02:36<01:37,  2.73it/s]Evaluating:  62%|██████▏   | 427/692 [02:36<01:37,  2.73it/s]Evaluating:  62%|██████▏   | 428/692 [02:37<01:36,  2.73it/s]Evaluating:  62%|██████▏   | 429/692 [02:37<01:36,  2.73it/s]Evaluating:  62%|██████▏   | 430/692 [02:37<01:35,  2.73it/s]Evaluating:  62%|██████▏   | 431/692 [02:38<01:35,  2.73it/s]Evaluating:  62%|██████▏   | 432/692 [02:38<01:34,  2.74it/s]Evaluating:  63%|██████▎   | 433/692 [02:39<01:34,  2.73it/s]Evaluating:  63%|██████▎   | 434/692 [02:39<01:34,  2.73it/s]Evaluating:  63%|██████▎   | 435/692 [02:39<01:34,  2.71it/s]Evaluating:  63%|██████▎   | 436/692 [02:40<01:34,  2.72it/s]Evaluating:  63%|██████▎   | 437/692 [02:40<01:34,  2.71it/s]Evaluating:  63%|██████▎   | 438/692 [02:40<01:33,  2.72it/s]Evaluating:  63%|██████▎   | 439/692 [02:41<01:33,  2.72it/s]Evaluating:  64%|██████▎   | 440/692 [02:41<01:32,  2.72it/s]Evaluating:  64%|██████▎   | 441/692 [02:41<01:32,  2.73it/s]Evaluating:  64%|██████▍   | 442/692 [02:42<01:31,  2.73it/s]Evaluating:  64%|██████▍   | 443/692 [02:42<01:31,  2.73it/s]Evaluating:  64%|██████▍   | 444/692 [02:43<01:31,  2.72it/s]Evaluating:  64%|██████▍   | 445/692 [02:43<01:30,  2.73it/s]Evaluating:  64%|██████▍   | 446/692 [02:43<01:30,  2.73it/s]Evaluating:  65%|██████▍   | 447/692 [02:44<01:29,  2.73it/s]Evaluating:  65%|██████▍   | 448/692 [02:44<01:29,  2.73it/s]Evaluating:  65%|██████▍   | 449/692 [02:44<01:29,  2.73it/s]Evaluating:  65%|██████▌   | 450/692 [02:45<01:29,  2.71it/s]Evaluating:  65%|██████▌   | 451/692 [02:45<01:28,  2.72it/s]Evaluating:  65%|██████▌   | 452/692 [02:46<01:28,  2.71it/s]Evaluating:  65%|██████▌   | 453/692 [02:46<01:28,  2.71it/s]Evaluating:  66%|██████▌   | 454/692 [02:46<01:27,  2.71it/s]Evaluating:  66%|██████▌   | 455/692 [02:47<01:27,  2.72it/s]Evaluating:  66%|██████▌   | 456/692 [02:47<01:26,  2.72it/s]Evaluating:  66%|██████▌   | 457/692 [02:47<01:26,  2.72it/s]Evaluating:  66%|██████▌   | 458/692 [02:48<01:25,  2.72it/s]Evaluating:  66%|██████▋   | 459/692 [02:48<01:25,  2.72it/s]Evaluating:  66%|██████▋   | 460/692 [02:48<01:25,  2.71it/s]Evaluating:  67%|██████▋   | 461/692 [02:49<01:25,  2.71it/s]Evaluating:  67%|██████▋   | 462/692 [02:49<01:24,  2.71it/s]Evaluating:  67%|██████▋   | 463/692 [02:50<01:24,  2.71it/s]Evaluating:  67%|██████▋   | 464/692 [02:50<01:24,  2.71it/s]Evaluating:  67%|██████▋   | 465/692 [02:50<01:23,  2.70it/s]Evaluating:  67%|██████▋   | 466/692 [02:51<01:23,  2.71it/s]Evaluating:  67%|██████▋   | 467/692 [02:51<01:22,  2.72it/s]Evaluating:  68%|██████▊   | 468/692 [02:51<01:22,  2.71it/s]Evaluating:  68%|██████▊   | 469/692 [02:52<01:22,  2.71it/s]Evaluating:  68%|██████▊   | 470/692 [02:52<01:21,  2.71it/s]Evaluating:  68%|██████▊   | 471/692 [02:53<01:21,  2.71it/s]Evaluating:  68%|██████▊   | 472/692 [02:53<01:21,  2.71it/s]Evaluating:  68%|██████▊   | 473/692 [02:53<01:20,  2.71it/s]Evaluating:  68%|██████▊   | 474/692 [02:54<01:20,  2.72it/s]Evaluating:  69%|██████▊   | 475/692 [02:54<01:19,  2.72it/s]Evaluating:  69%|██████▉   | 476/692 [02:54<01:19,  2.72it/s]Evaluating:  69%|██████▉   | 477/692 [02:55<01:19,  2.71it/s]Evaluating:  69%|██████▉   | 478/692 [02:55<01:19,  2.70it/s]Evaluating:  69%|██████▉   | 479/692 [02:55<01:18,  2.71it/s]Evaluating:  69%|██████▉   | 480/692 [02:56<01:17,  2.72it/s]Evaluating:  70%|██████▉   | 481/692 [02:56<01:17,  2.71it/s]Evaluating:  70%|██████▉   | 482/692 [02:57<01:17,  2.72it/s]Evaluating:  70%|██████▉   | 483/692 [02:57<01:16,  2.72it/s]Evaluating:  70%|██████▉   | 484/692 [02:57<01:16,  2.72it/s]Evaluating:  70%|███████   | 485/692 [02:58<01:16,  2.72it/s]Evaluating:  70%|███████   | 486/692 [02:58<01:15,  2.72it/s]Evaluating:  70%|███████   | 487/692 [02:58<01:15,  2.72it/s]Evaluating:  71%|███████   | 488/692 [02:59<01:14,  2.73it/s]Evaluating:  71%|███████   | 489/692 [02:59<01:14,  2.73it/s]Evaluating:  71%|███████   | 490/692 [03:00<01:13,  2.73it/s]Evaluating:  71%|███████   | 491/692 [03:00<01:13,  2.73it/s]Evaluating:  71%|███████   | 492/692 [03:00<01:13,  2.74it/s]Evaluating:  71%|███████   | 493/692 [03:01<01:13,  2.72it/s]Evaluating:  71%|███████▏  | 494/692 [03:01<01:12,  2.73it/s]Evaluating:  72%|███████▏  | 495/692 [03:01<01:12,  2.73it/s]Evaluating:  72%|███████▏  | 496/692 [03:02<01:11,  2.73it/s]Evaluating:  72%|███████▏  | 497/692 [03:02<01:11,  2.73it/s]Evaluating:  72%|███████▏  | 498/692 [03:02<01:10,  2.73it/s]Evaluating:  72%|███████▏  | 499/692 [03:03<01:10,  2.73it/s]Evaluating:  72%|███████▏  | 500/692 [03:03<01:10,  2.73it/s]Evaluating:  72%|███████▏  | 501/692 [03:04<01:09,  2.73it/s]Evaluating:  73%|███████▎  | 502/692 [03:04<01:09,  2.74it/s]Evaluating:  73%|███████▎  | 503/692 [03:04<01:09,  2.73it/s]Evaluating:  73%|███████▎  | 504/692 [03:05<01:09,  2.71it/s]Evaluating:  73%|███████▎  | 505/692 [03:05<01:09,  2.71it/s]Evaluating:  73%|███████▎  | 506/692 [03:05<01:08,  2.70it/s]Evaluating:  73%|███████▎  | 507/692 [03:06<01:08,  2.70it/s]Evaluating:  73%|███████▎  | 508/692 [03:06<01:08,  2.70it/s]Evaluating:  74%|███████▎  | 509/692 [03:06<01:07,  2.71it/s]Evaluating:  74%|███████▎  | 510/692 [03:07<01:07,  2.71it/s]Evaluating:  74%|███████▍  | 511/692 [03:07<01:06,  2.72it/s]Evaluating:  74%|███████▍  | 512/692 [03:08<01:05,  2.73it/s]Evaluating:  74%|███████▍  | 513/692 [03:08<01:05,  2.73it/s]Evaluating:  74%|███████▍  | 514/692 [03:08<01:05,  2.73it/s]Evaluating:  74%|███████▍  | 515/692 [03:09<01:04,  2.73it/s]Evaluating:  75%|███████▍  | 516/692 [03:09<01:04,  2.73it/s]Evaluating:  75%|███████▍  | 517/692 [03:09<01:04,  2.72it/s]Evaluating:  75%|███████▍  | 518/692 [03:10<01:04,  2.71it/s]Evaluating:  75%|███████▌  | 519/692 [03:10<01:03,  2.72it/s]Evaluating:  75%|███████▌  | 520/692 [03:11<01:03,  2.72it/s]Evaluating:  75%|███████▌  | 521/692 [03:11<01:03,  2.71it/s]Evaluating:  75%|███████▌  | 522/692 [03:11<01:02,  2.72it/s]Evaluating:  76%|███████▌  | 523/692 [03:12<01:01,  2.73it/s]Evaluating:  76%|███████▌  | 524/692 [03:12<01:01,  2.73it/s]Evaluating:  76%|███████▌  | 525/692 [03:12<01:02,  2.67it/s]Evaluating:  76%|███████▌  | 526/692 [03:13<01:01,  2.69it/s]Evaluating:  76%|███████▌  | 527/692 [03:13<01:01,  2.70it/s]Evaluating:  76%|███████▋  | 528/692 [03:13<01:00,  2.71it/s]Evaluating:  76%|███████▋  | 529/692 [03:14<01:00,  2.71it/s]Evaluating:  77%|███████▋  | 530/692 [03:14<00:59,  2.73it/s]Evaluating:  77%|███████▋  | 531/692 [03:15<00:58,  2.73it/s]Evaluating:  77%|███████▋  | 532/692 [03:15<00:58,  2.73it/s]Evaluating:  77%|███████▋  | 533/692 [03:15<00:58,  2.74it/s]Evaluating:  77%|███████▋  | 534/692 [03:16<00:57,  2.74it/s]Evaluating:  77%|███████▋  | 535/692 [03:16<00:57,  2.74it/s]Evaluating:  77%|███████▋  | 536/692 [03:16<00:56,  2.74it/s]Evaluating:  78%|███████▊  | 537/692 [03:17<00:56,  2.74it/s]Evaluating:  78%|███████▊  | 538/692 [03:17<00:56,  2.74it/s]Evaluating:  78%|███████▊  | 539/692 [03:18<00:55,  2.74it/s]Evaluating:  78%|███████▊  | 540/692 [03:18<00:55,  2.75it/s]Evaluating:  78%|███████▊  | 541/692 [03:18<00:55,  2.74it/s]Evaluating:  78%|███████▊  | 542/692 [03:19<00:54,  2.74it/s]Evaluating:  78%|███████▊  | 543/692 [03:19<00:54,  2.73it/s]Evaluating:  79%|███████▊  | 544/692 [03:19<00:54,  2.73it/s]Evaluating:  79%|███████▉  | 545/692 [03:20<00:53,  2.73it/s]Evaluating:  79%|███████▉  | 546/692 [03:20<00:53,  2.73it/s]Evaluating:  79%|███████▉  | 547/692 [03:20<00:53,  2.73it/s]Evaluating:  79%|███████▉  | 548/692 [03:21<00:52,  2.74it/s]Evaluating:  79%|███████▉  | 549/692 [03:21<00:52,  2.73it/s]Evaluating:  79%|███████▉  | 550/692 [03:22<00:52,  2.73it/s]Evaluating:  80%|███████▉  | 551/692 [03:22<00:51,  2.72it/s]Evaluating:  80%|███████▉  | 552/692 [03:22<00:51,  2.72it/s]Evaluating:  80%|███████▉  | 553/692 [03:23<00:51,  2.72it/s]Evaluating:  80%|████████  | 554/692 [03:23<00:50,  2.72it/s]Evaluating:  80%|████████  | 555/692 [03:23<00:50,  2.72it/s]Evaluating:  80%|████████  | 556/692 [03:24<00:50,  2.72it/s]Evaluating:  80%|████████  | 557/692 [03:24<00:49,  2.71it/s]Evaluating:  81%|████████  | 558/692 [03:24<00:49,  2.71it/s]Evaluating:  81%|████████  | 559/692 [03:25<00:49,  2.71it/s]Evaluating:  81%|████████  | 560/692 [03:25<00:48,  2.72it/s]Evaluating:  81%|████████  | 561/692 [03:26<00:48,  2.72it/s]Evaluating:  81%|████████  | 562/692 [03:26<00:47,  2.73it/s]Evaluating:  81%|████████▏ | 563/692 [03:26<00:47,  2.74it/s]Evaluating:  82%|████████▏ | 564/692 [03:27<00:46,  2.75it/s]Evaluating:  82%|████████▏ | 565/692 [03:27<00:46,  2.75it/s]Evaluating:  82%|████████▏ | 566/692 [03:27<00:46,  2.73it/s]Evaluating:  82%|████████▏ | 567/692 [03:28<00:45,  2.74it/s]Evaluating:  82%|████████▏ | 568/692 [03:28<00:45,  2.73it/s]Evaluating:  82%|████████▏ | 569/692 [03:29<00:45,  2.73it/s]Evaluating:  82%|████████▏ | 570/692 [03:29<00:44,  2.73it/s]Evaluating:  83%|████████▎ | 571/692 [03:29<00:44,  2.74it/s]Evaluating:  83%|████████▎ | 572/692 [03:30<00:43,  2.74it/s]Evaluating:  83%|████████▎ | 573/692 [03:30<00:43,  2.74it/s]Evaluating:  83%|████████▎ | 574/692 [03:30<00:43,  2.74it/s]Evaluating:  83%|████████▎ | 575/692 [03:31<00:42,  2.75it/s]Evaluating:  83%|████████▎ | 576/692 [03:31<00:42,  2.74it/s]Evaluating:  83%|████████▎ | 577/692 [03:31<00:42,  2.74it/s]Evaluating:  84%|████████▎ | 578/692 [03:32<00:41,  2.73it/s]Evaluating:  84%|████████▎ | 579/692 [03:32<00:41,  2.73it/s]Evaluating:  84%|████████▍ | 580/692 [03:33<00:41,  2.73it/s]Evaluating:  84%|████████▍ | 581/692 [03:33<00:40,  2.72it/s]Evaluating:  84%|████████▍ | 582/692 [03:33<00:40,  2.74it/s]Evaluating:  84%|████████▍ | 583/692 [03:34<00:39,  2.74it/s]Evaluating:  84%|████████▍ | 584/692 [03:34<00:39,  2.74it/s]Evaluating:  85%|████████▍ | 585/692 [03:34<00:39,  2.74it/s]Evaluating:  85%|████████▍ | 586/692 [03:35<00:38,  2.74it/s]Evaluating:  85%|████████▍ | 587/692 [03:35<00:38,  2.75it/s]Evaluating:  85%|████████▍ | 588/692 [03:35<00:37,  2.74it/s]Evaluating:  85%|████████▌ | 589/692 [03:36<00:37,  2.75it/s]Evaluating:  85%|████████▌ | 590/692 [03:36<00:37,  2.75it/s]Evaluating:  85%|████████▌ | 591/692 [03:37<00:36,  2.74it/s]Evaluating:  86%|████████▌ | 592/692 [03:37<00:36,  2.73it/s]Evaluating:  86%|████████▌ | 593/692 [03:37<00:36,  2.73it/s]Evaluating:  86%|████████▌ | 594/692 [03:38<00:35,  2.73it/s]Evaluating:  86%|████████▌ | 595/692 [03:38<00:35,  2.73it/s]Evaluating:  86%|████████▌ | 596/692 [03:38<00:35,  2.73it/s]Evaluating:  86%|████████▋ | 597/692 [03:39<00:34,  2.72it/s]Evaluating:  86%|████████▋ | 598/692 [03:39<00:34,  2.71it/s]Evaluating:  87%|████████▋ | 599/692 [03:39<00:34,  2.72it/s]Evaluating:  87%|████████▋ | 600/692 [03:40<00:33,  2.73it/s]Evaluating:  87%|████████▋ | 601/692 [03:40<00:33,  2.73it/s]Evaluating:  87%|████████▋ | 602/692 [03:41<00:32,  2.73it/s]Evaluating:  87%|████████▋ | 603/692 [03:41<00:32,  2.73it/s]Evaluating:  87%|████████▋ | 604/692 [03:41<00:32,  2.73it/s]Evaluating:  87%|████████▋ | 605/692 [03:42<00:31,  2.72it/s]Evaluating:  88%|████████▊ | 606/692 [03:42<00:31,  2.71it/s]Evaluating:  88%|████████▊ | 607/692 [03:42<00:31,  2.71it/s]Evaluating:  88%|████████▊ | 608/692 [03:43<00:31,  2.71it/s]Evaluating:  88%|████████▊ | 609/692 [03:43<00:30,  2.71it/s]Evaluating:  88%|████████▊ | 610/692 [03:44<00:30,  2.71it/s]Evaluating:  88%|████████▊ | 611/692 [03:44<00:29,  2.71it/s]Evaluating:  88%|████████▊ | 612/692 [03:44<00:29,  2.72it/s]Evaluating:  89%|████████▊ | 613/692 [03:45<00:28,  2.72it/s]Evaluating:  89%|████████▊ | 614/692 [03:45<00:28,  2.72it/s]Evaluating:  89%|████████▉ | 615/692 [03:45<00:28,  2.71it/s]Evaluating:  89%|████████▉ | 616/692 [03:46<00:28,  2.71it/s]Evaluating:  89%|████████▉ | 617/692 [03:46<00:27,  2.72it/s]Evaluating:  89%|████████▉ | 618/692 [03:46<00:27,  2.72it/s]Evaluating:  89%|████████▉ | 619/692 [03:47<00:26,  2.72it/s]Evaluating:  90%|████████▉ | 620/692 [03:47<00:26,  2.73it/s]Evaluating:  90%|████████▉ | 621/692 [03:48<00:26,  2.72it/s]Evaluating:  90%|████████▉ | 622/692 [03:48<00:25,  2.72it/s]Evaluating:  90%|█████████ | 623/692 [03:48<00:25,  2.72it/s]Evaluating:  90%|█████████ | 624/692 [03:49<00:25,  2.72it/s]Evaluating:  90%|█████████ | 625/692 [03:49<00:24,  2.72it/s]Evaluating:  90%|█████████ | 626/692 [03:49<00:24,  2.72it/s]Evaluating:  91%|█████████ | 627/692 [03:50<00:23,  2.73it/s]Evaluating:  91%|█████████ | 628/692 [03:50<00:23,  2.72it/s]Evaluating:  91%|█████████ | 629/692 [03:51<00:23,  2.72it/s]Evaluating:  91%|█████████ | 630/692 [03:51<00:22,  2.72it/s]Evaluating:  91%|█████████ | 631/692 [03:51<00:22,  2.72it/s]Evaluating:  91%|█████████▏| 632/692 [03:52<00:22,  2.73it/s]Evaluating:  91%|█████████▏| 633/692 [03:52<00:21,  2.73it/s]Evaluating:  92%|█████████▏| 634/692 [03:52<00:21,  2.73it/s]Evaluating:  92%|█████████▏| 635/692 [03:53<00:20,  2.73it/s]Evaluating:  92%|█████████▏| 636/692 [03:53<00:20,  2.73it/s]Evaluating:  92%|█████████▏| 637/692 [03:53<00:20,  2.72it/s]Evaluating:  92%|█████████▏| 638/692 [03:54<00:19,  2.73it/s]Evaluating:  92%|█████████▏| 639/692 [03:54<00:19,  2.73it/s]Evaluating:  92%|█████████▏| 640/692 [03:55<00:18,  2.74it/s]Evaluating:  93%|█████████▎| 641/692 [03:55<00:18,  2.75it/s]Evaluating:  93%|█████████▎| 642/692 [03:55<00:18,  2.75it/s]Evaluating:  93%|█████████▎| 643/692 [03:56<00:17,  2.74it/s]Evaluating:  93%|█████████▎| 644/692 [03:56<00:17,  2.74it/s]Evaluating:  93%|█████████▎| 645/692 [03:56<00:17,  2.73it/s]Evaluating:  93%|█████████▎| 646/692 [03:57<00:16,  2.73it/s]Evaluating:  93%|█████████▎| 647/692 [03:57<00:16,  2.73it/s]Evaluating:  94%|█████████▎| 648/692 [03:57<00:16,  2.74it/s]Evaluating:  94%|█████████▍| 649/692 [03:58<00:15,  2.75it/s]Evaluating:  94%|█████████▍| 650/692 [03:58<00:15,  2.74it/s]Evaluating:  94%|█████████▍| 651/692 [03:59<00:14,  2.75it/s]Evaluating:  94%|█████████▍| 652/692 [03:59<00:14,  2.75it/s]Evaluating:  94%|█████████▍| 653/692 [03:59<00:14,  2.75it/s]Evaluating:  95%|█████████▍| 654/692 [04:00<00:13,  2.74it/s]Evaluating:  95%|█████████▍| 655/692 [04:00<00:13,  2.74it/s]Evaluating:  95%|█████████▍| 656/692 [04:00<00:13,  2.73it/s]Evaluating:  95%|█████████▍| 657/692 [04:01<00:12,  2.74it/s]Evaluating:  95%|█████████▌| 658/692 [04:01<00:12,  2.75it/s]Evaluating:  95%|█████████▌| 659/692 [04:01<00:11,  2.75it/s]Evaluating:  95%|█████████▌| 660/692 [04:02<00:11,  2.75it/s]Evaluating:  96%|█████████▌| 661/692 [04:02<00:11,  2.74it/s]Evaluating:  96%|█████████▌| 662/692 [04:03<00:10,  2.74it/s]Evaluating:  96%|█████████▌| 663/692 [04:03<00:10,  2.74it/s]Evaluating:  96%|█████████▌| 664/692 [04:03<00:10,  2.73it/s]Evaluating:  96%|█████████▌| 665/692 [04:04<00:09,  2.73it/s]Evaluating:  96%|█████████▌| 666/692 [04:04<00:09,  2.72it/s]Evaluating:  96%|█████████▋| 667/692 [04:04<00:09,  2.72it/s]Evaluating:  97%|█████████▋| 668/692 [04:05<00:08,  2.73it/s]Evaluating:  97%|█████████▋| 669/692 [04:05<00:08,  2.73it/s]Evaluating:  97%|█████████▋| 670/692 [04:05<00:08,  2.74it/s]Evaluating:  97%|█████████▋| 671/692 [04:06<00:07,  2.74it/s]Evaluating:  97%|█████████▋| 672/692 [04:06<00:07,  2.73it/s]Evaluating:  97%|█████████▋| 673/692 [04:07<00:06,  2.74it/s]Evaluating:  97%|█████████▋| 674/692 [04:07<00:06,  2.74it/s]Evaluating:  98%|█████████▊| 675/692 [04:07<00:06,  2.73it/s]Evaluating:  98%|█████████▊| 676/692 [04:08<00:05,  2.73it/s]Evaluating:  98%|█████████▊| 677/692 [04:08<00:05,  2.74it/s]Evaluating:  98%|█████████▊| 678/692 [04:08<00:05,  2.74it/s]Evaluating:  98%|█████████▊| 679/692 [04:09<00:04,  2.74it/s]Evaluating:  98%|█████████▊| 680/692 [04:09<00:04,  2.74it/s]Evaluating:  98%|█████████▊| 681/692 [04:10<00:04,  2.74it/s]Evaluating:  99%|█████████▊| 682/692 [04:10<00:03,  2.73it/s]Evaluating:  99%|█████████▊| 683/692 [04:10<00:03,  2.74it/s]Evaluating:  99%|█████████▉| 684/692 [04:11<00:02,  2.74it/s]Evaluating:  99%|█████████▉| 685/692 [04:11<00:02,  2.74it/s]Evaluating:  99%|█████████▉| 686/692 [04:11<00:02,  2.74it/s]Evaluating:  99%|█████████▉| 687/692 [04:12<00:01,  2.73it/s]Evaluating:  99%|█████████▉| 688/692 [04:12<00:01,  2.73it/s]Evaluating: 100%|█████████▉| 689/692 [04:12<00:01,  2.73it/s]Evaluating: 100%|█████████▉| 690/692 [04:13<00:00,  2.73it/s]Evaluating: 100%|█████████▉| 691/692 [04:13<00:00,  2.72it/s]Evaluating: 100%|██████████| 692/692 [04:13<00:00,  3.16it/s]Evaluating: 100%|██████████| 692/692 [04:13<00:00,  2.73it/s]
05/10/2022 00:09:00 - INFO - __main__ -     Evaluation done in total 253.869618 secs (0.045891 sec per example)
05/10/2022 00:09:23 - INFO - __main__ -   Results: {'exact': 50.16181229773463, 'f1': 71.79428017941842, 'total': 5253, 'HasAns_exact': 50.16181229773463, 'HasAns_f1': 71.79428017941842, 'HasAns_total': 5253, 'best_exact': 50.16181229773463, 'best_exact_thresh': 0.0, 'best_f1': 71.79428017941842, 'best_f1_thresh': 0.0}
  de 
2022-05-10 00:09:26.778884: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/10/2022 00:09:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'qa_outputs.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.20.attention.self.key.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.16.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.16.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:09:54 - INFO - __main__ -   lang2id = None
05/10/2022 00:09:58 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='de', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-de-question-de.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/10/2022 00:09:58 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/10/2022 00:09:58 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'qa_outputs.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.20.attention.self.key.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.16.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.16.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:10:22 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/2525 [00:00<?, ?it/s]  5%|▍         | 121/2525 [00:00<00:01, 1206.77it/s] 11%|█         | 273/2525 [00:00<00:01, 1355.31it/s] 16%|█▌        | 409/2525 [00:00<00:01, 1072.07it/s] 21%|██        | 523/2525 [00:00<00:01, 1091.59it/s] 28%|██▊       | 701/2525 [00:00<00:01, 1267.82it/s] 35%|███▌      | 884/2525 [00:00<00:01, 1385.31it/s] 41%|████      | 1025/2525 [00:00<00:01, 1376.98it/s] 46%|████▌     | 1165/2525 [00:00<00:01, 1246.25it/s] 51%|█████▏    | 1298/2525 [00:01<00:00, 1266.11it/s] 58%|█████▊    | 1463/2525 [00:01<00:00, 1372.15it/s] 63%|██████▎   | 1603/2525 [00:01<00:00, 1192.61it/s] 69%|██████▉   | 1744/2525 [00:01<00:00, 1245.90it/s] 76%|███████▌  | 1912/2525 [00:01<00:00, 1338.01it/s] 81%|████████  | 2050/2525 [00:01<00:00, 1047.53it/s] 91%|█████████ | 2288/2525 [00:01<00:00, 1359.32it/s]100%|██████████| 2525/2525 [00:01<00:00, 1351.03it/s]
convert squad examples to features:   0%|          | 0/4517 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/4517 [00:00<09:42,  7.76it/s]convert squad examples to features:   1%|          | 33/4517 [00:00<00:48, 92.97it/s]convert squad examples to features:   1%|▏         | 65/4517 [00:00<00:33, 134.71it/s]convert squad examples to features:   2%|▏         | 97/4517 [00:01<00:47, 93.74it/s] convert squad examples to features:   3%|▎         | 129/4517 [00:01<00:43, 101.15it/s]convert squad examples to features:   4%|▎         | 161/4517 [00:01<00:34, 125.74it/s]convert squad examples to features:   4%|▍         | 193/4517 [00:01<00:27, 154.69it/s]convert squad examples to features:   5%|▍         | 225/4517 [00:01<00:29, 145.87it/s]convert squad examples to features:   6%|▌         | 257/4517 [00:02<00:28, 149.77it/s]convert squad examples to features:   7%|▋         | 321/4517 [00:02<00:26, 160.54it/s]convert squad examples to features:   9%|▊         | 385/4517 [00:02<00:25, 161.59it/s]convert squad examples to features:   9%|▉         | 417/4517 [00:02<00:23, 177.50it/s]convert squad examples to features:  10%|▉         | 449/4517 [00:03<00:21, 186.62it/s]convert squad examples to features:  11%|█         | 481/4517 [00:03<00:25, 158.19it/s]convert squad examples to features:  11%|█▏        | 513/4517 [00:03<00:23, 170.59it/s]convert squad examples to features:  12%|█▏        | 545/4517 [00:03<00:22, 177.58it/s]convert squad examples to features:  13%|█▎        | 577/4517 [00:04<00:33, 117.18it/s]convert squad examples to features:  13%|█▎        | 609/4517 [00:04<00:30, 130.14it/s]convert squad examples to features:  14%|█▍        | 641/4517 [00:04<00:27, 142.84it/s]convert squad examples to features:  15%|█▍        | 673/4517 [00:04<00:23, 162.13it/s]convert squad examples to features:  16%|█▌        | 705/4517 [00:05<00:31, 119.91it/s]convert squad examples to features:  16%|█▋        | 737/4517 [00:05<00:26, 141.10it/s]convert squad examples to features:  18%|█▊        | 801/4517 [00:05<00:23, 156.54it/s]convert squad examples to features:  18%|█▊        | 833/4517 [00:05<00:21, 173.52it/s]convert squad examples to features:  19%|█▉        | 865/4517 [00:05<00:19, 189.14it/s]convert squad examples to features:  21%|██        | 929/4517 [00:06<00:22, 159.58it/s]convert squad examples to features:  21%|██▏       | 961/4517 [00:06<00:21, 167.54it/s]convert squad examples to features:  22%|██▏       | 993/4517 [00:06<00:19, 181.88it/s]convert squad examples to features:  23%|██▎       | 1025/4517 [00:06<00:17, 199.68it/s]convert squad examples to features:  23%|██▎       | 1057/4517 [00:06<00:15, 217.46it/s]convert squad examples to features:  24%|██▍       | 1089/4517 [00:06<00:16, 213.95it/s]convert squad examples to features:  25%|██▍       | 1121/4517 [00:07<00:14, 228.92it/s]convert squad examples to features:  26%|██▌       | 1153/4517 [00:07<00:14, 237.69it/s]convert squad examples to features:  26%|██▌       | 1185/4517 [00:07<00:16, 200.09it/s]convert squad examples to features:  27%|██▋       | 1217/4517 [00:07<00:15, 211.35it/s]convert squad examples to features:  28%|██▊       | 1249/4517 [00:07<00:15, 211.29it/s]convert squad examples to features:  29%|██▉       | 1313/4517 [00:07<00:13, 241.36it/s]convert squad examples to features:  30%|██▉       | 1345/4517 [00:08<00:15, 202.42it/s]convert squad examples to features:  31%|███       | 1409/4517 [00:08<00:13, 234.14it/s]convert squad examples to features:  32%|███▏      | 1441/4517 [00:08<00:14, 219.11it/s]convert squad examples to features:  33%|███▎      | 1473/4517 [00:08<00:13, 228.35it/s]convert squad examples to features:  33%|███▎      | 1505/4517 [00:08<00:13, 230.71it/s]convert squad examples to features:  34%|███▍      | 1537/4517 [00:08<00:12, 231.03it/s]convert squad examples to features:  35%|███▍      | 1569/4517 [00:09<00:12, 232.63it/s]convert squad examples to features:  35%|███▌      | 1601/4517 [00:09<00:14, 199.91it/s]convert squad examples to features:  36%|███▌      | 1633/4517 [00:09<00:17, 165.67it/s]convert squad examples to features:  37%|███▋      | 1665/4517 [00:09<00:17, 162.81it/s]convert squad examples to features:  38%|███▊      | 1697/4517 [00:09<00:17, 158.39it/s]convert squad examples to features:  38%|███▊      | 1729/4517 [00:10<00:15, 174.80it/s]convert squad examples to features:  39%|███▉      | 1761/4517 [00:10<00:13, 200.36it/s]convert squad examples to features:  40%|███▉      | 1793/4517 [00:10<00:13, 205.23it/s]convert squad examples to features:  40%|████      | 1825/4517 [00:10<00:15, 169.50it/s]convert squad examples to features:  41%|████      | 1857/4517 [00:10<00:17, 149.52it/s]convert squad examples to features:  42%|████▏     | 1889/4517 [00:11<00:17, 153.46it/s]convert squad examples to features:  43%|████▎     | 1921/4517 [00:11<00:14, 177.20it/s]convert squad examples to features:  43%|████▎     | 1953/4517 [00:11<00:12, 197.33it/s]convert squad examples to features:  44%|████▍     | 1985/4517 [00:11<00:12, 209.81it/s]convert squad examples to features:  45%|████▍     | 2017/4517 [00:11<00:12, 203.40it/s]convert squad examples to features:  45%|████▌     | 2049/4517 [00:11<00:10, 228.26it/s]convert squad examples to features:  46%|████▌     | 2081/4517 [00:11<00:12, 187.90it/s]convert squad examples to features:  47%|████▋     | 2113/4517 [00:12<00:13, 174.67it/s]convert squad examples to features:  48%|████▊     | 2177/4517 [00:12<00:10, 217.51it/s]convert squad examples to features:  49%|████▉     | 2209/4517 [00:12<00:13, 171.64it/s]convert squad examples to features:  50%|████▉     | 2241/4517 [00:12<00:11, 194.95it/s]convert squad examples to features:  50%|█████     | 2273/4517 [00:13<00:15, 145.55it/s]convert squad examples to features:  52%|█████▏    | 2337/4517 [00:13<00:10, 200.15it/s]convert squad examples to features:  53%|█████▎    | 2401/4517 [00:13<00:09, 221.26it/s]convert squad examples to features:  54%|█████▍    | 2433/4517 [00:13<00:09, 216.06it/s]convert squad examples to features:  55%|█████▍    | 2465/4517 [00:13<00:11, 184.73it/s]convert squad examples to features:  55%|█████▌    | 2497/4517 [00:14<00:10, 199.31it/s]convert squad examples to features:  56%|█████▌    | 2529/4517 [00:14<00:21, 92.31it/s] convert squad examples to features:  57%|█████▋    | 2561/4517 [00:15<00:17, 110.64it/s]convert squad examples to features:  57%|█████▋    | 2593/4517 [00:15<00:15, 126.25it/s]convert squad examples to features:  58%|█████▊    | 2625/4517 [00:15<00:12, 146.16it/s]convert squad examples to features:  59%|█████▉    | 2657/4517 [00:15<00:11, 167.96it/s]convert squad examples to features:  60%|█████▉    | 2689/4517 [00:15<00:10, 181.31it/s]convert squad examples to features:  60%|██████    | 2721/4517 [00:15<00:10, 176.96it/s]convert squad examples to features:  61%|██████    | 2753/4517 [00:16<00:09, 191.41it/s]convert squad examples to features:  62%|██████▏   | 2785/4517 [00:16<00:09, 180.22it/s]convert squad examples to features:  62%|██████▏   | 2817/4517 [00:16<00:08, 194.02it/s]convert squad examples to features:  63%|██████▎   | 2849/4517 [00:16<00:07, 211.11it/s]convert squad examples to features:  64%|██████▍   | 2881/4517 [00:16<00:07, 209.30it/s]convert squad examples to features:  64%|██████▍   | 2913/4517 [00:16<00:09, 176.36it/s]convert squad examples to features:  65%|██████▌   | 2945/4517 [00:17<00:12, 121.03it/s]convert squad examples to features:  66%|██████▌   | 2977/4517 [00:17<00:11, 135.38it/s]convert squad examples to features:  67%|██████▋   | 3009/4517 [00:17<00:13, 111.87it/s]convert squad examples to features:  67%|██████▋   | 3041/4517 [00:18<00:10, 134.79it/s]convert squad examples to features:  68%|██████▊   | 3073/4517 [00:18<00:10, 137.99it/s]convert squad examples to features:  69%|██████▊   | 3105/4517 [00:18<00:10, 141.06it/s]convert squad examples to features:  69%|██████▉   | 3137/4517 [00:18<00:09, 139.74it/s]convert squad examples to features:  70%|███████   | 3169/4517 [00:18<00:08, 161.93it/s]convert squad examples to features:  71%|███████   | 3201/4517 [00:18<00:07, 179.56it/s]convert squad examples to features:  72%|███████▏  | 3233/4517 [00:19<00:07, 165.97it/s]convert squad examples to features:  72%|███████▏  | 3265/4517 [00:19<00:06, 193.79it/s]convert squad examples to features:  73%|███████▎  | 3297/4517 [00:19<00:05, 207.42it/s]convert squad examples to features:  74%|███████▎  | 3329/4517 [00:19<00:05, 220.58it/s]convert squad examples to features:  74%|███████▍  | 3361/4517 [00:19<00:05, 225.46it/s]convert squad examples to features:  75%|███████▌  | 3393/4517 [00:19<00:05, 201.72it/s]convert squad examples to features:  76%|███████▌  | 3425/4517 [00:20<00:06, 163.73it/s]convert squad examples to features:  77%|███████▋  | 3457/4517 [00:20<00:06, 151.66it/s]convert squad examples to features:  77%|███████▋  | 3489/4517 [00:20<00:06, 161.24it/s]convert squad examples to features:  79%|███████▊  | 3553/4517 [00:20<00:05, 178.34it/s]convert squad examples to features:  79%|███████▉  | 3585/4517 [00:20<00:04, 199.88it/s]convert squad examples to features:  80%|████████  | 3617/4517 [00:21<00:04, 208.77it/s]convert squad examples to features:  81%|████████  | 3649/4517 [00:21<00:04, 181.80it/s]convert squad examples to features:  81%|████████▏ | 3681/4517 [00:21<00:05, 151.36it/s]convert squad examples to features:  82%|████████▏ | 3713/4517 [00:22<00:07, 112.96it/s]convert squad examples to features:  83%|████████▎ | 3745/4517 [00:22<00:07, 96.88it/s] convert squad examples to features:  84%|████████▎ | 3777/4517 [00:23<00:08, 84.51it/s]convert squad examples to features:  84%|████████▍ | 3809/4517 [00:23<00:07, 94.29it/s]convert squad examples to features:  85%|████████▌ | 3841/4517 [00:23<00:07, 93.61it/s]convert squad examples to features:  86%|████████▌ | 3873/4517 [00:23<00:06, 93.31it/s]convert squad examples to features:  86%|████████▋ | 3905/4517 [00:24<00:05, 118.08it/s]convert squad examples to features:  87%|████████▋ | 3937/4517 [00:24<00:04, 141.04it/s]convert squad examples to features:  89%|████████▊ | 4001/4517 [00:24<00:02, 183.12it/s]convert squad examples to features:  89%|████████▉ | 4033/4517 [00:24<00:02, 196.21it/s]convert squad examples to features:  90%|████████▉ | 4065/4517 [00:24<00:02, 216.86it/s]convert squad examples to features:  91%|█████████ | 4097/4517 [00:24<00:02, 200.65it/s]convert squad examples to features:  91%|█████████▏| 4129/4517 [00:24<00:01, 222.78it/s]convert squad examples to features:  92%|█████████▏| 4161/4517 [00:25<00:01, 224.73it/s]convert squad examples to features:  93%|█████████▎| 4193/4517 [00:25<00:01, 219.20it/s]convert squad examples to features:  94%|█████████▎| 4225/4517 [00:25<00:01, 214.56it/s]convert squad examples to features:  95%|█████████▍| 4289/4517 [00:25<00:00, 255.66it/s]convert squad examples to features:  96%|█████████▌| 4321/4517 [00:25<00:00, 257.90it/s]convert squad examples to features:  97%|█████████▋| 4385/4517 [00:26<00:00, 233.37it/s]convert squad examples to features:  98%|█████████▊| 4449/4517 [00:26<00:00, 253.14it/s]convert squad examples to features:  99%|█████████▉| 4481/4517 [00:26<00:00, 261.37it/s]convert squad examples to features: 100%|██████████| 4517/4517 [00:26<00:00, 171.23it/s]
add example index and unique id:   0%|          | 0/4517 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 4517/4517 [00:00<00:00, 621006.66it/s]
05/10/2022 00:10:51 - INFO - __main__ -   Saving features into cached file ./cached_test-context-de-question-de.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_de
05/10/2022 00:10:58 - INFO - __main__ -   ***** Running evaluation  *****
05/10/2022 00:10:58 - INFO - __main__ -     Num examples = 5697
05/10/2022 00:10:58 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/713 [00:00<?, ?it/s]Evaluating:   0%|          | 1/713 [00:00<11:45,  1.01it/s]Evaluating:   0%|          | 2/713 [00:01<07:22,  1.61it/s]Evaluating:   0%|          | 3/713 [00:01<05:58,  1.98it/s]Evaluating:   1%|          | 4/713 [00:02<05:17,  2.23it/s]Evaluating:   1%|          | 5/713 [00:02<04:56,  2.39it/s]Evaluating:   1%|          | 6/713 [00:02<04:42,  2.50it/s]Evaluating:   1%|          | 7/713 [00:03<04:33,  2.58it/s]Evaluating:   1%|          | 8/713 [00:03<04:28,  2.63it/s]Evaluating:   1%|▏         | 9/713 [00:03<04:23,  2.67it/s]Evaluating:   1%|▏         | 10/713 [00:04<04:22,  2.68it/s]Evaluating:   2%|▏         | 11/713 [00:04<04:19,  2.70it/s]Evaluating:   2%|▏         | 12/713 [00:04<04:18,  2.71it/s]Evaluating:   2%|▏         | 13/713 [00:05<04:18,  2.71it/s]Evaluating:   2%|▏         | 14/713 [00:05<04:17,  2.72it/s]Evaluating:   2%|▏         | 15/713 [00:06<04:15,  2.73it/s]Evaluating:   2%|▏         | 16/713 [00:06<04:14,  2.74it/s]Evaluating:   2%|▏         | 17/713 [00:06<04:14,  2.74it/s]Evaluating:   3%|▎         | 18/713 [00:07<04:14,  2.73it/s]Evaluating:   3%|▎         | 19/713 [00:07<04:13,  2.74it/s]Evaluating:   3%|▎         | 20/713 [00:07<04:14,  2.73it/s]Evaluating:   3%|▎         | 21/713 [00:08<04:13,  2.73it/s]Evaluating:   3%|▎         | 22/713 [00:08<04:14,  2.72it/s]Evaluating:   3%|▎         | 23/713 [00:09<04:13,  2.73it/s]Evaluating:   3%|▎         | 24/713 [00:09<04:13,  2.72it/s]Evaluating:   4%|▎         | 25/713 [00:09<04:12,  2.73it/s]Evaluating:   4%|▎         | 26/713 [00:10<04:12,  2.72it/s]Evaluating:   4%|▍         | 27/713 [00:10<04:12,  2.72it/s]Evaluating:   4%|▍         | 28/713 [00:10<04:12,  2.72it/s]Evaluating:   4%|▍         | 29/713 [00:11<04:12,  2.71it/s]Evaluating:   4%|▍         | 30/713 [00:11<04:12,  2.71it/s]Evaluating:   4%|▍         | 31/713 [00:11<04:11,  2.71it/s]Evaluating:   4%|▍         | 32/713 [00:12<04:10,  2.72it/s]Evaluating:   5%|▍         | 33/713 [00:12<04:09,  2.72it/s]Evaluating:   5%|▍         | 34/713 [00:13<04:08,  2.73it/s]Evaluating:   5%|▍         | 35/713 [00:13<04:07,  2.74it/s]Evaluating:   5%|▌         | 36/713 [00:13<04:07,  2.74it/s]Evaluating:   5%|▌         | 37/713 [00:14<04:06,  2.74it/s]Evaluating:   5%|▌         | 38/713 [00:14<04:05,  2.75it/s]Evaluating:   5%|▌         | 39/713 [00:14<04:05,  2.75it/s]Evaluating:   6%|▌         | 40/713 [00:15<04:05,  2.74it/s]Evaluating:   6%|▌         | 41/713 [00:15<04:05,  2.74it/s]Evaluating:   6%|▌         | 42/713 [00:15<04:05,  2.73it/s]Evaluating:   6%|▌         | 43/713 [00:16<04:04,  2.74it/s]Evaluating:   6%|▌         | 44/713 [00:16<04:03,  2.74it/s]Evaluating:   6%|▋         | 45/713 [00:17<04:02,  2.75it/s]Evaluating:   6%|▋         | 46/713 [00:17<04:02,  2.75it/s]Evaluating:   7%|▋         | 47/713 [00:17<04:02,  2.75it/s]Evaluating:   7%|▋         | 48/713 [00:18<04:02,  2.74it/s]Evaluating:   7%|▋         | 49/713 [00:18<04:02,  2.74it/s]Evaluating:   7%|▋         | 50/713 [00:18<04:02,  2.74it/s]Evaluating:   7%|▋         | 51/713 [00:19<04:01,  2.74it/s]Evaluating:   7%|▋         | 52/713 [00:19<04:02,  2.73it/s]Evaluating:   7%|▋         | 53/713 [00:20<04:01,  2.73it/s]Evaluating:   8%|▊         | 54/713 [00:20<04:01,  2.72it/s]Evaluating:   8%|▊         | 55/713 [00:20<04:06,  2.67it/s]Evaluating:   8%|▊         | 56/713 [00:21<04:04,  2.69it/s]Evaluating:   8%|▊         | 57/713 [00:21<04:02,  2.71it/s]Evaluating:   8%|▊         | 58/713 [00:21<04:00,  2.72it/s]Evaluating:   8%|▊         | 59/713 [00:22<03:59,  2.73it/s]Evaluating:   8%|▊         | 60/713 [00:22<03:58,  2.74it/s]Evaluating:   9%|▊         | 61/713 [00:22<03:56,  2.75it/s]Evaluating:   9%|▊         | 62/713 [00:23<03:56,  2.75it/s]Evaluating:   9%|▉         | 63/713 [00:23<03:56,  2.74it/s]Evaluating:   9%|▉         | 64/713 [00:24<03:56,  2.75it/s]Evaluating:   9%|▉         | 65/713 [00:24<03:56,  2.74it/s]Evaluating:   9%|▉         | 66/713 [00:24<03:56,  2.73it/s]Evaluating:   9%|▉         | 67/713 [00:25<03:57,  2.72it/s]Evaluating:  10%|▉         | 68/713 [00:25<03:56,  2.72it/s]Evaluating:  10%|▉         | 69/713 [00:25<03:55,  2.73it/s]Evaluating:  10%|▉         | 70/713 [00:26<03:56,  2.72it/s]Evaluating:  10%|▉         | 71/713 [00:26<03:55,  2.72it/s]Evaluating:  10%|█         | 72/713 [00:26<03:54,  2.73it/s]Evaluating:  10%|█         | 73/713 [00:27<03:53,  2.74it/s]Evaluating:  10%|█         | 74/713 [00:27<03:53,  2.74it/s]Evaluating:  11%|█         | 75/713 [00:28<03:52,  2.74it/s]Evaluating:  11%|█         | 76/713 [00:28<03:51,  2.75it/s]Evaluating:  11%|█         | 77/713 [00:28<03:50,  2.75it/s]Evaluating:  11%|█         | 78/713 [00:29<03:51,  2.74it/s]Evaluating:  11%|█         | 79/713 [00:29<03:50,  2.75it/s]Evaluating:  11%|█         | 80/713 [00:29<03:50,  2.75it/s]Evaluating:  11%|█▏        | 81/713 [00:30<03:50,  2.75it/s]Evaluating:  12%|█▏        | 82/713 [00:30<03:49,  2.74it/s]Evaluating:  12%|█▏        | 83/713 [00:30<03:50,  2.73it/s]Evaluating:  12%|█▏        | 84/713 [00:31<03:50,  2.73it/s]Evaluating:  12%|█▏        | 85/713 [00:31<03:50,  2.72it/s]Evaluating:  12%|█▏        | 86/713 [00:32<03:50,  2.72it/s]Evaluating:  12%|█▏        | 87/713 [00:32<03:50,  2.72it/s]Evaluating:  12%|█▏        | 88/713 [00:32<03:48,  2.73it/s]Evaluating:  12%|█▏        | 89/713 [00:33<03:47,  2.74it/s]Evaluating:  13%|█▎        | 90/713 [00:33<03:47,  2.74it/s]Evaluating:  13%|█▎        | 91/713 [00:33<03:47,  2.74it/s]Evaluating:  13%|█▎        | 92/713 [00:34<03:46,  2.75it/s]Evaluating:  13%|█▎        | 93/713 [00:34<03:46,  2.73it/s]Evaluating:  13%|█▎        | 94/713 [00:35<03:46,  2.73it/s]Evaluating:  13%|█▎        | 95/713 [00:35<03:46,  2.73it/s]Evaluating:  13%|█▎        | 96/713 [00:35<03:46,  2.73it/s]Evaluating:  14%|█▎        | 97/713 [00:36<03:47,  2.71it/s]Evaluating:  14%|█▎        | 98/713 [00:36<03:46,  2.72it/s]Evaluating:  14%|█▍        | 99/713 [00:36<03:45,  2.72it/s]Evaluating:  14%|█▍        | 100/713 [00:37<03:45,  2.72it/s]Evaluating:  14%|█▍        | 101/713 [00:37<03:44,  2.73it/s]Evaluating:  14%|█▍        | 102/713 [00:37<03:44,  2.73it/s]Evaluating:  14%|█▍        | 103/713 [00:38<03:42,  2.74it/s]Evaluating:  15%|█▍        | 104/713 [00:38<03:43,  2.73it/s]Evaluating:  15%|█▍        | 105/713 [00:39<03:42,  2.73it/s]Evaluating:  15%|█▍        | 106/713 [00:39<03:42,  2.73it/s]Evaluating:  15%|█▌        | 107/713 [00:39<03:42,  2.72it/s]Evaluating:  15%|█▌        | 108/713 [00:40<03:42,  2.72it/s]Evaluating:  15%|█▌        | 109/713 [00:40<03:40,  2.74it/s]Evaluating:  15%|█▌        | 110/713 [00:40<03:41,  2.73it/s]Evaluating:  16%|█▌        | 111/713 [00:41<03:41,  2.72it/s]Evaluating:  16%|█▌        | 112/713 [00:41<03:40,  2.73it/s]Evaluating:  16%|█▌        | 113/713 [00:41<03:39,  2.74it/s]Evaluating:  16%|█▌        | 114/713 [00:42<03:39,  2.73it/s]Evaluating:  16%|█▌        | 115/713 [00:42<03:39,  2.72it/s]Evaluating:  16%|█▋        | 116/713 [00:43<03:39,  2.72it/s]Evaluating:  16%|█▋        | 117/713 [00:43<03:39,  2.72it/s]Evaluating:  17%|█▋        | 118/713 [00:43<03:38,  2.72it/s]Evaluating:  17%|█▋        | 119/713 [00:44<03:38,  2.71it/s]Evaluating:  17%|█▋        | 120/713 [00:44<03:38,  2.71it/s]Evaluating:  17%|█▋        | 121/713 [00:44<03:36,  2.73it/s]Evaluating:  17%|█▋        | 122/713 [00:45<03:36,  2.73it/s]Evaluating:  17%|█▋        | 123/713 [00:45<03:36,  2.73it/s]Evaluating:  17%|█▋        | 124/713 [00:46<03:36,  2.72it/s]Evaluating:  18%|█▊        | 125/713 [00:46<03:35,  2.73it/s]Evaluating:  18%|█▊        | 126/713 [00:46<03:35,  2.72it/s]Evaluating:  18%|█▊        | 127/713 [00:47<03:34,  2.73it/s]Evaluating:  18%|█▊        | 128/713 [00:47<03:33,  2.74it/s]Evaluating:  18%|█▊        | 129/713 [00:47<03:33,  2.74it/s]Evaluating:  18%|█▊        | 130/713 [00:48<03:32,  2.74it/s]Evaluating:  18%|█▊        | 131/713 [00:48<03:32,  2.74it/s]Evaluating:  19%|█▊        | 132/713 [00:48<03:32,  2.73it/s]Evaluating:  19%|█▊        | 133/713 [00:49<03:32,  2.72it/s]Evaluating:  19%|█▉        | 134/713 [00:49<03:31,  2.73it/s]Evaluating:  19%|█▉        | 135/713 [00:50<03:31,  2.73it/s]Evaluating:  19%|█▉        | 136/713 [00:50<03:31,  2.73it/s]Evaluating:  19%|█▉        | 137/713 [00:50<03:31,  2.73it/s]Evaluating:  19%|█▉        | 138/713 [00:51<03:30,  2.73it/s]Evaluating:  19%|█▉        | 139/713 [00:51<03:29,  2.74it/s]Evaluating:  20%|█▉        | 140/713 [00:51<03:29,  2.74it/s]Evaluating:  20%|█▉        | 141/713 [00:52<03:28,  2.74it/s]Evaluating:  20%|█▉        | 142/713 [00:52<03:27,  2.75it/s]Evaluating:  20%|██        | 143/713 [00:52<03:27,  2.74it/s]Evaluating:  20%|██        | 144/713 [00:53<03:26,  2.75it/s]Evaluating:  20%|██        | 145/713 [00:53<03:27,  2.74it/s]Evaluating:  20%|██        | 146/713 [00:54<03:27,  2.73it/s]Evaluating:  21%|██        | 147/713 [00:54<03:28,  2.71it/s]Evaluating:  21%|██        | 148/713 [00:54<03:28,  2.71it/s]Evaluating:  21%|██        | 149/713 [00:55<03:27,  2.72it/s]Evaluating:  21%|██        | 150/713 [00:55<03:26,  2.73it/s]Evaluating:  21%|██        | 151/713 [00:55<03:25,  2.73it/s]Evaluating:  21%|██▏       | 152/713 [00:56<03:25,  2.73it/s]Evaluating:  21%|██▏       | 153/713 [00:56<03:25,  2.72it/s]Evaluating:  22%|██▏       | 154/713 [00:57<03:26,  2.71it/s]Evaluating:  22%|██▏       | 155/713 [00:57<03:26,  2.70it/s]Evaluating:  22%|██▏       | 156/713 [00:57<03:25,  2.71it/s]Evaluating:  22%|██▏       | 157/713 [00:58<03:24,  2.72it/s]Evaluating:  22%|██▏       | 158/713 [00:58<03:23,  2.73it/s]Evaluating:  22%|██▏       | 159/713 [00:58<03:22,  2.74it/s]Evaluating:  22%|██▏       | 160/713 [00:59<03:22,  2.73it/s]Evaluating:  23%|██▎       | 161/713 [00:59<03:22,  2.72it/s]Evaluating:  23%|██▎       | 162/713 [00:59<03:23,  2.71it/s]Evaluating:  23%|██▎       | 163/713 [01:00<03:22,  2.72it/s]Evaluating:  23%|██▎       | 164/713 [01:00<03:21,  2.73it/s]Evaluating:  23%|██▎       | 165/713 [01:01<03:20,  2.73it/s]Evaluating:  23%|██▎       | 166/713 [01:01<03:20,  2.73it/s]Evaluating:  23%|██▎       | 167/713 [01:01<03:20,  2.73it/s]Evaluating:  24%|██▎       | 168/713 [01:02<03:19,  2.73it/s]Evaluating:  24%|██▎       | 169/713 [01:02<03:19,  2.73it/s]Evaluating:  24%|██▍       | 170/713 [01:02<03:19,  2.72it/s]Evaluating:  24%|██▍       | 171/713 [01:03<03:19,  2.72it/s]Evaluating:  24%|██▍       | 172/713 [01:03<03:18,  2.72it/s]Evaluating:  24%|██▍       | 173/713 [01:03<03:18,  2.72it/s]Evaluating:  24%|██▍       | 174/713 [01:04<03:17,  2.73it/s]Evaluating:  25%|██▍       | 175/713 [01:04<03:16,  2.74it/s]Evaluating:  25%|██▍       | 176/713 [01:05<03:15,  2.74it/s]Evaluating:  25%|██▍       | 177/713 [01:05<03:16,  2.73it/s]Evaluating:  25%|██▍       | 178/713 [01:05<03:15,  2.73it/s]Evaluating:  25%|██▌       | 179/713 [01:06<03:15,  2.73it/s]Evaluating:  25%|██▌       | 180/713 [01:06<03:15,  2.73it/s]Evaluating:  25%|██▌       | 181/713 [01:06<03:14,  2.74it/s]Evaluating:  26%|██▌       | 182/713 [01:07<03:13,  2.75it/s]Evaluating:  26%|██▌       | 183/713 [01:07<03:13,  2.74it/s]Evaluating:  26%|██▌       | 184/713 [01:07<03:13,  2.73it/s]Evaluating:  26%|██▌       | 185/713 [01:08<03:13,  2.73it/s]Evaluating:  26%|██▌       | 186/713 [01:08<03:12,  2.73it/s]Evaluating:  26%|██▌       | 187/713 [01:09<03:12,  2.73it/s]Evaluating:  26%|██▋       | 188/713 [01:09<03:12,  2.73it/s]Evaluating:  27%|██▋       | 189/713 [01:09<03:12,  2.72it/s]Evaluating:  27%|██▋       | 190/713 [01:10<03:12,  2.71it/s]Evaluating:  27%|██▋       | 191/713 [01:10<03:12,  2.71it/s]Evaluating:  27%|██▋       | 192/713 [01:10<03:11,  2.72it/s]Evaluating:  27%|██▋       | 193/713 [01:11<03:10,  2.72it/s]Evaluating:  27%|██▋       | 194/713 [01:11<03:10,  2.72it/s]Evaluating:  27%|██▋       | 195/713 [01:12<03:10,  2.73it/s]Evaluating:  27%|██▋       | 196/713 [01:12<03:09,  2.72it/s]Evaluating:  28%|██▊       | 197/713 [01:12<03:09,  2.72it/s]Evaluating:  28%|██▊       | 198/713 [01:13<03:08,  2.73it/s]Evaluating:  28%|██▊       | 199/713 [01:13<03:08,  2.73it/s]Evaluating:  28%|██▊       | 200/713 [01:13<03:07,  2.73it/s]Evaluating:  28%|██▊       | 201/713 [01:14<03:07,  2.73it/s]Evaluating:  28%|██▊       | 202/713 [01:14<03:06,  2.74it/s]Evaluating:  28%|██▊       | 203/713 [01:14<03:05,  2.75it/s]Evaluating:  29%|██▊       | 204/713 [01:15<03:04,  2.75it/s]Evaluating:  29%|██▉       | 205/713 [01:15<03:04,  2.75it/s]Evaluating:  29%|██▉       | 206/713 [01:16<03:04,  2.75it/s]Evaluating:  29%|██▉       | 207/713 [01:16<03:03,  2.75it/s]Evaluating:  29%|██▉       | 208/713 [01:16<03:03,  2.75it/s]Evaluating:  29%|██▉       | 209/713 [01:17<03:03,  2.75it/s]Evaluating:  29%|██▉       | 210/713 [01:17<03:04,  2.73it/s]Evaluating:  30%|██▉       | 211/713 [01:17<03:04,  2.73it/s]Evaluating:  30%|██▉       | 212/713 [01:18<03:04,  2.72it/s]Evaluating:  30%|██▉       | 213/713 [01:18<03:04,  2.71it/s]Evaluating:  30%|███       | 214/713 [01:18<03:03,  2.72it/s]Evaluating:  30%|███       | 215/713 [01:19<03:02,  2.73it/s]Evaluating:  30%|███       | 216/713 [01:19<03:01,  2.73it/s]Evaluating:  30%|███       | 217/713 [01:20<03:01,  2.74it/s]Evaluating:  31%|███       | 218/713 [01:20<03:00,  2.73it/s]Evaluating:  31%|███       | 219/713 [01:20<02:59,  2.74it/s]Evaluating:  31%|███       | 220/713 [01:21<02:59,  2.75it/s]Evaluating:  31%|███       | 221/713 [01:21<02:59,  2.74it/s]Evaluating:  31%|███       | 222/713 [01:21<02:59,  2.74it/s]Evaluating:  31%|███▏      | 223/713 [01:22<02:59,  2.74it/s]Evaluating:  31%|███▏      | 224/713 [01:22<02:59,  2.73it/s]Evaluating:  32%|███▏      | 225/713 [01:22<02:58,  2.74it/s]Evaluating:  32%|███▏      | 226/713 [01:23<02:57,  2.75it/s]Evaluating:  32%|███▏      | 227/713 [01:23<02:56,  2.75it/s]Evaluating:  32%|███▏      | 228/713 [01:24<02:56,  2.75it/s]Evaluating:  32%|███▏      | 229/713 [01:24<02:55,  2.75it/s]Evaluating:  32%|███▏      | 230/713 [01:24<02:55,  2.75it/s]Evaluating:  32%|███▏      | 231/713 [01:25<02:56,  2.74it/s]Evaluating:  33%|███▎      | 232/713 [01:25<02:55,  2.74it/s]Evaluating:  33%|███▎      | 233/713 [01:25<02:55,  2.74it/s]Evaluating:  33%|███▎      | 234/713 [01:26<02:54,  2.74it/s]Evaluating:  33%|███▎      | 235/713 [01:26<02:55,  2.73it/s]Evaluating:  33%|███▎      | 236/713 [01:27<02:54,  2.73it/s]Evaluating:  33%|███▎      | 237/713 [01:27<02:58,  2.66it/s]Evaluating:  33%|███▎      | 238/713 [01:27<02:56,  2.69it/s]Evaluating:  34%|███▎      | 239/713 [01:28<02:55,  2.70it/s]Evaluating:  34%|███▎      | 240/713 [01:28<02:54,  2.71it/s]Evaluating:  34%|███▍      | 241/713 [01:28<02:54,  2.71it/s]Evaluating:  34%|███▍      | 242/713 [01:29<02:53,  2.72it/s]Evaluating:  34%|███▍      | 243/713 [01:29<02:53,  2.72it/s]Evaluating:  34%|███▍      | 244/713 [01:29<02:52,  2.71it/s]Evaluating:  34%|███▍      | 245/713 [01:30<02:52,  2.72it/s]Evaluating:  35%|███▍      | 246/713 [01:30<02:51,  2.72it/s]Evaluating:  35%|███▍      | 247/713 [01:31<02:51,  2.71it/s]Evaluating:  35%|███▍      | 248/713 [01:31<02:51,  2.72it/s]Evaluating:  35%|███▍      | 249/713 [01:31<02:50,  2.73it/s]Evaluating:  35%|███▌      | 250/713 [01:32<02:49,  2.72it/s]Evaluating:  35%|███▌      | 251/713 [01:32<02:49,  2.72it/s]Evaluating:  35%|███▌      | 252/713 [01:32<02:49,  2.72it/s]Evaluating:  35%|███▌      | 253/713 [01:33<02:49,  2.71it/s]Evaluating:  36%|███▌      | 254/713 [01:33<02:48,  2.72it/s]Evaluating:  36%|███▌      | 255/713 [01:34<02:49,  2.71it/s]Evaluating:  36%|███▌      | 256/713 [01:34<02:48,  2.71it/s]Evaluating:  36%|███▌      | 257/713 [01:34<02:47,  2.72it/s]Evaluating:  36%|███▌      | 258/713 [01:35<02:46,  2.73it/s]Evaluating:  36%|███▋      | 259/713 [01:35<02:46,  2.73it/s]Evaluating:  36%|███▋      | 260/713 [01:35<02:46,  2.73it/s]Evaluating:  37%|███▋      | 261/713 [01:36<02:46,  2.71it/s]Evaluating:  37%|███▋      | 262/713 [01:36<02:47,  2.69it/s]Evaluating:  37%|███▋      | 263/713 [01:36<02:46,  2.70it/s]Evaluating:  37%|███▋      | 264/713 [01:37<02:46,  2.70it/s]Evaluating:  37%|███▋      | 265/713 [01:37<02:45,  2.70it/s]Evaluating:  37%|███▋      | 266/713 [01:38<02:44,  2.72it/s]Evaluating:  37%|███▋      | 267/713 [01:38<02:43,  2.72it/s]Evaluating:  38%|███▊      | 268/713 [01:38<02:43,  2.72it/s]Evaluating:  38%|███▊      | 269/713 [01:39<02:43,  2.72it/s]Evaluating:  38%|███▊      | 270/713 [01:39<02:42,  2.72it/s]Evaluating:  38%|███▊      | 271/713 [01:39<02:41,  2.73it/s]Evaluating:  38%|███▊      | 272/713 [01:40<02:41,  2.73it/s]Evaluating:  38%|███▊      | 273/713 [01:40<02:40,  2.73it/s]Evaluating:  38%|███▊      | 274/713 [01:41<02:41,  2.71it/s]Evaluating:  39%|███▊      | 275/713 [01:41<02:41,  2.72it/s]Evaluating:  39%|███▊      | 276/713 [01:41<02:40,  2.72it/s]Evaluating:  39%|███▉      | 277/713 [01:42<02:39,  2.73it/s]Evaluating:  39%|███▉      | 278/713 [01:42<02:39,  2.72it/s]Evaluating:  39%|███▉      | 279/713 [01:42<02:39,  2.72it/s]Evaluating:  39%|███▉      | 280/713 [01:43<02:38,  2.73it/s]Evaluating:  39%|███▉      | 281/713 [01:43<02:38,  2.72it/s]Evaluating:  40%|███▉      | 282/713 [01:43<02:39,  2.71it/s]Evaluating:  40%|███▉      | 283/713 [01:44<02:39,  2.69it/s]Evaluating:  40%|███▉      | 284/713 [01:44<02:39,  2.70it/s]Evaluating:  40%|███▉      | 285/713 [01:45<02:38,  2.70it/s]Evaluating:  40%|████      | 286/713 [01:45<02:38,  2.69it/s]Evaluating:  40%|████      | 287/713 [01:45<02:37,  2.71it/s]Evaluating:  40%|████      | 288/713 [01:46<02:36,  2.71it/s]Evaluating:  41%|████      | 289/713 [01:46<02:37,  2.70it/s]Evaluating:  41%|████      | 290/713 [01:46<02:36,  2.70it/s]Evaluating:  41%|████      | 291/713 [01:47<02:36,  2.70it/s]Evaluating:  41%|████      | 292/713 [01:47<02:36,  2.70it/s]Evaluating:  41%|████      | 293/713 [01:48<02:35,  2.70it/s]Evaluating:  41%|████      | 294/713 [01:48<02:34,  2.71it/s]Evaluating:  41%|████▏     | 295/713 [01:48<02:34,  2.70it/s]Evaluating:  42%|████▏     | 296/713 [01:49<02:35,  2.69it/s]Evaluating:  42%|████▏     | 297/713 [01:49<02:34,  2.69it/s]Evaluating:  42%|████▏     | 298/713 [01:49<02:33,  2.70it/s]Evaluating:  42%|████▏     | 299/713 [01:50<02:32,  2.71it/s]Evaluating:  42%|████▏     | 300/713 [01:50<02:32,  2.71it/s]Evaluating:  42%|████▏     | 301/713 [01:50<02:31,  2.72it/s]Evaluating:  42%|████▏     | 302/713 [01:51<02:30,  2.73it/s]Evaluating:  42%|████▏     | 303/713 [01:51<02:30,  2.73it/s]Evaluating:  43%|████▎     | 304/713 [01:52<02:29,  2.74it/s]Evaluating:  43%|████▎     | 305/713 [01:52<02:29,  2.73it/s]Evaluating:  43%|████▎     | 306/713 [01:52<02:29,  2.72it/s]Evaluating:  43%|████▎     | 307/713 [01:53<02:28,  2.73it/s]Evaluating:  43%|████▎     | 308/713 [01:53<02:28,  2.73it/s]Evaluating:  43%|████▎     | 309/713 [01:53<02:28,  2.73it/s]Evaluating:  43%|████▎     | 310/713 [01:54<02:27,  2.74it/s]Evaluating:  44%|████▎     | 311/713 [01:54<02:27,  2.73it/s]Evaluating:  44%|████▍     | 312/713 [01:55<02:27,  2.73it/s]Evaluating:  44%|████▍     | 313/713 [01:55<02:26,  2.73it/s]Evaluating:  44%|████▍     | 314/713 [01:55<02:26,  2.72it/s]Evaluating:  44%|████▍     | 315/713 [01:56<02:26,  2.71it/s]Evaluating:  44%|████▍     | 316/713 [01:56<02:26,  2.71it/s]Evaluating:  44%|████▍     | 317/713 [01:56<02:26,  2.71it/s]Evaluating:  45%|████▍     | 318/713 [01:57<02:25,  2.71it/s]Evaluating:  45%|████▍     | 319/713 [01:57<02:25,  2.70it/s]Evaluating:  45%|████▍     | 320/713 [01:57<02:24,  2.71it/s]Evaluating:  45%|████▌     | 321/713 [01:58<02:23,  2.73it/s]Evaluating:  45%|████▌     | 322/713 [01:58<02:23,  2.72it/s]Evaluating:  45%|████▌     | 323/713 [01:59<02:22,  2.73it/s]Evaluating:  45%|████▌     | 324/713 [01:59<02:22,  2.73it/s]Evaluating:  46%|████▌     | 325/713 [01:59<02:22,  2.73it/s]Evaluating:  46%|████▌     | 326/713 [02:00<02:22,  2.72it/s]Evaluating:  46%|████▌     | 327/713 [02:00<02:21,  2.72it/s]Evaluating:  46%|████▌     | 328/713 [02:00<02:21,  2.71it/s]Evaluating:  46%|████▌     | 329/713 [02:01<02:21,  2.71it/s]Evaluating:  46%|████▋     | 330/713 [02:01<02:21,  2.71it/s]Evaluating:  46%|████▋     | 331/713 [02:02<02:20,  2.72it/s]Evaluating:  47%|████▋     | 332/713 [02:02<02:19,  2.73it/s]Evaluating:  47%|████▋     | 333/713 [02:02<02:18,  2.74it/s]Evaluating:  47%|████▋     | 334/713 [02:03<02:18,  2.74it/s]Evaluating:  47%|████▋     | 335/713 [02:03<02:18,  2.73it/s]Evaluating:  47%|████▋     | 336/713 [02:03<02:17,  2.74it/s]Evaluating:  47%|████▋     | 337/713 [02:04<02:17,  2.74it/s]Evaluating:  47%|████▋     | 338/713 [02:04<02:17,  2.74it/s]Evaluating:  48%|████▊     | 339/713 [02:04<02:16,  2.73it/s]Evaluating:  48%|████▊     | 340/713 [02:05<02:16,  2.74it/s]Evaluating:  48%|████▊     | 341/713 [02:05<02:16,  2.73it/s]Evaluating:  48%|████▊     | 342/713 [02:06<02:15,  2.74it/s]Evaluating:  48%|████▊     | 343/713 [02:06<02:16,  2.72it/s]Evaluating:  48%|████▊     | 344/713 [02:06<02:15,  2.72it/s]Evaluating:  48%|████▊     | 345/713 [02:07<02:15,  2.72it/s]Evaluating:  49%|████▊     | 346/713 [02:07<02:15,  2.71it/s]Evaluating:  49%|████▊     | 347/713 [02:07<02:14,  2.71it/s]Evaluating:  49%|████▉     | 348/713 [02:08<02:13,  2.73it/s]Evaluating:  49%|████▉     | 349/713 [02:08<02:13,  2.73it/s]Evaluating:  49%|████▉     | 350/713 [02:08<02:13,  2.72it/s]Evaluating:  49%|████▉     | 351/713 [02:09<02:13,  2.72it/s]Evaluating:  49%|████▉     | 352/713 [02:09<02:12,  2.72it/s]Evaluating:  50%|████▉     | 353/713 [02:10<02:12,  2.72it/s]Evaluating:  50%|████▉     | 354/713 [02:10<02:11,  2.72it/s]Evaluating:  50%|████▉     | 355/713 [02:10<02:11,  2.72it/s]Evaluating:  50%|████▉     | 356/713 [02:11<02:10,  2.73it/s]Evaluating:  50%|█████     | 357/713 [02:11<02:11,  2.72it/s]Evaluating:  50%|█████     | 358/713 [02:11<02:10,  2.71it/s]Evaluating:  50%|█████     | 359/713 [02:12<02:10,  2.72it/s]Evaluating:  50%|█████     | 360/713 [02:12<02:09,  2.73it/s]Evaluating:  51%|█████     | 361/713 [02:13<02:09,  2.72it/s]Evaluating:  51%|█████     | 362/713 [02:13<02:08,  2.73it/s]Evaluating:  51%|█████     | 363/713 [02:13<02:08,  2.73it/s]Evaluating:  51%|█████     | 364/713 [02:14<02:08,  2.72it/s]Evaluating:  51%|█████     | 365/713 [02:14<02:08,  2.71it/s]Evaluating:  51%|█████▏    | 366/713 [02:14<02:07,  2.71it/s]Evaluating:  51%|█████▏    | 367/713 [02:15<02:07,  2.72it/s]Evaluating:  52%|█████▏    | 368/713 [02:15<02:07,  2.71it/s]Evaluating:  52%|█████▏    | 369/713 [02:15<02:06,  2.72it/s]Evaluating:  52%|█████▏    | 370/713 [02:16<02:06,  2.72it/s]Evaluating:  52%|█████▏    | 371/713 [02:16<02:06,  2.71it/s]Evaluating:  52%|█████▏    | 372/713 [02:17<02:04,  2.73it/s]Evaluating:  52%|█████▏    | 373/713 [02:17<02:04,  2.73it/s]Evaluating:  52%|█████▏    | 374/713 [02:17<02:04,  2.73it/s]Evaluating:  53%|█████▎    | 375/713 [02:18<02:04,  2.72it/s]Evaluating:  53%|█████▎    | 376/713 [02:18<02:03,  2.72it/s]Evaluating:  53%|█████▎    | 377/713 [02:18<02:03,  2.72it/s]Evaluating:  53%|█████▎    | 378/713 [02:19<02:02,  2.73it/s]Evaluating:  53%|█████▎    | 379/713 [02:19<02:02,  2.72it/s]Evaluating:  53%|█████▎    | 380/713 [02:19<02:02,  2.73it/s]Evaluating:  53%|█████▎    | 381/713 [02:20<02:02,  2.72it/s]Evaluating:  54%|█████▎    | 382/713 [02:20<02:02,  2.71it/s]Evaluating:  54%|█████▎    | 383/713 [02:21<02:01,  2.71it/s]Evaluating:  54%|█████▍    | 384/713 [02:21<02:01,  2.71it/s]Evaluating:  54%|█████▍    | 385/713 [02:21<02:00,  2.72it/s]Evaluating:  54%|█████▍    | 386/713 [02:22<02:00,  2.72it/s]Evaluating:  54%|█████▍    | 387/713 [02:22<01:59,  2.73it/s]Evaluating:  54%|█████▍    | 388/713 [02:22<01:59,  2.72it/s]Evaluating:  55%|█████▍    | 389/713 [02:23<01:59,  2.71it/s]Evaluating:  55%|█████▍    | 390/713 [02:23<01:58,  2.72it/s]Evaluating:  55%|█████▍    | 391/713 [02:24<01:58,  2.72it/s]Evaluating:  55%|█████▍    | 392/713 [02:24<01:58,  2.71it/s]Evaluating:  55%|█████▌    | 393/713 [02:24<01:58,  2.71it/s]Evaluating:  55%|█████▌    | 394/713 [02:25<01:58,  2.70it/s]Evaluating:  55%|█████▌    | 395/713 [02:25<01:57,  2.70it/s]Evaluating:  56%|█████▌    | 396/713 [02:25<01:57,  2.69it/s]Evaluating:  56%|█████▌    | 397/713 [02:26<01:57,  2.69it/s]Evaluating:  56%|█████▌    | 398/713 [02:26<01:56,  2.70it/s]Evaluating:  56%|█████▌    | 399/713 [02:27<01:56,  2.70it/s]Evaluating:  56%|█████▌    | 400/713 [02:27<01:55,  2.70it/s]Evaluating:  56%|█████▌    | 401/713 [02:27<01:55,  2.70it/s]Evaluating:  56%|█████▋    | 402/713 [02:28<01:55,  2.70it/s]Evaluating:  57%|█████▋    | 403/713 [02:28<01:54,  2.71it/s]Evaluating:  57%|█████▋    | 404/713 [02:28<01:53,  2.71it/s]Evaluating:  57%|█████▋    | 405/713 [02:29<01:53,  2.72it/s]Evaluating:  57%|█████▋    | 406/713 [02:29<01:52,  2.72it/s]Evaluating:  57%|█████▋    | 407/713 [02:29<01:52,  2.72it/s]Evaluating:  57%|█████▋    | 408/713 [02:30<01:52,  2.72it/s]Evaluating:  57%|█████▋    | 409/713 [02:30<01:51,  2.72it/s]Evaluating:  58%|█████▊    | 410/713 [02:31<01:51,  2.72it/s]Evaluating:  58%|█████▊    | 411/713 [02:31<01:50,  2.73it/s]Evaluating:  58%|█████▊    | 412/713 [02:31<01:50,  2.72it/s]Evaluating:  58%|█████▊    | 413/713 [02:32<01:50,  2.72it/s]Evaluating:  58%|█████▊    | 414/713 [02:32<01:49,  2.73it/s]Evaluating:  58%|█████▊    | 415/713 [02:32<01:49,  2.73it/s]Evaluating:  58%|█████▊    | 416/713 [02:33<01:49,  2.72it/s]Evaluating:  58%|█████▊    | 417/713 [02:33<01:49,  2.71it/s]Evaluating:  59%|█████▊    | 418/713 [02:34<01:48,  2.72it/s]Evaluating:  59%|█████▉    | 419/713 [02:34<01:47,  2.73it/s]Evaluating:  59%|█████▉    | 420/713 [02:34<01:47,  2.73it/s]Evaluating:  59%|█████▉    | 421/713 [02:35<01:47,  2.73it/s]Evaluating:  59%|█████▉    | 422/713 [02:35<01:46,  2.73it/s]Evaluating:  59%|█████▉    | 423/713 [02:35<01:46,  2.74it/s]Evaluating:  59%|█████▉    | 424/713 [02:36<01:45,  2.74it/s]Evaluating:  60%|█████▉    | 425/713 [02:36<01:45,  2.74it/s]Evaluating:  60%|█████▉    | 426/713 [02:36<01:44,  2.73it/s]Evaluating:  60%|█████▉    | 427/713 [02:37<01:44,  2.73it/s]Evaluating:  60%|██████    | 428/713 [02:37<01:44,  2.73it/s]Evaluating:  60%|██████    | 429/713 [02:38<01:44,  2.72it/s]Evaluating:  60%|██████    | 430/713 [02:38<01:44,  2.71it/s]Evaluating:  60%|██████    | 431/713 [02:38<01:43,  2.71it/s]Evaluating:  61%|██████    | 432/713 [02:39<01:43,  2.72it/s]Evaluating:  61%|██████    | 433/713 [02:39<01:42,  2.72it/s]Evaluating:  61%|██████    | 434/713 [02:39<01:42,  2.72it/s]Evaluating:  61%|██████    | 435/713 [02:40<01:42,  2.71it/s]Evaluating:  61%|██████    | 436/713 [02:40<01:42,  2.71it/s]Evaluating:  61%|██████▏   | 437/713 [02:40<01:41,  2.72it/s]Evaluating:  61%|██████▏   | 438/713 [02:41<01:41,  2.72it/s]Evaluating:  62%|██████▏   | 439/713 [02:41<01:40,  2.72it/s]Evaluating:  62%|██████▏   | 440/713 [02:42<01:40,  2.73it/s]Evaluating:  62%|██████▏   | 441/713 [02:42<01:39,  2.72it/s]Evaluating:  62%|██████▏   | 442/713 [02:42<01:39,  2.72it/s]Evaluating:  62%|██████▏   | 443/713 [02:43<01:39,  2.72it/s]Evaluating:  62%|██████▏   | 444/713 [02:43<01:38,  2.73it/s]Evaluating:  62%|██████▏   | 445/713 [02:43<01:37,  2.74it/s]Evaluating:  63%|██████▎   | 446/713 [02:44<01:37,  2.74it/s]Evaluating:  63%|██████▎   | 447/713 [02:44<01:37,  2.73it/s]Evaluating:  63%|██████▎   | 448/713 [02:45<01:37,  2.73it/s]Evaluating:  63%|██████▎   | 449/713 [02:45<01:36,  2.73it/s]Evaluating:  63%|██████▎   | 450/713 [02:45<01:36,  2.73it/s]Evaluating:  63%|██████▎   | 451/713 [02:46<01:35,  2.73it/s]Evaluating:  63%|██████▎   | 452/713 [02:46<01:35,  2.74it/s]Evaluating:  64%|██████▎   | 453/713 [02:46<01:38,  2.65it/s]Evaluating:  64%|██████▎   | 454/713 [02:47<01:36,  2.68it/s]Evaluating:  64%|██████▍   | 455/713 [02:47<01:35,  2.69it/s]Evaluating:  64%|██████▍   | 456/713 [02:47<01:35,  2.70it/s]Evaluating:  64%|██████▍   | 457/713 [02:48<01:34,  2.70it/s]Evaluating:  64%|██████▍   | 458/713 [02:48<01:34,  2.70it/s]Evaluating:  64%|██████▍   | 459/713 [02:49<01:33,  2.71it/s]Evaluating:  65%|██████▍   | 460/713 [02:49<01:33,  2.71it/s]Evaluating:  65%|██████▍   | 461/713 [02:49<01:33,  2.71it/s]Evaluating:  65%|██████▍   | 462/713 [02:50<01:32,  2.70it/s]Evaluating:  65%|██████▍   | 463/713 [02:50<01:32,  2.69it/s]Evaluating:  65%|██████▌   | 464/713 [02:50<01:32,  2.69it/s]Evaluating:  65%|██████▌   | 465/713 [02:51<01:31,  2.70it/s]Evaluating:  65%|██████▌   | 466/713 [02:51<01:31,  2.71it/s]Evaluating:  65%|██████▌   | 467/713 [02:52<01:30,  2.71it/s]Evaluating:  66%|██████▌   | 468/713 [02:52<01:30,  2.70it/s]Evaluating:  66%|██████▌   | 469/713 [02:52<01:29,  2.71it/s]Evaluating:  66%|██████▌   | 470/713 [02:53<01:29,  2.72it/s]Evaluating:  66%|██████▌   | 471/713 [02:53<01:28,  2.72it/s]Evaluating:  66%|██████▌   | 472/713 [02:53<01:28,  2.72it/s]Evaluating:  66%|██████▋   | 473/713 [02:54<01:28,  2.71it/s]Evaluating:  66%|██████▋   | 474/713 [02:54<01:28,  2.72it/s]Evaluating:  67%|██████▋   | 475/713 [02:54<01:27,  2.72it/s]Evaluating:  67%|██████▋   | 476/713 [02:55<01:27,  2.72it/s]Evaluating:  67%|██████▋   | 477/713 [02:55<01:27,  2.71it/s]Evaluating:  67%|██████▋   | 478/713 [02:56<01:26,  2.71it/s]Evaluating:  67%|██████▋   | 479/713 [02:56<01:26,  2.70it/s]Evaluating:  67%|██████▋   | 480/713 [02:56<01:26,  2.71it/s]Evaluating:  67%|██████▋   | 481/713 [02:57<01:25,  2.71it/s]Evaluating:  68%|██████▊   | 482/713 [02:57<01:25,  2.71it/s]Evaluating:  68%|██████▊   | 483/713 [02:57<01:24,  2.71it/s]Evaluating:  68%|██████▊   | 484/713 [02:58<01:24,  2.72it/s]Evaluating:  68%|██████▊   | 485/713 [02:58<01:23,  2.72it/s]Evaluating:  68%|██████▊   | 486/713 [02:59<01:23,  2.72it/s]Evaluating:  68%|██████▊   | 487/713 [02:59<01:23,  2.72it/s]Evaluating:  68%|██████▊   | 488/713 [02:59<01:22,  2.73it/s]Evaluating:  69%|██████▊   | 489/713 [03:00<01:22,  2.72it/s]Evaluating:  69%|██████▊   | 490/713 [03:00<01:21,  2.73it/s]Evaluating:  69%|██████▉   | 491/713 [03:00<01:21,  2.73it/s]Evaluating:  69%|██████▉   | 492/713 [03:01<01:20,  2.74it/s]Evaluating:  69%|██████▉   | 493/713 [03:01<01:20,  2.73it/s]Evaluating:  69%|██████▉   | 494/713 [03:01<01:20,  2.73it/s]Evaluating:  69%|██████▉   | 495/713 [03:02<01:20,  2.72it/s]Evaluating:  70%|██████▉   | 496/713 [03:02<01:19,  2.72it/s]Evaluating:  70%|██████▉   | 497/713 [03:03<01:19,  2.71it/s]Evaluating:  70%|██████▉   | 498/713 [03:03<01:18,  2.72it/s]Evaluating:  70%|██████▉   | 499/713 [03:03<01:18,  2.72it/s]Evaluating:  70%|███████   | 500/713 [03:04<01:18,  2.72it/s]Evaluating:  70%|███████   | 501/713 [03:04<01:18,  2.72it/s]Evaluating:  70%|███████   | 502/713 [03:04<01:17,  2.72it/s]Evaluating:  71%|███████   | 503/713 [03:05<01:17,  2.71it/s]Evaluating:  71%|███████   | 504/713 [03:05<01:16,  2.72it/s]Evaluating:  71%|███████   | 505/713 [03:06<01:16,  2.72it/s]Evaluating:  71%|███████   | 506/713 [03:06<01:16,  2.72it/s]Evaluating:  71%|███████   | 507/713 [03:06<01:15,  2.72it/s]Evaluating:  71%|███████   | 508/713 [03:07<01:15,  2.71it/s]Evaluating:  71%|███████▏  | 509/713 [03:07<01:15,  2.71it/s]Evaluating:  72%|███████▏  | 510/713 [03:07<01:14,  2.72it/s]Evaluating:  72%|███████▏  | 511/713 [03:08<01:14,  2.73it/s]Evaluating:  72%|███████▏  | 512/713 [03:08<01:13,  2.72it/s]Evaluating:  72%|███████▏  | 513/713 [03:08<01:13,  2.73it/s]Evaluating:  72%|███████▏  | 514/713 [03:09<01:12,  2.73it/s]Evaluating:  72%|███████▏  | 515/713 [03:09<01:12,  2.74it/s]Evaluating:  72%|███████▏  | 516/713 [03:10<01:12,  2.73it/s]Evaluating:  73%|███████▎  | 517/713 [03:10<01:11,  2.73it/s]Evaluating:  73%|███████▎  | 518/713 [03:10<01:11,  2.72it/s]Evaluating:  73%|███████▎  | 519/713 [03:11<01:11,  2.72it/s]Evaluating:  73%|███████▎  | 520/713 [03:11<01:10,  2.74it/s]Evaluating:  73%|███████▎  | 521/713 [03:11<01:10,  2.74it/s]Evaluating:  73%|███████▎  | 522/713 [03:12<01:09,  2.74it/s]Evaluating:  73%|███████▎  | 523/713 [03:12<01:09,  2.73it/s]Evaluating:  73%|███████▎  | 524/713 [03:12<01:09,  2.72it/s]Evaluating:  74%|███████▎  | 525/713 [03:13<01:09,  2.72it/s]Evaluating:  74%|███████▍  | 526/713 [03:13<01:08,  2.73it/s]Evaluating:  74%|███████▍  | 527/713 [03:14<01:08,  2.73it/s]Evaluating:  74%|███████▍  | 528/713 [03:14<01:08,  2.72it/s]Evaluating:  74%|███████▍  | 529/713 [03:14<01:07,  2.72it/s]Evaluating:  74%|███████▍  | 530/713 [03:15<01:07,  2.73it/s]Evaluating:  74%|███████▍  | 531/713 [03:15<01:06,  2.73it/s]Evaluating:  75%|███████▍  | 532/713 [03:15<01:06,  2.74it/s]Evaluating:  75%|███████▍  | 533/713 [03:16<01:05,  2.73it/s]Evaluating:  75%|███████▍  | 534/713 [03:16<01:05,  2.73it/s]Evaluating:  75%|███████▌  | 535/713 [03:17<01:05,  2.73it/s]Evaluating:  75%|███████▌  | 536/713 [03:17<01:04,  2.73it/s]Evaluating:  75%|███████▌  | 537/713 [03:17<01:04,  2.71it/s]Evaluating:  75%|███████▌  | 538/713 [03:18<01:04,  2.71it/s]Evaluating:  76%|███████▌  | 539/713 [03:18<01:03,  2.72it/s]Evaluating:  76%|███████▌  | 540/713 [03:18<01:03,  2.72it/s]Evaluating:  76%|███████▌  | 541/713 [03:19<01:03,  2.72it/s]Evaluating:  76%|███████▌  | 542/713 [03:19<01:02,  2.72it/s]Evaluating:  76%|███████▌  | 543/713 [03:19<01:02,  2.72it/s]Evaluating:  76%|███████▋  | 544/713 [03:20<01:02,  2.72it/s]Evaluating:  76%|███████▋  | 545/713 [03:20<01:01,  2.72it/s]Evaluating:  77%|███████▋  | 546/713 [03:21<01:01,  2.72it/s]Evaluating:  77%|███████▋  | 547/713 [03:21<01:01,  2.72it/s]Evaluating:  77%|███████▋  | 548/713 [03:21<01:00,  2.71it/s]Evaluating:  77%|███████▋  | 549/713 [03:22<01:00,  2.72it/s]Evaluating:  77%|███████▋  | 550/713 [03:22<00:59,  2.73it/s]Evaluating:  77%|███████▋  | 551/713 [03:22<00:59,  2.74it/s]Evaluating:  77%|███████▋  | 552/713 [03:23<00:58,  2.74it/s]Evaluating:  78%|███████▊  | 553/713 [03:23<00:58,  2.74it/s]Evaluating:  78%|███████▊  | 554/713 [03:23<00:58,  2.73it/s]Evaluating:  78%|███████▊  | 555/713 [03:24<00:57,  2.73it/s]Evaluating:  78%|███████▊  | 556/713 [03:24<00:57,  2.73it/s]Evaluating:  78%|███████▊  | 557/713 [03:25<00:57,  2.73it/s]Evaluating:  78%|███████▊  | 558/713 [03:25<00:56,  2.73it/s]Evaluating:  78%|███████▊  | 559/713 [03:25<00:56,  2.74it/s]Evaluating:  79%|███████▊  | 560/713 [03:26<00:55,  2.74it/s]Evaluating:  79%|███████▊  | 561/713 [03:26<00:55,  2.73it/s]Evaluating:  79%|███████▉  | 562/713 [03:26<00:55,  2.72it/s]Evaluating:  79%|███████▉  | 563/713 [03:27<00:55,  2.72it/s]Evaluating:  79%|███████▉  | 564/713 [03:27<00:54,  2.72it/s]Evaluating:  79%|███████▉  | 565/713 [03:28<00:54,  2.73it/s]Evaluating:  79%|███████▉  | 566/713 [03:28<00:53,  2.73it/s]Evaluating:  80%|███████▉  | 567/713 [03:28<00:53,  2.74it/s]Evaluating:  80%|███████▉  | 568/713 [03:29<00:52,  2.74it/s]Evaluating:  80%|███████▉  | 569/713 [03:29<00:52,  2.74it/s]Evaluating:  80%|███████▉  | 570/713 [03:29<00:52,  2.74it/s]Evaluating:  80%|████████  | 571/713 [03:30<00:51,  2.74it/s]Evaluating:  80%|████████  | 572/713 [03:30<00:51,  2.74it/s]Evaluating:  80%|████████  | 573/713 [03:30<00:51,  2.73it/s]Evaluating:  81%|████████  | 574/713 [03:31<00:51,  2.71it/s]Evaluating:  81%|████████  | 575/713 [03:31<00:50,  2.72it/s]Evaluating:  81%|████████  | 576/713 [03:32<00:50,  2.71it/s]Evaluating:  81%|████████  | 577/713 [03:32<00:50,  2.70it/s]Evaluating:  81%|████████  | 578/713 [03:32<00:49,  2.70it/s]Evaluating:  81%|████████  | 579/713 [03:33<00:49,  2.71it/s]Evaluating:  81%|████████▏ | 580/713 [03:33<00:49,  2.71it/s]Evaluating:  81%|████████▏ | 581/713 [03:33<00:48,  2.70it/s]Evaluating:  82%|████████▏ | 582/713 [03:34<00:48,  2.69it/s]Evaluating:  82%|████████▏ | 583/713 [03:34<00:48,  2.69it/s]Evaluating:  82%|████████▏ | 584/713 [03:35<00:47,  2.69it/s]Evaluating:  82%|████████▏ | 585/713 [03:35<00:47,  2.71it/s]Evaluating:  82%|████████▏ | 586/713 [03:35<00:47,  2.69it/s]Evaluating:  82%|████████▏ | 587/713 [03:36<00:46,  2.69it/s]Evaluating:  82%|████████▏ | 588/713 [03:36<00:46,  2.70it/s]Evaluating:  83%|████████▎ | 589/713 [03:36<00:46,  2.69it/s]Evaluating:  83%|████████▎ | 590/713 [03:37<00:45,  2.69it/s]Evaluating:  83%|████████▎ | 591/713 [03:37<00:45,  2.68it/s]Evaluating:  83%|████████▎ | 592/713 [03:38<00:45,  2.68it/s]Evaluating:  83%|████████▎ | 593/713 [03:38<00:44,  2.69it/s]Evaluating:  83%|████████▎ | 594/713 [03:38<00:44,  2.69it/s]Evaluating:  83%|████████▎ | 595/713 [03:39<00:43,  2.68it/s]Evaluating:  84%|████████▎ | 596/713 [03:39<00:43,  2.69it/s]Evaluating:  84%|████████▎ | 597/713 [03:39<00:43,  2.69it/s]Evaluating:  84%|████████▍ | 598/713 [03:40<00:42,  2.70it/s]Evaluating:  84%|████████▍ | 599/713 [03:40<00:42,  2.71it/s]Evaluating:  84%|████████▍ | 600/713 [03:40<00:41,  2.71it/s]Evaluating:  84%|████████▍ | 601/713 [03:41<00:41,  2.71it/s]Evaluating:  84%|████████▍ | 602/713 [03:41<00:40,  2.71it/s]Evaluating:  85%|████████▍ | 603/713 [03:42<00:40,  2.71it/s]Evaluating:  85%|████████▍ | 604/713 [03:42<00:40,  2.70it/s]Evaluating:  85%|████████▍ | 605/713 [03:42<00:40,  2.70it/s]Evaluating:  85%|████████▍ | 606/713 [03:43<00:39,  2.69it/s]Evaluating:  85%|████████▌ | 607/713 [03:43<00:39,  2.69it/s]Evaluating:  85%|████████▌ | 608/713 [03:43<00:38,  2.71it/s]Evaluating:  85%|████████▌ | 609/713 [03:44<00:38,  2.70it/s]Evaluating:  86%|████████▌ | 610/713 [03:44<00:37,  2.72it/s]Evaluating:  86%|████████▌ | 611/713 [03:45<00:37,  2.71it/s]Evaluating:  86%|████████▌ | 612/713 [03:45<00:37,  2.70it/s]Evaluating:  86%|████████▌ | 613/713 [03:45<00:36,  2.71it/s]Evaluating:  86%|████████▌ | 614/713 [03:46<00:36,  2.71it/s]Evaluating:  86%|████████▋ | 615/713 [03:46<00:36,  2.72it/s]Evaluating:  86%|████████▋ | 616/713 [03:46<00:35,  2.71it/s]Evaluating:  87%|████████▋ | 617/713 [03:47<00:35,  2.73it/s]Evaluating:  87%|████████▋ | 618/713 [03:47<00:34,  2.73it/s]Evaluating:  87%|████████▋ | 619/713 [03:47<00:34,  2.73it/s]Evaluating:  87%|████████▋ | 620/713 [03:48<00:34,  2.72it/s]Evaluating:  87%|████████▋ | 621/713 [03:48<00:33,  2.72it/s]Evaluating:  87%|████████▋ | 622/713 [03:49<00:33,  2.71it/s]Evaluating:  87%|████████▋ | 623/713 [03:49<00:33,  2.70it/s]Evaluating:  88%|████████▊ | 624/713 [03:49<00:32,  2.70it/s]Evaluating:  88%|████████▊ | 625/713 [03:50<00:32,  2.70it/s]Evaluating:  88%|████████▊ | 626/713 [03:50<00:32,  2.71it/s]Evaluating:  88%|████████▊ | 627/713 [03:50<00:31,  2.71it/s]Evaluating:  88%|████████▊ | 628/713 [03:51<00:31,  2.70it/s]Evaluating:  88%|████████▊ | 629/713 [03:51<00:31,  2.70it/s]Evaluating:  88%|████████▊ | 630/713 [03:52<00:30,  2.70it/s]Evaluating:  88%|████████▊ | 631/713 [03:52<00:30,  2.71it/s]Evaluating:  89%|████████▊ | 632/713 [03:52<00:29,  2.72it/s]Evaluating:  89%|████████▉ | 633/713 [03:53<00:29,  2.71it/s]Evaluating:  89%|████████▉ | 634/713 [03:53<00:29,  2.72it/s]Evaluating:  89%|████████▉ | 635/713 [03:53<00:28,  2.72it/s]Evaluating:  89%|████████▉ | 636/713 [03:54<00:28,  2.72it/s]Evaluating:  89%|████████▉ | 637/713 [03:54<00:28,  2.71it/s]Evaluating:  89%|████████▉ | 638/713 [03:54<00:27,  2.72it/s]Evaluating:  90%|████████▉ | 639/713 [03:55<00:27,  2.72it/s]Evaluating:  90%|████████▉ | 640/713 [03:55<00:26,  2.72it/s]Evaluating:  90%|████████▉ | 641/713 [03:56<00:26,  2.71it/s]Evaluating:  90%|█████████ | 642/713 [03:56<00:26,  2.72it/s]Evaluating:  90%|█████████ | 643/713 [03:56<00:25,  2.72it/s]Evaluating:  90%|█████████ | 644/713 [03:57<00:25,  2.72it/s]Evaluating:  90%|█████████ | 645/713 [03:57<00:24,  2.73it/s]Evaluating:  91%|█████████ | 646/713 [03:57<00:24,  2.73it/s]Evaluating:  91%|█████████ | 647/713 [03:58<00:24,  2.73it/s]Evaluating:  91%|█████████ | 648/713 [03:58<00:23,  2.74it/s]Evaluating:  91%|█████████ | 649/713 [03:59<00:23,  2.74it/s]Evaluating:  91%|█████████ | 650/713 [03:59<00:23,  2.72it/s]Evaluating:  91%|█████████▏| 651/713 [03:59<00:22,  2.73it/s]Evaluating:  91%|█████████▏| 652/713 [04:00<00:22,  2.73it/s]Evaluating:  92%|█████████▏| 653/713 [04:00<00:21,  2.73it/s]Evaluating:  92%|█████████▏| 654/713 [04:00<00:21,  2.73it/s]Evaluating:  92%|█████████▏| 655/713 [04:01<00:21,  2.73it/s]Evaluating:  92%|█████████▏| 656/713 [04:01<00:20,  2.72it/s]Evaluating:  92%|█████████▏| 657/713 [04:01<00:20,  2.72it/s]Evaluating:  92%|█████████▏| 658/713 [04:02<00:20,  2.72it/s]Evaluating:  92%|█████████▏| 659/713 [04:02<00:19,  2.72it/s]Evaluating:  93%|█████████▎| 660/713 [04:03<00:19,  2.72it/s]Evaluating:  93%|█████████▎| 661/713 [04:03<00:19,  2.72it/s]Evaluating:  93%|█████████▎| 662/713 [04:03<00:18,  2.71it/s]Evaluating:  93%|█████████▎| 663/713 [04:04<00:18,  2.72it/s]Evaluating:  93%|█████████▎| 664/713 [04:04<00:18,  2.72it/s]Evaluating:  93%|█████████▎| 665/713 [04:04<00:17,  2.72it/s]Evaluating:  93%|█████████▎| 666/713 [04:05<00:17,  2.72it/s]Evaluating:  94%|█████████▎| 667/713 [04:05<00:16,  2.73it/s]Evaluating:  94%|█████████▎| 668/713 [04:05<00:16,  2.73it/s]Evaluating:  94%|█████████▍| 669/713 [04:06<00:16,  2.65it/s]Evaluating:  94%|█████████▍| 670/713 [04:06<00:16,  2.68it/s]Evaluating:  94%|█████████▍| 671/713 [04:07<00:15,  2.69it/s]Evaluating:  94%|█████████▍| 672/713 [04:07<00:15,  2.69it/s]Evaluating:  94%|█████████▍| 673/713 [04:07<00:14,  2.70it/s]Evaluating:  95%|█████████▍| 674/713 [04:08<00:14,  2.70it/s]Evaluating:  95%|█████████▍| 675/713 [04:08<00:14,  2.71it/s]Evaluating:  95%|█████████▍| 676/713 [04:08<00:13,  2.70it/s]Evaluating:  95%|█████████▍| 677/713 [04:09<00:13,  2.71it/s]Evaluating:  95%|█████████▌| 678/713 [04:09<00:12,  2.71it/s]Evaluating:  95%|█████████▌| 679/713 [04:10<00:12,  2.72it/s]Evaluating:  95%|█████████▌| 680/713 [04:10<00:12,  2.72it/s]Evaluating:  96%|█████████▌| 681/713 [04:10<00:11,  2.73it/s]Evaluating:  96%|█████████▌| 682/713 [04:11<00:11,  2.73it/s]Evaluating:  96%|█████████▌| 683/713 [04:11<00:10,  2.73it/s]Evaluating:  96%|█████████▌| 684/713 [04:11<00:10,  2.73it/s]Evaluating:  96%|█████████▌| 685/713 [04:12<00:10,  2.73it/s]Evaluating:  96%|█████████▌| 686/713 [04:12<00:09,  2.72it/s]Evaluating:  96%|█████████▋| 687/713 [04:13<00:09,  2.72it/s]Evaluating:  96%|█████████▋| 688/713 [04:13<00:09,  2.72it/s]Evaluating:  97%|█████████▋| 689/713 [04:13<00:08,  2.72it/s]Evaluating:  97%|█████████▋| 690/713 [04:14<00:08,  2.72it/s]Evaluating:  97%|█████████▋| 691/713 [04:14<00:08,  2.73it/s]Evaluating:  97%|█████████▋| 692/713 [04:14<00:07,  2.73it/s]Evaluating:  97%|█████████▋| 693/713 [04:15<00:07,  2.73it/s]Evaluating:  97%|█████████▋| 694/713 [04:15<00:06,  2.72it/s]Evaluating:  97%|█████████▋| 695/713 [04:15<00:06,  2.73it/s]Evaluating:  98%|█████████▊| 696/713 [04:16<00:06,  2.74it/s]Evaluating:  98%|█████████▊| 697/713 [04:16<00:05,  2.73it/s]Evaluating:  98%|█████████▊| 698/713 [04:17<00:05,  2.72it/s]Evaluating:  98%|█████████▊| 699/713 [04:17<00:05,  2.71it/s]Evaluating:  98%|█████████▊| 700/713 [04:17<00:04,  2.71it/s]Evaluating:  98%|█████████▊| 701/713 [04:18<00:04,  2.72it/s]Evaluating:  98%|█████████▊| 702/713 [04:18<00:04,  2.73it/s]Evaluating:  99%|█████████▊| 703/713 [04:18<00:03,  2.72it/s]Evaluating:  99%|█████████▊| 704/713 [04:19<00:03,  2.73it/s]Evaluating:  99%|█████████▉| 705/713 [04:19<00:02,  2.72it/s]Evaluating:  99%|█████████▉| 706/713 [04:19<00:02,  2.72it/s]Evaluating:  99%|█████████▉| 707/713 [04:20<00:02,  2.72it/s]Evaluating:  99%|█████████▉| 708/713 [04:20<00:01,  2.71it/s]Evaluating:  99%|█████████▉| 709/713 [04:21<00:01,  2.72it/s]Evaluating: 100%|█████████▉| 710/713 [04:21<00:01,  2.72it/s]Evaluating: 100%|█████████▉| 711/713 [04:21<00:00,  2.72it/s]Evaluating: 100%|█████████▉| 712/713 [04:22<00:00,  2.72it/s]Evaluating: 100%|██████████| 713/713 [04:22<00:00,  2.72it/s]
05/10/2022 00:15:20 - INFO - __main__ -     Evaluation done in total 262.258657 secs (0.046035 sec per example)
05/10/2022 00:15:41 - INFO - __main__ -   Results: {'exact': 51.89284923621873, 'f1': 67.77179122991174, 'total': 4517, 'HasAns_exact': 51.89284923621873, 'HasAns_f1': 67.77179122991174, 'HasAns_total': 4517, 'best_exact': 51.89284923621873, 'best_exact_thresh': 0.0, 'best_f1': 67.77179122991174, 'best_f1_thresh': 0.0}
  ar 
2022-05-10 00:15:44.282801: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/10/2022 00:15:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'qa_outputs.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'qa_outputs.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.attention.self.value.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.11.attention.output.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.22.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:16:10 - INFO - __main__ -   lang2id = None
05/10/2022 00:16:13 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='ar', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-ar-question-ar.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/10/2022 00:16:13 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/10/2022 00:16:13 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'qa_outputs.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'qa_outputs.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.attention.self.value.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.11.attention.output.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.22.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:16:41 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/2389 [00:00<?, ?it/s]  2%|▏         | 58/2389 [00:00<00:04, 556.90it/s]  5%|▍         | 114/2389 [00:00<00:04, 550.11it/s]  7%|▋         | 170/2389 [00:00<00:04, 553.96it/s]  9%|▉         | 226/2389 [00:00<00:04, 539.98it/s] 13%|█▎        | 318/2389 [00:00<00:03, 670.40it/s] 16%|█▌        | 386/2389 [00:00<00:03, 635.80it/s] 21%|██        | 501/2389 [00:00<00:02, 776.85it/s] 25%|██▍       | 587/2389 [00:00<00:02, 801.01it/s] 29%|██▉       | 695/2389 [00:00<00:02, 830.93it/s] 33%|███▎      | 779/2389 [00:01<00:02, 718.72it/s] 36%|███▌      | 857/2389 [00:01<00:02, 722.99it/s] 40%|████      | 957/2389 [00:01<00:01, 796.77it/s] 46%|████▌     | 1101/2389 [00:01<00:01, 974.35it/s] 50%|█████     | 1202/2389 [00:01<00:01, 904.30it/s] 55%|█████▍    | 1304/2389 [00:01<00:01, 934.46it/s] 59%|█████▉    | 1412/2389 [00:01<00:01, 973.18it/s] 63%|██████▎   | 1512/2389 [00:01<00:00, 899.72it/s] 67%|██████▋   | 1605/2389 [00:01<00:00, 894.65it/s] 73%|███████▎  | 1748/2389 [00:02<00:00, 1041.61it/s] 78%|███████▊  | 1855/2389 [00:02<00:00, 826.00it/s]  81%|████████▏ | 1946/2389 [00:02<00:00, 833.73it/s] 90%|████████▉ | 2141/2389 [00:02<00:00, 1113.45it/s]100%|██████████| 2389/2389 [00:02<00:00, 923.96it/s] 
convert squad examples to features:   0%|          | 0/5335 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/5335 [00:00<15:59,  5.56it/s]convert squad examples to features:   1%|          | 33/5335 [00:00<00:48, 108.50it/s]convert squad examples to features:   1%|          | 65/5335 [00:00<00:35, 149.82it/s]convert squad examples to features:   2%|▏         | 97/5335 [00:01<00:58, 89.62it/s] convert squad examples to features:   2%|▏         | 129/5335 [00:01<00:47, 110.51it/s]convert squad examples to features:   3%|▎         | 161/5335 [00:01<00:44, 117.31it/s]convert squad examples to features:   4%|▎         | 193/5335 [00:01<00:43, 119.50it/s]convert squad examples to features:   4%|▍         | 225/5335 [00:02<00:48, 106.26it/s]convert squad examples to features:   5%|▍         | 257/5335 [00:02<00:49, 102.14it/s]convert squad examples to features:   5%|▌         | 289/5335 [00:02<00:53, 93.60it/s] convert squad examples to features:   6%|▌         | 321/5335 [00:03<00:46, 108.75it/s]convert squad examples to features:   7%|▋         | 353/5335 [00:03<00:45, 109.88it/s]convert squad examples to features:   7%|▋         | 385/5335 [00:03<00:44, 112.28it/s]convert squad examples to features:   8%|▊         | 417/5335 [00:03<00:48, 102.24it/s]convert squad examples to features:   8%|▊         | 449/5335 [00:04<00:39, 123.05it/s]convert squad examples to features:   9%|▉         | 481/5335 [00:04<00:35, 138.11it/s]convert squad examples to features:  10%|▉         | 513/5335 [00:04<00:44, 108.36it/s]convert squad examples to features:  10%|█         | 545/5335 [00:04<00:36, 133.00it/s]convert squad examples to features:  11%|█         | 577/5335 [00:04<00:32, 144.25it/s]convert squad examples to features:  11%|█▏        | 609/5335 [00:05<00:30, 156.34it/s]convert squad examples to features:  12%|█▏        | 641/5335 [00:05<00:32, 144.68it/s]convert squad examples to features:  13%|█▎        | 673/5335 [00:05<00:28, 163.44it/s]convert squad examples to features:  13%|█▎        | 705/5335 [00:05<00:29, 157.49it/s]convert squad examples to features:  14%|█▍        | 737/5335 [00:06<00:30, 152.64it/s]convert squad examples to features:  14%|█▍        | 769/5335 [00:06<00:27, 164.87it/s]convert squad examples to features:  15%|█▌        | 801/5335 [00:06<00:48, 93.94it/s] convert squad examples to features:  16%|█▌        | 833/5335 [00:07<00:58, 77.30it/s]convert squad examples to features:  16%|█▌        | 865/5335 [00:07<00:48, 91.66it/s]convert squad examples to features:  17%|█▋        | 897/5335 [00:07<00:39, 112.05it/s]convert squad examples to features:  17%|█▋        | 929/5335 [00:07<00:32, 137.15it/s]convert squad examples to features:  18%|█▊        | 961/5335 [00:08<00:31, 136.75it/s]convert squad examples to features:  19%|█▊        | 993/5335 [00:08<00:34, 124.62it/s]convert squad examples to features:  19%|█▉        | 1025/5335 [00:08<00:28, 152.51it/s]convert squad examples to features:  20%|█▉        | 1057/5335 [00:08<00:30, 141.47it/s]convert squad examples to features:  20%|██        | 1089/5335 [00:08<00:26, 162.67it/s]convert squad examples to features:  21%|██        | 1121/5335 [00:09<00:34, 123.43it/s]convert squad examples to features:  22%|██▏       | 1153/5335 [00:09<00:30, 137.10it/s]convert squad examples to features:  22%|██▏       | 1185/5335 [00:09<00:27, 152.45it/s]convert squad examples to features:  23%|██▎       | 1217/5335 [00:09<00:24, 168.83it/s]convert squad examples to features:  23%|██▎       | 1249/5335 [00:10<00:31, 131.47it/s]convert squad examples to features:  24%|██▍       | 1281/5335 [00:10<00:31, 128.97it/s]convert squad examples to features:  25%|██▍       | 1313/5335 [00:10<00:33, 119.38it/s]convert squad examples to features:  25%|██▌       | 1345/5335 [00:10<00:30, 130.11it/s]convert squad examples to features:  26%|██▌       | 1377/5335 [00:11<00:25, 155.23it/s]convert squad examples to features:  26%|██▋       | 1409/5335 [00:11<00:25, 152.40it/s]convert squad examples to features:  27%|██▋       | 1441/5335 [00:11<00:24, 157.70it/s]convert squad examples to features:  28%|██▊       | 1473/5335 [00:11<00:29, 130.26it/s]convert squad examples to features:  29%|██▉       | 1537/5335 [00:11<00:19, 199.56it/s]convert squad examples to features:  29%|██▉       | 1569/5335 [00:12<00:23, 158.11it/s]convert squad examples to features:  30%|███       | 1601/5335 [00:12<00:23, 159.75it/s]convert squad examples to features:  31%|███       | 1633/5335 [00:12<00:28, 130.24it/s]convert squad examples to features:  31%|███       | 1665/5335 [00:12<00:25, 141.73it/s]convert squad examples to features:  32%|███▏      | 1697/5335 [00:13<00:25, 144.71it/s]convert squad examples to features:  32%|███▏      | 1729/5335 [00:13<00:23, 153.97it/s]convert squad examples to features:  33%|███▎      | 1761/5335 [00:13<00:22, 159.46it/s]convert squad examples to features:  34%|███▎      | 1793/5335 [00:13<00:20, 169.06it/s]convert squad examples to features:  34%|███▍      | 1825/5335 [00:13<00:18, 191.06it/s]convert squad examples to features:  35%|███▍      | 1857/5335 [00:13<00:17, 195.78it/s]convert squad examples to features:  35%|███▌      | 1889/5335 [00:14<00:20, 168.15it/s]convert squad examples to features:  36%|███▌      | 1921/5335 [00:14<00:27, 122.77it/s]convert squad examples to features:  37%|███▋      | 1953/5335 [00:14<00:25, 133.25it/s]convert squad examples to features:  37%|███▋      | 1985/5335 [00:15<00:22, 150.06it/s]convert squad examples to features:  38%|███▊      | 2017/5335 [00:15<00:21, 155.24it/s]convert squad examples to features:  38%|███▊      | 2049/5335 [00:15<00:19, 169.39it/s]convert squad examples to features:  39%|███▉      | 2081/5335 [00:15<00:24, 134.37it/s]convert squad examples to features:  40%|███▉      | 2113/5335 [00:15<00:20, 154.76it/s]convert squad examples to features:  40%|████      | 2145/5335 [00:16<00:24, 129.64it/s]convert squad examples to features:  41%|████      | 2177/5335 [00:16<00:24, 129.17it/s]convert squad examples to features:  41%|████▏     | 2209/5335 [00:16<00:23, 134.65it/s]convert squad examples to features:  43%|████▎     | 2273/5335 [00:16<00:18, 166.31it/s]convert squad examples to features:  43%|████▎     | 2305/5335 [00:17<00:16, 187.65it/s]convert squad examples to features:  44%|████▍     | 2337/5335 [00:17<00:24, 124.39it/s]convert squad examples to features:  44%|████▍     | 2369/5335 [00:17<00:22, 130.71it/s]convert squad examples to features:  45%|████▌     | 2401/5335 [00:17<00:21, 138.42it/s]convert squad examples to features:  46%|████▌     | 2433/5335 [00:18<00:19, 147.89it/s]convert squad examples to features:  46%|████▌     | 2465/5335 [00:18<00:19, 146.96it/s]convert squad examples to features:  47%|████▋     | 2529/5335 [00:18<00:16, 175.33it/s]convert squad examples to features:  48%|████▊     | 2561/5335 [00:18<00:14, 189.12it/s]convert squad examples to features:  49%|████▉     | 2625/5335 [00:19<00:13, 205.89it/s]convert squad examples to features:  50%|████▉     | 2657/5335 [00:19<00:15, 169.67it/s]convert squad examples to features:  51%|█████     | 2721/5335 [00:19<00:14, 181.68it/s]convert squad examples to features:  52%|█████▏    | 2753/5335 [00:19<00:13, 189.47it/s]convert squad examples to features:  53%|█████▎    | 2817/5335 [00:20<00:12, 197.77it/s]convert squad examples to features:  53%|█████▎    | 2849/5335 [00:20<00:14, 174.79it/s]convert squad examples to features:  54%|█████▍    | 2881/5335 [00:20<00:14, 170.54it/s]convert squad examples to features:  55%|█████▍    | 2913/5335 [00:20<00:15, 155.12it/s]convert squad examples to features:  55%|█████▌    | 2945/5335 [00:21<00:16, 144.01it/s]convert squad examples to features:  56%|█████▌    | 2977/5335 [00:21<00:14, 163.64it/s]convert squad examples to features:  56%|█████▋    | 3009/5335 [00:21<00:17, 133.31it/s]convert squad examples to features:  57%|█████▋    | 3041/5335 [00:21<00:17, 131.68it/s]convert squad examples to features:  58%|█████▊    | 3073/5335 [00:21<00:16, 138.94it/s]convert squad examples to features:  58%|█████▊    | 3105/5335 [00:22<00:16, 131.43it/s]convert squad examples to features:  59%|█████▉    | 3137/5335 [00:22<00:16, 135.45it/s]convert squad examples to features:  59%|█████▉    | 3169/5335 [00:22<00:17, 125.49it/s]convert squad examples to features:  60%|██████    | 3201/5335 [00:23<00:23, 92.08it/s] convert squad examples to features:  61%|██████    | 3233/5335 [00:23<00:18, 113.65it/s]convert squad examples to features:  61%|██████    | 3265/5335 [00:23<00:19, 105.22it/s]convert squad examples to features:  62%|██████▏   | 3297/5335 [00:24<00:20, 101.44it/s]convert squad examples to features:  62%|██████▏   | 3329/5335 [00:24<00:18, 106.58it/s]convert squad examples to features:  63%|██████▎   | 3361/5335 [00:24<00:16, 118.35it/s]convert squad examples to features:  64%|██████▎   | 3393/5335 [00:24<00:15, 123.50it/s]convert squad examples to features:  64%|██████▍   | 3425/5335 [00:25<00:15, 121.24it/s]convert squad examples to features:  65%|██████▍   | 3457/5335 [00:25<00:20, 92.19it/s] convert squad examples to features:  65%|██████▌   | 3489/5335 [00:26<00:19, 93.05it/s]convert squad examples to features:  66%|██████▌   | 3521/5335 [00:26<00:19, 92.40it/s]convert squad examples to features:  67%|██████▋   | 3553/5335 [00:26<00:16, 108.74it/s]convert squad examples to features:  67%|██████▋   | 3585/5335 [00:26<00:17, 99.16it/s] convert squad examples to features:  68%|██████▊   | 3617/5335 [00:27<00:15, 111.49it/s]convert squad examples to features:  68%|██████▊   | 3649/5335 [00:27<00:14, 116.97it/s]convert squad examples to features:  69%|██████▉   | 3681/5335 [00:27<00:15, 106.19it/s]convert squad examples to features:  70%|██████▉   | 3713/5335 [00:27<00:13, 124.62it/s]convert squad examples to features:  70%|███████   | 3745/5335 [00:28<00:13, 119.93it/s]convert squad examples to features:  71%|███████   | 3777/5335 [00:28<00:11, 134.45it/s]convert squad examples to features:  71%|███████▏  | 3809/5335 [00:28<00:11, 135.45it/s]convert squad examples to features:  72%|███████▏  | 3841/5335 [00:28<00:13, 113.89it/s]convert squad examples to features:  73%|███████▎  | 3905/5335 [00:29<00:09, 151.40it/s]convert squad examples to features:  74%|███████▍  | 3937/5335 [00:29<00:10, 135.45it/s]convert squad examples to features:  74%|███████▍  | 3969/5335 [00:29<00:09, 146.40it/s]convert squad examples to features:  75%|███████▍  | 4001/5335 [00:29<00:08, 155.49it/s]convert squad examples to features:  76%|███████▌  | 4033/5335 [00:30<00:08, 156.99it/s]convert squad examples to features:  76%|███████▌  | 4065/5335 [00:30<00:16, 79.22it/s] convert squad examples to features:  77%|███████▋  | 4097/5335 [00:31<00:13, 92.87it/s]convert squad examples to features:  77%|███████▋  | 4129/5335 [00:31<00:11, 105.63it/s]convert squad examples to features:  78%|███████▊  | 4161/5335 [00:31<00:09, 123.34it/s]convert squad examples to features:  79%|███████▊  | 4193/5335 [00:31<00:09, 125.65it/s]convert squad examples to features:  79%|███████▉  | 4225/5335 [00:32<00:08, 127.86it/s]convert squad examples to features:  80%|███████▉  | 4257/5335 [00:32<00:08, 132.93it/s]convert squad examples to features:  80%|████████  | 4289/5335 [00:32<00:06, 151.31it/s]convert squad examples to features:  81%|████████  | 4321/5335 [00:32<00:06, 150.65it/s]convert squad examples to features:  82%|████████▏ | 4353/5335 [00:33<00:11, 84.89it/s] convert squad examples to features:  82%|████████▏ | 4385/5335 [00:33<00:09, 100.44it/s]convert squad examples to features:  83%|████████▎ | 4417/5335 [00:33<00:09, 97.13it/s] convert squad examples to features:  83%|████████▎ | 4449/5335 [00:34<00:08, 105.59it/s]convert squad examples to features:  84%|████████▍ | 4481/5335 [00:34<00:08, 104.97it/s]convert squad examples to features:  85%|████████▍ | 4513/5335 [00:34<00:07, 116.25it/s]convert squad examples to features:  85%|████████▌ | 4545/5335 [00:35<00:07, 103.65it/s]convert squad examples to features:  86%|████████▌ | 4577/5335 [00:35<00:07, 96.19it/s] convert squad examples to features:  86%|████████▋ | 4609/5335 [00:35<00:08, 84.28it/s]convert squad examples to features:  87%|████████▋ | 4641/5335 [00:36<00:07, 97.39it/s]convert squad examples to features:  88%|████████▊ | 4673/5335 [00:36<00:05, 111.72it/s]convert squad examples to features:  88%|████████▊ | 4705/5335 [00:36<00:05, 124.78it/s]convert squad examples to features:  89%|████████▉ | 4737/5335 [00:36<00:05, 107.28it/s]convert squad examples to features:  89%|████████▉ | 4769/5335 [00:37<00:05, 102.34it/s]convert squad examples to features:  90%|████████▉ | 4801/5335 [00:37<00:04, 121.12it/s]convert squad examples to features:  91%|█████████ | 4833/5335 [00:37<00:03, 142.39it/s]convert squad examples to features:  91%|█████████ | 4865/5335 [00:37<00:03, 124.08it/s]convert squad examples to features:  92%|█████████▏| 4897/5335 [00:38<00:03, 138.58it/s]convert squad examples to features:  92%|█████████▏| 4929/5335 [00:38<00:02, 163.87it/s]convert squad examples to features:  93%|█████████▎| 4961/5335 [00:38<00:01, 190.19it/s]convert squad examples to features:  94%|█████████▎| 4993/5335 [00:38<00:01, 172.39it/s]convert squad examples to features:  94%|█████████▍| 5025/5335 [00:38<00:01, 174.09it/s]convert squad examples to features:  95%|█████████▍| 5057/5335 [00:38<00:01, 193.08it/s]convert squad examples to features:  95%|█████████▌| 5089/5335 [00:38<00:01, 202.02it/s]convert squad examples to features:  96%|█████████▌| 5121/5335 [00:39<00:01, 175.37it/s]convert squad examples to features:  97%|█████████▋| 5185/5335 [00:39<00:00, 222.12it/s]convert squad examples to features:  98%|█████████▊| 5217/5335 [00:39<00:00, 236.13it/s]convert squad examples to features:  98%|█████████▊| 5249/5335 [00:39<00:00, 233.97it/s]convert squad examples to features:  99%|█████████▉| 5281/5335 [00:39<00:00, 245.49it/s]convert squad examples to features: 100%|██████████| 5335/5335 [00:39<00:00, 134.12it/s]/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/xfactr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2272: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/5335 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 5335/5335 [00:00<00:00, 176719.78it/s]
05/10/2022 00:17:24 - INFO - __main__ -   Saving features into cached file ./cached_test-context-ar-question-ar.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_ar
05/10/2022 00:17:34 - INFO - __main__ -   ***** Running evaluation  *****
05/10/2022 00:17:34 - INFO - __main__ -     Num examples = 7566
05/10/2022 00:17:34 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/946 [00:00<?, ?it/s]Evaluating:   0%|          | 1/946 [00:01<16:00,  1.02s/it]Evaluating:   0%|          | 2/946 [00:01<09:55,  1.59it/s]Evaluating:   0%|          | 3/946 [00:01<07:58,  1.97it/s]Evaluating:   0%|          | 4/946 [00:02<07:03,  2.23it/s]Evaluating:   1%|          | 5/946 [00:02<06:34,  2.39it/s]Evaluating:   1%|          | 6/946 [00:02<06:16,  2.50it/s]Evaluating:   1%|          | 7/946 [00:03<06:03,  2.58it/s]Evaluating:   1%|          | 8/946 [00:03<05:55,  2.64it/s]Evaluating:   1%|          | 9/946 [00:03<05:49,  2.68it/s]Evaluating:   1%|          | 10/946 [00:04<05:45,  2.71it/s]Evaluating:   1%|          | 11/946 [00:04<05:43,  2.72it/s]Evaluating:   1%|▏         | 12/946 [00:04<05:42,  2.73it/s]Evaluating:   1%|▏         | 13/946 [00:05<05:42,  2.73it/s]Evaluating:   1%|▏         | 14/946 [00:05<05:41,  2.73it/s]Evaluating:   2%|▏         | 15/946 [00:06<05:43,  2.71it/s]Evaluating:   2%|▏         | 16/946 [00:06<05:42,  2.72it/s]Evaluating:   2%|▏         | 17/946 [00:06<05:40,  2.73it/s]Evaluating:   2%|▏         | 18/946 [00:07<05:41,  2.72it/s]Evaluating:   2%|▏         | 19/946 [00:07<05:42,  2.71it/s]Evaluating:   2%|▏         | 20/946 [00:07<05:40,  2.72it/s]Evaluating:   2%|▏         | 21/946 [00:08<05:40,  2.72it/s]Evaluating:   2%|▏         | 22/946 [00:08<05:39,  2.72it/s]Evaluating:   2%|▏         | 23/946 [00:09<05:39,  2.72it/s]Evaluating:   3%|▎         | 24/946 [00:09<05:37,  2.73it/s]Evaluating:   3%|▎         | 25/946 [00:09<05:37,  2.73it/s]Evaluating:   3%|▎         | 26/946 [00:10<05:35,  2.74it/s]Evaluating:   3%|▎         | 27/946 [00:10<05:35,  2.74it/s]Evaluating:   3%|▎         | 28/946 [00:10<05:35,  2.74it/s]Evaluating:   3%|▎         | 29/946 [00:11<05:34,  2.74it/s]Evaluating:   3%|▎         | 30/946 [00:11<05:33,  2.74it/s]Evaluating:   3%|▎         | 31/946 [00:11<05:35,  2.73it/s]Evaluating:   3%|▎         | 32/946 [00:12<05:36,  2.71it/s]Evaluating:   3%|▎         | 33/946 [00:12<05:36,  2.71it/s]Evaluating:   4%|▎         | 34/946 [00:13<05:36,  2.71it/s]Evaluating:   4%|▎         | 35/946 [00:13<05:36,  2.71it/s]Evaluating:   4%|▍         | 36/946 [00:13<05:35,  2.71it/s]Evaluating:   4%|▍         | 37/946 [00:14<05:33,  2.73it/s]Evaluating:   4%|▍         | 38/946 [00:14<05:32,  2.73it/s]Evaluating:   4%|▍         | 39/946 [00:14<05:32,  2.73it/s]Evaluating:   4%|▍         | 40/946 [00:15<05:39,  2.67it/s]Evaluating:   4%|▍         | 41/946 [00:15<05:36,  2.69it/s]Evaluating:   4%|▍         | 42/946 [00:16<05:35,  2.69it/s]Evaluating:   5%|▍         | 43/946 [00:16<05:34,  2.70it/s]Evaluating:   5%|▍         | 44/946 [00:16<05:34,  2.70it/s]Evaluating:   5%|▍         | 45/946 [00:17<05:33,  2.70it/s]Evaluating:   5%|▍         | 46/946 [00:17<05:32,  2.71it/s]Evaluating:   5%|▍         | 47/946 [00:17<05:31,  2.72it/s]Evaluating:   5%|▌         | 48/946 [00:18<05:30,  2.71it/s]Evaluating:   5%|▌         | 49/946 [00:18<05:28,  2.73it/s]Evaluating:   5%|▌         | 50/946 [00:18<05:28,  2.73it/s]Evaluating:   5%|▌         | 51/946 [00:19<05:28,  2.72it/s]Evaluating:   5%|▌         | 52/946 [00:19<05:30,  2.70it/s]Evaluating:   6%|▌         | 53/946 [00:20<05:29,  2.71it/s]Evaluating:   6%|▌         | 54/946 [00:20<05:29,  2.71it/s]Evaluating:   6%|▌         | 55/946 [00:20<05:29,  2.70it/s]Evaluating:   6%|▌         | 56/946 [00:21<05:28,  2.71it/s]Evaluating:   6%|▌         | 57/946 [00:21<05:27,  2.71it/s]Evaluating:   6%|▌         | 58/946 [00:21<05:26,  2.72it/s]Evaluating:   6%|▌         | 59/946 [00:22<05:25,  2.73it/s]Evaluating:   6%|▋         | 60/946 [00:22<05:25,  2.72it/s]Evaluating:   6%|▋         | 61/946 [00:23<05:24,  2.73it/s]Evaluating:   7%|▋         | 62/946 [00:23<05:24,  2.73it/s]Evaluating:   7%|▋         | 63/946 [00:23<05:23,  2.73it/s]Evaluating:   7%|▋         | 64/946 [00:24<05:23,  2.73it/s]Evaluating:   7%|▋         | 65/946 [00:24<05:22,  2.73it/s]Evaluating:   7%|▋         | 66/946 [00:24<05:23,  2.72it/s]Evaluating:   7%|▋         | 67/946 [00:25<05:23,  2.72it/s]Evaluating:   7%|▋         | 68/946 [00:25<05:20,  2.74it/s]Evaluating:   7%|▋         | 69/946 [00:25<05:19,  2.75it/s]Evaluating:   7%|▋         | 70/946 [00:26<05:19,  2.74it/s]Evaluating:   8%|▊         | 71/946 [00:26<05:20,  2.73it/s]Evaluating:   8%|▊         | 72/946 [00:27<05:20,  2.73it/s]Evaluating:   8%|▊         | 73/946 [00:27<05:20,  2.72it/s]Evaluating:   8%|▊         | 74/946 [00:27<05:21,  2.71it/s]Evaluating:   8%|▊         | 75/946 [00:28<05:21,  2.71it/s]Evaluating:   8%|▊         | 76/946 [00:28<05:21,  2.70it/s]Evaluating:   8%|▊         | 77/946 [00:28<05:22,  2.70it/s]Evaluating:   8%|▊         | 78/946 [00:29<05:22,  2.69it/s]Evaluating:   8%|▊         | 79/946 [00:29<05:20,  2.70it/s]Evaluating:   8%|▊         | 80/946 [00:30<05:17,  2.72it/s]Evaluating:   9%|▊         | 81/946 [00:30<05:19,  2.71it/s]Evaluating:   9%|▊         | 82/946 [00:30<05:18,  2.71it/s]Evaluating:   9%|▉         | 83/946 [00:31<05:18,  2.71it/s]Evaluating:   9%|▉         | 84/946 [00:31<05:17,  2.72it/s]Evaluating:   9%|▉         | 85/946 [00:31<05:17,  2.71it/s]Evaluating:   9%|▉         | 86/946 [00:32<05:17,  2.71it/s]Evaluating:   9%|▉         | 87/946 [00:32<05:17,  2.70it/s]Evaluating:   9%|▉         | 88/946 [00:32<05:16,  2.71it/s]Evaluating:   9%|▉         | 89/946 [00:33<05:15,  2.72it/s]Evaluating:  10%|▉         | 90/946 [00:33<05:15,  2.71it/s]Evaluating:  10%|▉         | 91/946 [00:34<05:13,  2.73it/s]Evaluating:  10%|▉         | 92/946 [00:34<05:12,  2.73it/s]Evaluating:  10%|▉         | 93/946 [00:34<05:11,  2.74it/s]Evaluating:  10%|▉         | 94/946 [00:35<05:10,  2.75it/s]Evaluating:  10%|█         | 95/946 [00:35<05:10,  2.74it/s]Evaluating:  10%|█         | 96/946 [00:35<05:11,  2.73it/s]Evaluating:  10%|█         | 97/946 [00:36<05:10,  2.73it/s]Evaluating:  10%|█         | 98/946 [00:36<05:11,  2.72it/s]Evaluating:  10%|█         | 99/946 [00:36<05:10,  2.73it/s]Evaluating:  11%|█         | 100/946 [00:37<05:10,  2.73it/s]Evaluating:  11%|█         | 101/946 [00:37<05:10,  2.72it/s]Evaluating:  11%|█         | 102/946 [00:38<05:09,  2.73it/s]Evaluating:  11%|█         | 103/946 [00:38<05:09,  2.72it/s]Evaluating:  11%|█         | 104/946 [00:38<05:10,  2.71it/s]Evaluating:  11%|█         | 105/946 [00:39<05:09,  2.72it/s]Evaluating:  11%|█         | 106/946 [00:39<05:08,  2.73it/s]Evaluating:  11%|█▏        | 107/946 [00:39<05:07,  2.73it/s]Evaluating:  11%|█▏        | 108/946 [00:40<05:06,  2.73it/s]Evaluating:  12%|█▏        | 109/946 [00:40<05:06,  2.73it/s]Evaluating:  12%|█▏        | 110/946 [00:41<05:07,  2.71it/s]Evaluating:  12%|█▏        | 111/946 [00:41<05:07,  2.72it/s]Evaluating:  12%|█▏        | 112/946 [00:41<05:06,  2.73it/s]Evaluating:  12%|█▏        | 113/946 [00:42<05:04,  2.73it/s]Evaluating:  12%|█▏        | 114/946 [00:42<05:03,  2.74it/s]Evaluating:  12%|█▏        | 115/946 [00:42<05:04,  2.73it/s]Evaluating:  12%|█▏        | 116/946 [00:43<05:06,  2.70it/s]Evaluating:  12%|█▏        | 117/946 [00:43<05:07,  2.70it/s]Evaluating:  12%|█▏        | 118/946 [00:43<05:05,  2.71it/s]Evaluating:  13%|█▎        | 119/946 [00:44<05:04,  2.71it/s]Evaluating:  13%|█▎        | 120/946 [00:44<05:04,  2.72it/s]Evaluating:  13%|█▎        | 121/946 [00:45<05:03,  2.72it/s]Evaluating:  13%|█▎        | 122/946 [00:45<05:02,  2.72it/s]Evaluating:  13%|█▎        | 123/946 [00:45<05:02,  2.72it/s]Evaluating:  13%|█▎        | 124/946 [00:46<05:02,  2.72it/s]Evaluating:  13%|█▎        | 125/946 [00:46<05:02,  2.71it/s]Evaluating:  13%|█▎        | 126/946 [00:46<05:01,  2.72it/s]Evaluating:  13%|█▎        | 127/946 [00:47<04:59,  2.73it/s]Evaluating:  14%|█▎        | 128/946 [00:47<04:59,  2.73it/s]Evaluating:  14%|█▎        | 129/946 [00:48<05:00,  2.72it/s]Evaluating:  14%|█▎        | 130/946 [00:48<05:00,  2.71it/s]Evaluating:  14%|█▍        | 131/946 [00:48<04:59,  2.72it/s]Evaluating:  14%|█▍        | 132/946 [00:49<04:59,  2.72it/s]Evaluating:  14%|█▍        | 133/946 [00:49<04:59,  2.71it/s]Evaluating:  14%|█▍        | 134/946 [00:49<05:00,  2.70it/s]Evaluating:  14%|█▍        | 135/946 [00:50<05:01,  2.69it/s]Evaluating:  14%|█▍        | 136/946 [00:50<05:00,  2.69it/s]Evaluating:  14%|█▍        | 137/946 [00:50<04:59,  2.70it/s]Evaluating:  15%|█▍        | 138/946 [00:51<04:59,  2.70it/s]Evaluating:  15%|█▍        | 139/946 [00:51<04:57,  2.71it/s]Evaluating:  15%|█▍        | 140/946 [00:52<04:56,  2.72it/s]Evaluating:  15%|█▍        | 141/946 [00:52<04:55,  2.72it/s]Evaluating:  15%|█▌        | 142/946 [00:52<04:54,  2.73it/s]Evaluating:  15%|█▌        | 143/946 [00:53<04:53,  2.73it/s]Evaluating:  15%|█▌        | 144/946 [00:53<04:53,  2.73it/s]Evaluating:  15%|█▌        | 145/946 [00:53<04:52,  2.74it/s]Evaluating:  15%|█▌        | 146/946 [00:54<04:51,  2.74it/s]Evaluating:  16%|█▌        | 147/946 [00:54<04:51,  2.74it/s]Evaluating:  16%|█▌        | 148/946 [00:55<04:52,  2.73it/s]Evaluating:  16%|█▌        | 149/946 [00:55<04:53,  2.72it/s]Evaluating:  16%|█▌        | 150/946 [00:55<04:54,  2.70it/s]Evaluating:  16%|█▌        | 151/946 [00:56<04:54,  2.70it/s]Evaluating:  16%|█▌        | 152/946 [00:56<04:52,  2.71it/s]Evaluating:  16%|█▌        | 153/946 [00:56<04:52,  2.71it/s]Evaluating:  16%|█▋        | 154/946 [00:57<04:53,  2.70it/s]Evaluating:  16%|█▋        | 155/946 [00:57<04:53,  2.70it/s]Evaluating:  16%|█▋        | 156/946 [00:57<04:52,  2.71it/s]Evaluating:  17%|█▋        | 157/946 [00:58<04:51,  2.70it/s]Evaluating:  17%|█▋        | 158/946 [00:58<04:51,  2.70it/s]Evaluating:  17%|█▋        | 159/946 [00:59<04:51,  2.70it/s]Evaluating:  17%|█▋        | 160/946 [00:59<04:50,  2.70it/s]Evaluating:  17%|█▋        | 161/946 [00:59<04:50,  2.70it/s]Evaluating:  17%|█▋        | 162/946 [01:00<04:51,  2.69it/s]Evaluating:  17%|█▋        | 163/946 [01:00<04:51,  2.68it/s]Evaluating:  17%|█▋        | 164/946 [01:00<04:51,  2.68it/s]Evaluating:  17%|█▋        | 165/946 [01:01<04:51,  2.68it/s]Evaluating:  18%|█▊        | 166/946 [01:01<04:51,  2.68it/s]Evaluating:  18%|█▊        | 167/946 [01:02<04:50,  2.68it/s]Evaluating:  18%|█▊        | 168/946 [01:02<04:50,  2.68it/s]Evaluating:  18%|█▊        | 169/946 [01:02<04:48,  2.69it/s]Evaluating:  18%|█▊        | 170/946 [01:03<04:46,  2.70it/s]Evaluating:  18%|█▊        | 171/946 [01:03<04:45,  2.71it/s]Evaluating:  18%|█▊        | 172/946 [01:03<04:44,  2.72it/s]Evaluating:  18%|█▊        | 173/946 [01:04<04:44,  2.72it/s]Evaluating:  18%|█▊        | 174/946 [01:04<04:45,  2.70it/s]Evaluating:  18%|█▊        | 175/946 [01:05<04:47,  2.68it/s]Evaluating:  19%|█▊        | 176/946 [01:05<04:44,  2.70it/s]Evaluating:  19%|█▊        | 177/946 [01:05<04:42,  2.72it/s]Evaluating:  19%|█▉        | 178/946 [01:06<04:41,  2.72it/s]Evaluating:  19%|█▉        | 179/946 [01:06<04:40,  2.74it/s]Evaluating:  19%|█▉        | 180/946 [01:06<04:39,  2.74it/s]Evaluating:  19%|█▉        | 181/946 [01:07<04:39,  2.74it/s]Evaluating:  19%|█▉        | 182/946 [01:07<04:38,  2.74it/s]Evaluating:  19%|█▉        | 183/946 [01:07<04:39,  2.73it/s]Evaluating:  19%|█▉        | 184/946 [01:08<04:38,  2.74it/s]Evaluating:  20%|█▉        | 185/946 [01:08<04:38,  2.73it/s]Evaluating:  20%|█▉        | 186/946 [01:09<04:39,  2.72it/s]Evaluating:  20%|█▉        | 187/946 [01:09<04:39,  2.72it/s]Evaluating:  20%|█▉        | 188/946 [01:09<04:39,  2.72it/s]Evaluating:  20%|█▉        | 189/946 [01:10<04:39,  2.71it/s]Evaluating:  20%|██        | 190/946 [01:10<04:40,  2.69it/s]Evaluating:  20%|██        | 191/946 [01:10<04:39,  2.70it/s]Evaluating:  20%|██        | 192/946 [01:11<04:38,  2.71it/s]Evaluating:  20%|██        | 193/946 [01:11<04:37,  2.72it/s]Evaluating:  21%|██        | 194/946 [01:11<04:36,  2.72it/s]Evaluating:  21%|██        | 195/946 [01:12<04:35,  2.72it/s]Evaluating:  21%|██        | 196/946 [01:12<04:35,  2.72it/s]Evaluating:  21%|██        | 197/946 [01:13<04:34,  2.73it/s]Evaluating:  21%|██        | 198/946 [01:13<04:35,  2.71it/s]Evaluating:  21%|██        | 199/946 [01:13<04:35,  2.71it/s]Evaluating:  21%|██        | 200/946 [01:14<04:34,  2.72it/s]Evaluating:  21%|██        | 201/946 [01:14<04:33,  2.73it/s]Evaluating:  21%|██▏       | 202/946 [01:14<04:32,  2.73it/s]Evaluating:  21%|██▏       | 203/946 [01:15<04:32,  2.73it/s]Evaluating:  22%|██▏       | 204/946 [01:15<04:32,  2.72it/s]Evaluating:  22%|██▏       | 205/946 [01:16<04:34,  2.70it/s]Evaluating:  22%|██▏       | 206/946 [01:16<04:33,  2.71it/s]Evaluating:  22%|██▏       | 207/946 [01:16<04:31,  2.72it/s]Evaluating:  22%|██▏       | 208/946 [01:17<04:32,  2.71it/s]Evaluating:  22%|██▏       | 209/946 [01:17<04:31,  2.71it/s]Evaluating:  22%|██▏       | 210/946 [01:17<04:31,  2.71it/s]Evaluating:  22%|██▏       | 211/946 [01:18<04:29,  2.72it/s]Evaluating:  22%|██▏       | 212/946 [01:18<04:29,  2.72it/s]Evaluating:  23%|██▎       | 213/946 [01:18<04:30,  2.71it/s]Evaluating:  23%|██▎       | 214/946 [01:19<04:31,  2.70it/s]Evaluating:  23%|██▎       | 215/946 [01:19<04:30,  2.70it/s]Evaluating:  23%|██▎       | 216/946 [01:20<04:29,  2.70it/s]Evaluating:  23%|██▎       | 217/946 [01:20<04:28,  2.71it/s]Evaluating:  23%|██▎       | 218/946 [01:20<04:29,  2.71it/s]Evaluating:  23%|██▎       | 219/946 [01:21<04:27,  2.72it/s]Evaluating:  23%|██▎       | 220/946 [01:21<04:33,  2.66it/s]Evaluating:  23%|██▎       | 221/946 [01:21<04:31,  2.67it/s]Evaluating:  23%|██▎       | 222/946 [01:22<04:31,  2.67it/s]Evaluating:  24%|██▎       | 223/946 [01:22<04:29,  2.69it/s]Evaluating:  24%|██▎       | 224/946 [01:23<04:27,  2.70it/s]Evaluating:  24%|██▍       | 225/946 [01:23<04:26,  2.71it/s]Evaluating:  24%|██▍       | 226/946 [01:23<04:24,  2.72it/s]Evaluating:  24%|██▍       | 227/946 [01:24<04:24,  2.72it/s]Evaluating:  24%|██▍       | 228/946 [01:24<04:25,  2.71it/s]Evaluating:  24%|██▍       | 229/946 [01:24<04:23,  2.72it/s]Evaluating:  24%|██▍       | 230/946 [01:25<04:22,  2.72it/s]Evaluating:  24%|██▍       | 231/946 [01:25<04:22,  2.72it/s]Evaluating:  25%|██▍       | 232/946 [01:26<04:22,  2.72it/s]Evaluating:  25%|██▍       | 233/946 [01:26<04:20,  2.73it/s]Evaluating:  25%|██▍       | 234/946 [01:26<04:21,  2.72it/s]Evaluating:  25%|██▍       | 235/946 [01:27<04:21,  2.72it/s]Evaluating:  25%|██▍       | 236/946 [01:27<04:21,  2.72it/s]Evaluating:  25%|██▌       | 237/946 [01:27<04:22,  2.71it/s]Evaluating:  25%|██▌       | 238/946 [01:28<04:21,  2.70it/s]Evaluating:  25%|██▌       | 239/946 [01:28<04:21,  2.70it/s]Evaluating:  25%|██▌       | 240/946 [01:28<04:22,  2.69it/s]Evaluating:  25%|██▌       | 241/946 [01:29<04:22,  2.69it/s]Evaluating:  26%|██▌       | 242/946 [01:29<04:22,  2.68it/s]Evaluating:  26%|██▌       | 243/946 [01:30<04:23,  2.66it/s]Evaluating:  26%|██▌       | 244/946 [01:30<04:22,  2.67it/s]Evaluating:  26%|██▌       | 245/946 [01:30<04:21,  2.68it/s]Evaluating:  26%|██▌       | 246/946 [01:31<04:19,  2.69it/s]Evaluating:  26%|██▌       | 247/946 [01:31<04:18,  2.71it/s]Evaluating:  26%|██▌       | 248/946 [01:31<04:16,  2.72it/s]Evaluating:  26%|██▋       | 249/946 [01:32<04:17,  2.71it/s]Evaluating:  26%|██▋       | 250/946 [01:32<04:18,  2.69it/s]Evaluating:  27%|██▋       | 251/946 [01:33<04:17,  2.70it/s]Evaluating:  27%|██▋       | 252/946 [01:33<04:16,  2.71it/s]Evaluating:  27%|██▋       | 253/946 [01:33<04:16,  2.71it/s]Evaluating:  27%|██▋       | 254/946 [01:34<04:16,  2.70it/s]Evaluating:  27%|██▋       | 255/946 [01:34<04:16,  2.70it/s]Evaluating:  27%|██▋       | 256/946 [01:34<04:15,  2.70it/s]Evaluating:  27%|██▋       | 257/946 [01:35<04:13,  2.72it/s]Evaluating:  27%|██▋       | 258/946 [01:35<04:12,  2.72it/s]Evaluating:  27%|██▋       | 259/946 [01:36<04:12,  2.73it/s]Evaluating:  27%|██▋       | 260/946 [01:36<04:11,  2.73it/s]Evaluating:  28%|██▊       | 261/946 [01:36<04:10,  2.74it/s]Evaluating:  28%|██▊       | 262/946 [01:37<04:09,  2.74it/s]Evaluating:  28%|██▊       | 263/946 [01:37<04:09,  2.74it/s]Evaluating:  28%|██▊       | 264/946 [01:37<04:09,  2.74it/s]Evaluating:  28%|██▊       | 265/946 [01:38<04:09,  2.73it/s]Evaluating:  28%|██▊       | 266/946 [01:38<04:10,  2.71it/s]Evaluating:  28%|██▊       | 267/946 [01:38<04:10,  2.71it/s]Evaluating:  28%|██▊       | 268/946 [01:39<04:09,  2.71it/s]Evaluating:  28%|██▊       | 269/946 [01:39<04:09,  2.71it/s]Evaluating:  29%|██▊       | 270/946 [01:40<04:09,  2.71it/s]Evaluating:  29%|██▊       | 271/946 [01:40<04:09,  2.70it/s]Evaluating:  29%|██▉       | 272/946 [01:40<04:08,  2.71it/s]Evaluating:  29%|██▉       | 273/946 [01:41<04:08,  2.70it/s]Evaluating:  29%|██▉       | 274/946 [01:41<04:08,  2.71it/s]Evaluating:  29%|██▉       | 275/946 [01:41<04:08,  2.70it/s]Evaluating:  29%|██▉       | 276/946 [01:42<04:06,  2.71it/s]Evaluating:  29%|██▉       | 277/946 [01:42<04:07,  2.71it/s]Evaluating:  29%|██▉       | 278/946 [01:42<04:05,  2.72it/s]Evaluating:  29%|██▉       | 279/946 [01:43<04:06,  2.71it/s]Evaluating:  30%|██▉       | 280/946 [01:43<04:05,  2.71it/s]Evaluating:  30%|██▉       | 281/946 [01:44<04:04,  2.72it/s]Evaluating:  30%|██▉       | 282/946 [01:44<04:04,  2.72it/s]Evaluating:  30%|██▉       | 283/946 [01:44<04:03,  2.72it/s]Evaluating:  30%|███       | 284/946 [01:45<04:03,  2.72it/s]Evaluating:  30%|███       | 285/946 [01:45<04:02,  2.72it/s]Evaluating:  30%|███       | 286/946 [01:45<04:02,  2.72it/s]Evaluating:  30%|███       | 287/946 [01:46<04:02,  2.72it/s]Evaluating:  30%|███       | 288/946 [01:46<04:03,  2.70it/s]Evaluating:  31%|███       | 289/946 [01:47<04:04,  2.68it/s]Evaluating:  31%|███       | 290/946 [01:47<04:04,  2.69it/s]Evaluating:  31%|███       | 291/946 [01:47<04:04,  2.68it/s]Evaluating:  31%|███       | 292/946 [01:48<04:03,  2.69it/s]Evaluating:  31%|███       | 293/946 [01:48<04:02,  2.70it/s]Evaluating:  31%|███       | 294/946 [01:48<04:02,  2.69it/s]Evaluating:  31%|███       | 295/946 [01:49<04:01,  2.70it/s]Evaluating:  31%|███▏      | 296/946 [01:49<04:00,  2.70it/s]Evaluating:  31%|███▏      | 297/946 [01:50<04:00,  2.70it/s]Evaluating:  32%|███▏      | 298/946 [01:50<03:59,  2.71it/s]Evaluating:  32%|███▏      | 299/946 [01:50<03:59,  2.70it/s]Evaluating:  32%|███▏      | 300/946 [01:51<03:59,  2.70it/s]Evaluating:  32%|███▏      | 301/946 [01:51<03:59,  2.69it/s]Evaluating:  32%|███▏      | 302/946 [01:51<03:59,  2.69it/s]Evaluating:  32%|███▏      | 303/946 [01:52<03:57,  2.71it/s]Evaluating:  32%|███▏      | 304/946 [01:52<03:57,  2.71it/s]Evaluating:  32%|███▏      | 305/946 [01:52<03:57,  2.70it/s]Evaluating:  32%|███▏      | 306/946 [01:53<03:56,  2.71it/s]Evaluating:  32%|███▏      | 307/946 [01:53<03:55,  2.71it/s]Evaluating:  33%|███▎      | 308/946 [01:54<03:54,  2.72it/s]Evaluating:  33%|███▎      | 309/946 [01:54<03:54,  2.71it/s]Evaluating:  33%|███▎      | 310/946 [01:54<03:54,  2.71it/s]Evaluating:  33%|███▎      | 311/946 [01:55<03:54,  2.71it/s]Evaluating:  33%|███▎      | 312/946 [01:55<03:53,  2.72it/s]Evaluating:  33%|███▎      | 313/946 [01:55<03:53,  2.71it/s]Evaluating:  33%|███▎      | 314/946 [01:56<03:53,  2.71it/s]Evaluating:  33%|███▎      | 315/946 [01:56<03:52,  2.71it/s]Evaluating:  33%|███▎      | 316/946 [01:57<03:52,  2.71it/s]Evaluating:  34%|███▎      | 317/946 [01:57<03:52,  2.71it/s]Evaluating:  34%|███▎      | 318/946 [01:57<03:51,  2.71it/s]Evaluating:  34%|███▎      | 319/946 [01:58<03:51,  2.71it/s]Evaluating:  34%|███▍      | 320/946 [01:58<03:50,  2.71it/s]Evaluating:  34%|███▍      | 321/946 [01:58<03:49,  2.72it/s]Evaluating:  34%|███▍      | 322/946 [01:59<03:49,  2.71it/s]Evaluating:  34%|███▍      | 323/946 [01:59<03:49,  2.72it/s]Evaluating:  34%|███▍      | 324/946 [01:59<03:49,  2.72it/s]Evaluating:  34%|███▍      | 325/946 [02:00<03:48,  2.72it/s]Evaluating:  34%|███▍      | 326/946 [02:00<03:48,  2.72it/s]Evaluating:  35%|███▍      | 327/946 [02:01<03:48,  2.71it/s]Evaluating:  35%|███▍      | 328/946 [02:01<03:47,  2.71it/s]Evaluating:  35%|███▍      | 329/946 [02:01<03:46,  2.73it/s]Evaluating:  35%|███▍      | 330/946 [02:02<03:45,  2.74it/s]Evaluating:  35%|███▍      | 331/946 [02:02<03:44,  2.73it/s]Evaluating:  35%|███▌      | 332/946 [02:02<03:44,  2.73it/s]Evaluating:  35%|███▌      | 333/946 [02:03<03:45,  2.72it/s]Evaluating:  35%|███▌      | 334/946 [02:03<03:44,  2.73it/s]Evaluating:  35%|███▌      | 335/946 [02:04<03:44,  2.72it/s]Evaluating:  36%|███▌      | 336/946 [02:04<03:43,  2.72it/s]Evaluating:  36%|███▌      | 337/946 [02:04<03:43,  2.73it/s]Evaluating:  36%|███▌      | 338/946 [02:05<03:43,  2.72it/s]Evaluating:  36%|███▌      | 339/946 [02:05<03:43,  2.72it/s]Evaluating:  36%|███▌      | 340/946 [02:05<03:44,  2.70it/s]Evaluating:  36%|███▌      | 341/946 [02:06<03:44,  2.70it/s]Evaluating:  36%|███▌      | 342/946 [02:06<03:43,  2.70it/s]Evaluating:  36%|███▋      | 343/946 [02:06<03:42,  2.71it/s]Evaluating:  36%|███▋      | 344/946 [02:07<03:41,  2.72it/s]Evaluating:  36%|███▋      | 345/946 [02:07<03:40,  2.72it/s]Evaluating:  37%|███▋      | 346/946 [02:08<03:41,  2.71it/s]Evaluating:  37%|███▋      | 347/946 [02:08<03:41,  2.71it/s]Evaluating:  37%|███▋      | 348/946 [02:08<03:41,  2.70it/s]Evaluating:  37%|███▋      | 349/946 [02:09<03:41,  2.70it/s]Evaluating:  37%|███▋      | 350/946 [02:09<03:40,  2.70it/s]Evaluating:  37%|███▋      | 351/946 [02:09<03:40,  2.70it/s]Evaluating:  37%|███▋      | 352/946 [02:10<03:40,  2.70it/s]Evaluating:  37%|███▋      | 353/946 [02:10<03:40,  2.69it/s]Evaluating:  37%|███▋      | 354/946 [02:11<03:39,  2.69it/s]Evaluating:  38%|███▊      | 355/946 [02:11<03:38,  2.71it/s]Evaluating:  38%|███▊      | 356/946 [02:11<03:37,  2.71it/s]Evaluating:  38%|███▊      | 357/946 [02:12<03:36,  2.71it/s]Evaluating:  38%|███▊      | 358/946 [02:12<03:36,  2.71it/s]Evaluating:  38%|███▊      | 359/946 [02:12<03:35,  2.72it/s]Evaluating:  38%|███▊      | 360/946 [02:13<03:35,  2.72it/s]Evaluating:  38%|███▊      | 361/946 [02:13<03:35,  2.71it/s]Evaluating:  38%|███▊      | 362/946 [02:13<03:35,  2.72it/s]Evaluating:  38%|███▊      | 363/946 [02:14<03:35,  2.71it/s]Evaluating:  38%|███▊      | 364/946 [02:14<03:34,  2.71it/s]Evaluating:  39%|███▊      | 365/946 [02:15<03:34,  2.71it/s]Evaluating:  39%|███▊      | 366/946 [02:15<03:34,  2.70it/s]Evaluating:  39%|███▉      | 367/946 [02:15<03:34,  2.70it/s]Evaluating:  39%|███▉      | 368/946 [02:16<03:34,  2.70it/s]Evaluating:  39%|███▉      | 369/946 [02:16<03:33,  2.71it/s]Evaluating:  39%|███▉      | 370/946 [02:16<03:32,  2.72it/s]Evaluating:  39%|███▉      | 371/946 [02:17<03:32,  2.71it/s]Evaluating:  39%|███▉      | 372/946 [02:17<03:32,  2.70it/s]Evaluating:  39%|███▉      | 373/946 [02:18<03:32,  2.70it/s]Evaluating:  40%|███▉      | 374/946 [02:18<03:31,  2.70it/s]Evaluating:  40%|███▉      | 375/946 [02:18<03:31,  2.71it/s]Evaluating:  40%|███▉      | 376/946 [02:19<03:30,  2.71it/s]Evaluating:  40%|███▉      | 377/946 [02:19<03:30,  2.71it/s]Evaluating:  40%|███▉      | 378/946 [02:19<03:30,  2.70it/s]Evaluating:  40%|████      | 379/946 [02:20<03:30,  2.70it/s]Evaluating:  40%|████      | 380/946 [02:20<03:29,  2.70it/s]Evaluating:  40%|████      | 381/946 [02:21<03:29,  2.69it/s]Evaluating:  40%|████      | 382/946 [02:21<03:29,  2.70it/s]Evaluating:  40%|████      | 383/946 [02:21<03:28,  2.70it/s]Evaluating:  41%|████      | 384/946 [02:22<03:27,  2.71it/s]Evaluating:  41%|████      | 385/946 [02:22<03:27,  2.70it/s]Evaluating:  41%|████      | 386/946 [02:22<03:26,  2.71it/s]Evaluating:  41%|████      | 387/946 [02:23<03:25,  2.72it/s]Evaluating:  41%|████      | 388/946 [02:23<03:24,  2.72it/s]Evaluating:  41%|████      | 389/946 [02:23<03:25,  2.71it/s]Evaluating:  41%|████      | 390/946 [02:24<03:25,  2.70it/s]Evaluating:  41%|████▏     | 391/946 [02:24<03:25,  2.69it/s]Evaluating:  41%|████▏     | 392/946 [02:25<03:26,  2.69it/s]Evaluating:  42%|████▏     | 393/946 [02:25<03:26,  2.68it/s]Evaluating:  42%|████▏     | 394/946 [02:25<03:25,  2.69it/s]Evaluating:  42%|████▏     | 395/946 [02:26<03:25,  2.68it/s]Evaluating:  42%|████▏     | 396/946 [02:26<03:23,  2.70it/s]Evaluating:  42%|████▏     | 397/946 [02:26<03:22,  2.71it/s]Evaluating:  42%|████▏     | 398/946 [02:27<03:21,  2.72it/s]Evaluating:  42%|████▏     | 399/946 [02:27<03:21,  2.71it/s]Evaluating:  42%|████▏     | 400/946 [02:28<03:21,  2.70it/s]Evaluating:  42%|████▏     | 401/946 [02:28<03:21,  2.70it/s]Evaluating:  42%|████▏     | 402/946 [02:28<03:21,  2.69it/s]Evaluating:  43%|████▎     | 403/946 [02:29<03:21,  2.70it/s]Evaluating:  43%|████▎     | 404/946 [02:29<03:20,  2.70it/s]Evaluating:  43%|████▎     | 405/946 [02:29<03:19,  2.71it/s]Evaluating:  43%|████▎     | 406/946 [02:30<03:19,  2.71it/s]Evaluating:  43%|████▎     | 407/946 [02:30<03:20,  2.69it/s]Evaluating:  43%|████▎     | 408/946 [02:31<03:19,  2.70it/s]Evaluating:  43%|████▎     | 409/946 [02:31<03:18,  2.71it/s]Evaluating:  43%|████▎     | 410/946 [02:31<03:17,  2.72it/s]Evaluating:  43%|████▎     | 411/946 [02:32<03:17,  2.72it/s]Evaluating:  44%|████▎     | 412/946 [02:32<03:17,  2.70it/s]Evaluating:  44%|████▎     | 413/946 [02:32<03:16,  2.71it/s]Evaluating:  44%|████▍     | 414/946 [02:33<03:15,  2.72it/s]Evaluating:  44%|████▍     | 415/946 [02:33<03:15,  2.72it/s]Evaluating:  44%|████▍     | 416/946 [02:33<03:14,  2.72it/s]Evaluating:  44%|████▍     | 417/946 [02:34<03:15,  2.70it/s]Evaluating:  44%|████▍     | 418/946 [02:34<03:14,  2.72it/s]Evaluating:  44%|████▍     | 419/946 [02:35<03:13,  2.73it/s]Evaluating:  44%|████▍     | 420/946 [02:35<03:13,  2.72it/s]Evaluating:  45%|████▍     | 421/946 [02:35<03:13,  2.72it/s]Evaluating:  45%|████▍     | 422/946 [02:36<03:13,  2.71it/s]Evaluating:  45%|████▍     | 423/946 [02:36<03:13,  2.70it/s]Evaluating:  45%|████▍     | 424/946 [02:36<03:13,  2.70it/s]Evaluating:  45%|████▍     | 425/946 [02:37<03:13,  2.69it/s]Evaluating:  45%|████▌     | 426/946 [02:37<03:13,  2.69it/s]Evaluating:  45%|████▌     | 427/946 [02:38<03:12,  2.69it/s]Evaluating:  45%|████▌     | 428/946 [02:38<03:12,  2.69it/s]Evaluating:  45%|████▌     | 429/946 [02:38<03:11,  2.70it/s]Evaluating:  45%|████▌     | 430/946 [02:39<03:10,  2.71it/s]Evaluating:  46%|████▌     | 431/946 [02:39<03:09,  2.72it/s]Evaluating:  46%|████▌     | 432/946 [02:39<03:09,  2.71it/s]Evaluating:  46%|████▌     | 433/946 [02:40<03:08,  2.72it/s]Evaluating:  46%|████▌     | 434/946 [02:40<03:08,  2.72it/s]Evaluating:  46%|████▌     | 435/946 [02:41<03:12,  2.65it/s]Evaluating:  46%|████▌     | 436/946 [02:41<03:11,  2.67it/s]Evaluating:  46%|████▌     | 437/946 [02:41<03:09,  2.68it/s]Evaluating:  46%|████▋     | 438/946 [02:42<03:09,  2.69it/s]Evaluating:  46%|████▋     | 439/946 [02:42<03:08,  2.70it/s]Evaluating:  47%|████▋     | 440/946 [02:42<03:07,  2.70it/s]Evaluating:  47%|████▋     | 441/946 [02:43<03:06,  2.70it/s]Evaluating:  47%|████▋     | 442/946 [02:43<03:05,  2.72it/s]Evaluating:  47%|████▋     | 443/946 [02:43<03:05,  2.72it/s]Evaluating:  47%|████▋     | 444/946 [02:44<03:05,  2.71it/s]Evaluating:  47%|████▋     | 445/946 [02:44<03:04,  2.72it/s]Evaluating:  47%|████▋     | 446/946 [02:45<03:04,  2.71it/s]Evaluating:  47%|████▋     | 447/946 [02:45<03:05,  2.70it/s]Evaluating:  47%|████▋     | 448/946 [02:45<03:04,  2.69it/s]Evaluating:  47%|████▋     | 449/946 [02:46<03:03,  2.71it/s]Evaluating:  48%|████▊     | 450/946 [02:46<03:02,  2.72it/s]Evaluating:  48%|████▊     | 451/946 [02:46<03:01,  2.72it/s]Evaluating:  48%|████▊     | 452/946 [02:47<03:01,  2.73it/s]Evaluating:  48%|████▊     | 453/946 [02:47<03:01,  2.72it/s]Evaluating:  48%|████▊     | 454/946 [02:48<03:00,  2.72it/s]Evaluating:  48%|████▊     | 455/946 [02:48<03:00,  2.72it/s]Evaluating:  48%|████▊     | 456/946 [02:48<03:00,  2.72it/s]Evaluating:  48%|████▊     | 457/946 [02:49<02:59,  2.72it/s]Evaluating:  48%|████▊     | 458/946 [02:49<02:58,  2.73it/s]Evaluating:  49%|████▊     | 459/946 [02:49<02:58,  2.72it/s]Evaluating:  49%|████▊     | 460/946 [02:50<02:58,  2.72it/s]Evaluating:  49%|████▊     | 461/946 [02:50<02:57,  2.73it/s]Evaluating:  49%|████▉     | 462/946 [02:50<02:56,  2.74it/s]Evaluating:  49%|████▉     | 463/946 [02:51<02:55,  2.75it/s]Evaluating:  49%|████▉     | 464/946 [02:51<02:55,  2.74it/s]Evaluating:  49%|████▉     | 465/946 [02:52<02:55,  2.74it/s]Evaluating:  49%|████▉     | 466/946 [02:52<02:54,  2.75it/s]Evaluating:  49%|████▉     | 467/946 [02:52<02:54,  2.74it/s]Evaluating:  49%|████▉     | 468/946 [02:53<02:54,  2.74it/s]Evaluating:  50%|████▉     | 469/946 [02:53<02:55,  2.72it/s]Evaluating:  50%|████▉     | 470/946 [02:53<02:54,  2.73it/s]Evaluating:  50%|████▉     | 471/946 [02:54<02:54,  2.73it/s]Evaluating:  50%|████▉     | 472/946 [02:54<02:54,  2.72it/s]Evaluating:  50%|█████     | 473/946 [02:54<02:54,  2.71it/s]Evaluating:  50%|█████     | 474/946 [02:55<02:54,  2.71it/s]Evaluating:  50%|█████     | 475/946 [02:55<02:53,  2.72it/s]Evaluating:  50%|█████     | 476/946 [02:56<02:53,  2.72it/s]Evaluating:  50%|█████     | 477/946 [02:56<02:52,  2.72it/s]Evaluating:  51%|█████     | 478/946 [02:56<02:52,  2.72it/s]Evaluating:  51%|█████     | 479/946 [02:57<02:51,  2.72it/s]Evaluating:  51%|█████     | 480/946 [02:57<02:50,  2.73it/s]Evaluating:  51%|█████     | 481/946 [02:57<02:50,  2.73it/s]Evaluating:  51%|█████     | 482/946 [02:58<02:50,  2.72it/s]Evaluating:  51%|█████     | 483/946 [02:58<02:50,  2.72it/s]Evaluating:  51%|█████     | 484/946 [02:59<02:49,  2.72it/s]Evaluating:  51%|█████▏    | 485/946 [02:59<02:48,  2.73it/s]Evaluating:  51%|█████▏    | 486/946 [02:59<02:48,  2.73it/s]Evaluating:  51%|█████▏    | 487/946 [03:00<02:48,  2.72it/s]Evaluating:  52%|█████▏    | 488/946 [03:00<02:48,  2.71it/s]Evaluating:  52%|█████▏    | 489/946 [03:00<02:48,  2.71it/s]Evaluating:  52%|█████▏    | 490/946 [03:01<02:47,  2.72it/s]Evaluating:  52%|█████▏    | 491/946 [03:01<02:47,  2.72it/s]Evaluating:  52%|█████▏    | 492/946 [03:01<02:46,  2.73it/s]Evaluating:  52%|█████▏    | 493/946 [03:02<02:45,  2.73it/s]Evaluating:  52%|█████▏    | 494/946 [03:02<02:45,  2.74it/s]Evaluating:  52%|█████▏    | 495/946 [03:03<02:45,  2.73it/s]Evaluating:  52%|█████▏    | 496/946 [03:03<02:45,  2.72it/s]Evaluating:  53%|█████▎    | 497/946 [03:03<02:45,  2.72it/s]Evaluating:  53%|█████▎    | 498/946 [03:04<02:44,  2.72it/s]Evaluating:  53%|█████▎    | 499/946 [03:04<02:43,  2.73it/s]Evaluating:  53%|█████▎    | 500/946 [03:04<02:43,  2.73it/s]Evaluating:  53%|█████▎    | 501/946 [03:05<02:43,  2.72it/s]Evaluating:  53%|█████▎    | 502/946 [03:05<02:43,  2.72it/s]Evaluating:  53%|█████▎    | 503/946 [03:05<02:42,  2.72it/s]Evaluating:  53%|█████▎    | 504/946 [03:06<02:41,  2.73it/s]Evaluating:  53%|█████▎    | 505/946 [03:06<02:41,  2.73it/s]Evaluating:  53%|█████▎    | 506/946 [03:07<02:41,  2.73it/s]Evaluating:  54%|█████▎    | 507/946 [03:07<02:41,  2.72it/s]Evaluating:  54%|█████▎    | 508/946 [03:07<02:40,  2.72it/s]Evaluating:  54%|█████▍    | 509/946 [03:08<02:40,  2.73it/s]Evaluating:  54%|█████▍    | 510/946 [03:08<02:39,  2.73it/s]Evaluating:  54%|█████▍    | 511/946 [03:08<02:39,  2.73it/s]Evaluating:  54%|█████▍    | 512/946 [03:09<02:40,  2.71it/s]Evaluating:  54%|█████▍    | 513/946 [03:09<02:40,  2.71it/s]Evaluating:  54%|█████▍    | 514/946 [03:10<02:40,  2.70it/s]Evaluating:  54%|█████▍    | 515/946 [03:10<02:39,  2.71it/s]Evaluating:  55%|█████▍    | 516/946 [03:10<02:37,  2.72it/s]Evaluating:  55%|█████▍    | 517/946 [03:11<02:37,  2.72it/s]Evaluating:  55%|█████▍    | 518/946 [03:11<02:37,  2.71it/s]Evaluating:  55%|█████▍    | 519/946 [03:11<02:37,  2.71it/s]Evaluating:  55%|█████▍    | 520/946 [03:12<02:37,  2.70it/s]Evaluating:  55%|█████▌    | 521/946 [03:12<02:36,  2.72it/s]Evaluating:  55%|█████▌    | 522/946 [03:12<02:35,  2.72it/s]Evaluating:  55%|█████▌    | 523/946 [03:13<02:35,  2.73it/s]Evaluating:  55%|█████▌    | 524/946 [03:13<02:35,  2.71it/s]Evaluating:  55%|█████▌    | 525/946 [03:14<02:35,  2.71it/s]Evaluating:  56%|█████▌    | 526/946 [03:14<02:35,  2.71it/s]Evaluating:  56%|█████▌    | 527/946 [03:14<02:34,  2.71it/s]Evaluating:  56%|█████▌    | 528/946 [03:15<02:34,  2.71it/s]Evaluating:  56%|█████▌    | 529/946 [03:15<02:34,  2.70it/s]Evaluating:  56%|█████▌    | 530/946 [03:15<02:33,  2.71it/s]Evaluating:  56%|█████▌    | 531/946 [03:16<02:33,  2.70it/s]Evaluating:  56%|█████▌    | 532/946 [03:16<02:33,  2.70it/s]Evaluating:  56%|█████▋    | 533/946 [03:17<02:32,  2.70it/s]Evaluating:  56%|█████▋    | 534/946 [03:17<02:31,  2.71it/s]Evaluating:  57%|█████▋    | 535/946 [03:17<02:31,  2.71it/s]Evaluating:  57%|█████▋    | 536/946 [03:18<02:31,  2.70it/s]Evaluating:  57%|█████▋    | 537/946 [03:18<02:31,  2.70it/s]Evaluating:  57%|█████▋    | 538/946 [03:18<02:30,  2.71it/s]Evaluating:  57%|█████▋    | 539/946 [03:19<02:29,  2.72it/s]Evaluating:  57%|█████▋    | 540/946 [03:19<02:30,  2.70it/s]Evaluating:  57%|█████▋    | 541/946 [03:19<02:29,  2.71it/s]Evaluating:  57%|█████▋    | 542/946 [03:20<02:28,  2.71it/s]Evaluating:  57%|█████▋    | 543/946 [03:20<02:28,  2.71it/s]Evaluating:  58%|█████▊    | 544/946 [03:21<02:28,  2.71it/s]Evaluating:  58%|█████▊    | 545/946 [03:21<02:28,  2.70it/s]Evaluating:  58%|█████▊    | 546/946 [03:21<02:28,  2.70it/s]Evaluating:  58%|█████▊    | 547/946 [03:22<02:27,  2.70it/s]Evaluating:  58%|█████▊    | 548/946 [03:22<02:26,  2.71it/s]Evaluating:  58%|█████▊    | 549/946 [03:22<02:25,  2.72it/s]Evaluating:  58%|█████▊    | 550/946 [03:23<02:25,  2.72it/s]Evaluating:  58%|█████▊    | 551/946 [03:23<02:25,  2.72it/s]Evaluating:  58%|█████▊    | 552/946 [03:24<02:25,  2.71it/s]Evaluating:  58%|█████▊    | 553/946 [03:24<02:25,  2.70it/s]Evaluating:  59%|█████▊    | 554/946 [03:24<02:25,  2.69it/s]Evaluating:  59%|█████▊    | 555/946 [03:25<02:25,  2.69it/s]Evaluating:  59%|█████▉    | 556/946 [03:25<02:24,  2.69it/s]Evaluating:  59%|█████▉    | 557/946 [03:25<02:24,  2.69it/s]Evaluating:  59%|█████▉    | 558/946 [03:26<02:24,  2.69it/s]Evaluating:  59%|█████▉    | 559/946 [03:26<02:23,  2.69it/s]Evaluating:  59%|█████▉    | 560/946 [03:27<02:23,  2.70it/s]Evaluating:  59%|█████▉    | 561/946 [03:27<02:23,  2.69it/s]Evaluating:  59%|█████▉    | 562/946 [03:27<02:22,  2.69it/s]Evaluating:  60%|█████▉    | 563/946 [03:28<02:21,  2.70it/s]Evaluating:  60%|█████▉    | 564/946 [03:28<02:21,  2.70it/s]Evaluating:  60%|█████▉    | 565/946 [03:28<02:21,  2.69it/s]Evaluating:  60%|█████▉    | 566/946 [03:29<02:21,  2.68it/s]Evaluating:  60%|█████▉    | 567/946 [03:29<02:21,  2.69it/s]Evaluating:  60%|██████    | 568/946 [03:30<02:20,  2.69it/s]Evaluating:  60%|██████    | 569/946 [03:30<02:20,  2.69it/s]Evaluating:  60%|██████    | 570/946 [03:30<02:19,  2.70it/s]Evaluating:  60%|██████    | 571/946 [03:31<02:19,  2.69it/s]Evaluating:  60%|██████    | 572/946 [03:31<02:18,  2.71it/s]Evaluating:  61%|██████    | 573/946 [03:31<02:18,  2.70it/s]Evaluating:  61%|██████    | 574/946 [03:32<02:17,  2.71it/s]Evaluating:  61%|██████    | 575/946 [03:32<02:16,  2.72it/s]Evaluating:  61%|██████    | 576/946 [03:32<02:16,  2.71it/s]Evaluating:  61%|██████    | 577/946 [03:33<02:16,  2.71it/s]Evaluating:  61%|██████    | 578/946 [03:33<02:15,  2.71it/s]Evaluating:  61%|██████    | 579/946 [03:34<02:15,  2.71it/s]Evaluating:  61%|██████▏   | 580/946 [03:34<02:16,  2.68it/s]Evaluating:  61%|██████▏   | 581/946 [03:34<02:15,  2.69it/s]Evaluating:  62%|██████▏   | 582/946 [03:35<02:15,  2.69it/s]Evaluating:  62%|██████▏   | 583/946 [03:35<02:15,  2.69it/s]Evaluating:  62%|██████▏   | 584/946 [03:35<02:14,  2.69it/s]Evaluating:  62%|██████▏   | 585/946 [03:36<02:13,  2.70it/s]Evaluating:  62%|██████▏   | 586/946 [03:36<02:13,  2.70it/s]Evaluating:  62%|██████▏   | 587/946 [03:37<02:12,  2.72it/s]Evaluating:  62%|██████▏   | 588/946 [03:37<02:11,  2.71it/s]Evaluating:  62%|██████▏   | 589/946 [03:37<02:11,  2.71it/s]Evaluating:  62%|██████▏   | 590/946 [03:38<02:10,  2.72it/s]Evaluating:  62%|██████▏   | 591/946 [03:38<02:10,  2.73it/s]Evaluating:  63%|██████▎   | 592/946 [03:38<02:10,  2.72it/s]Evaluating:  63%|██████▎   | 593/946 [03:39<02:09,  2.72it/s]Evaluating:  63%|██████▎   | 594/946 [03:39<02:09,  2.71it/s]Evaluating:  63%|██████▎   | 595/946 [03:39<02:09,  2.70it/s]Evaluating:  63%|██████▎   | 596/946 [03:40<02:08,  2.72it/s]Evaluating:  63%|██████▎   | 597/946 [03:40<02:08,  2.71it/s]Evaluating:  63%|██████▎   | 598/946 [03:41<02:08,  2.71it/s]Evaluating:  63%|██████▎   | 599/946 [03:41<02:08,  2.71it/s]Evaluating:  63%|██████▎   | 600/946 [03:41<02:07,  2.71it/s]Evaluating:  64%|██████▎   | 601/946 [03:42<02:07,  2.72it/s]Evaluating:  64%|██████▎   | 602/946 [03:42<02:07,  2.71it/s]Evaluating:  64%|██████▎   | 603/946 [03:42<02:06,  2.72it/s]Evaluating:  64%|██████▍   | 604/946 [03:43<02:05,  2.72it/s]Evaluating:  64%|██████▍   | 605/946 [03:43<02:05,  2.73it/s]Evaluating:  64%|██████▍   | 606/946 [03:44<02:05,  2.72it/s]Evaluating:  64%|██████▍   | 607/946 [03:44<02:04,  2.72it/s]Evaluating:  64%|██████▍   | 608/946 [03:44<02:04,  2.72it/s]Evaluating:  64%|██████▍   | 609/946 [03:45<02:03,  2.73it/s]Evaluating:  64%|██████▍   | 610/946 [03:45<02:03,  2.72it/s]Evaluating:  65%|██████▍   | 611/946 [03:45<02:03,  2.72it/s]Evaluating:  65%|██████▍   | 612/946 [03:46<02:02,  2.72it/s]Evaluating:  65%|██████▍   | 613/946 [03:46<02:02,  2.71it/s]Evaluating:  65%|██████▍   | 614/946 [03:46<02:02,  2.71it/s]Evaluating:  65%|██████▌   | 615/946 [03:47<02:02,  2.71it/s]Evaluating:  65%|██████▌   | 616/946 [03:47<02:02,  2.70it/s]Evaluating:  65%|██████▌   | 617/946 [03:48<02:02,  2.69it/s]Evaluating:  65%|██████▌   | 618/946 [03:48<02:01,  2.71it/s]Evaluating:  65%|██████▌   | 619/946 [03:48<02:00,  2.71it/s]Evaluating:  66%|██████▌   | 620/946 [03:49<02:00,  2.71it/s]Evaluating:  66%|██████▌   | 621/946 [03:49<02:00,  2.71it/s]Evaluating:  66%|██████▌   | 622/946 [03:49<01:59,  2.71it/s]Evaluating:  66%|██████▌   | 623/946 [03:50<01:58,  2.71it/s]Evaluating:  66%|██████▌   | 624/946 [03:50<01:58,  2.71it/s]Evaluating:  66%|██████▌   | 625/946 [03:51<01:58,  2.72it/s]Evaluating:  66%|██████▌   | 626/946 [03:51<01:58,  2.71it/s]Evaluating:  66%|██████▋   | 627/946 [03:51<01:57,  2.71it/s]Evaluating:  66%|██████▋   | 628/946 [03:52<01:57,  2.72it/s]Evaluating:  66%|██████▋   | 629/946 [03:52<01:56,  2.72it/s]Evaluating:  67%|██████▋   | 630/946 [03:52<01:56,  2.72it/s]Evaluating:  67%|██████▋   | 631/946 [03:53<01:55,  2.72it/s]Evaluating:  67%|██████▋   | 632/946 [03:53<01:55,  2.72it/s]Evaluating:  67%|██████▋   | 633/946 [03:53<01:55,  2.71it/s]Evaluating:  67%|██████▋   | 634/946 [03:54<01:54,  2.73it/s]Evaluating:  67%|██████▋   | 635/946 [03:54<01:53,  2.73it/s]Evaluating:  67%|██████▋   | 636/946 [03:55<01:53,  2.73it/s]Evaluating:  67%|██████▋   | 637/946 [03:55<01:52,  2.74it/s]Evaluating:  67%|██████▋   | 638/946 [03:55<01:52,  2.73it/s]Evaluating:  68%|██████▊   | 639/946 [03:56<01:53,  2.72it/s]Evaluating:  68%|██████▊   | 640/946 [03:56<01:53,  2.70it/s]Evaluating:  68%|██████▊   | 641/946 [03:56<01:52,  2.71it/s]Evaluating:  68%|██████▊   | 642/946 [03:57<01:52,  2.71it/s]Evaluating:  68%|██████▊   | 643/946 [03:57<01:51,  2.72it/s]Evaluating:  68%|██████▊   | 644/946 [03:58<01:50,  2.73it/s]Evaluating:  68%|██████▊   | 645/946 [03:58<01:50,  2.73it/s]Evaluating:  68%|██████▊   | 646/946 [03:58<01:50,  2.72it/s]Evaluating:  68%|██████▊   | 647/946 [03:59<01:50,  2.71it/s]Evaluating:  68%|██████▊   | 648/946 [03:59<01:50,  2.71it/s]Evaluating:  69%|██████▊   | 649/946 [03:59<01:49,  2.71it/s]Evaluating:  69%|██████▊   | 650/946 [04:00<01:49,  2.71it/s]Evaluating:  69%|██████▉   | 651/946 [04:00<01:51,  2.64it/s]Evaluating:  69%|██████▉   | 652/946 [04:00<01:50,  2.67it/s]Evaluating:  69%|██████▉   | 653/946 [04:01<01:49,  2.68it/s]Evaluating:  69%|██████▉   | 654/946 [04:01<01:48,  2.69it/s]Evaluating:  69%|██████▉   | 655/946 [04:02<01:48,  2.68it/s]Evaluating:  69%|██████▉   | 656/946 [04:02<01:47,  2.69it/s]Evaluating:  69%|██████▉   | 657/946 [04:02<01:47,  2.69it/s]Evaluating:  70%|██████▉   | 658/946 [04:03<01:46,  2.69it/s]Evaluating:  70%|██████▉   | 659/946 [04:03<01:46,  2.70it/s]Evaluating:  70%|██████▉   | 660/946 [04:03<01:45,  2.71it/s]Evaluating:  70%|██████▉   | 661/946 [04:04<01:44,  2.72it/s]Evaluating:  70%|██████▉   | 662/946 [04:04<01:45,  2.70it/s]Evaluating:  70%|███████   | 663/946 [04:05<01:44,  2.70it/s]Evaluating:  70%|███████   | 664/946 [04:05<01:44,  2.71it/s]Evaluating:  70%|███████   | 665/946 [04:05<01:43,  2.71it/s]Evaluating:  70%|███████   | 666/946 [04:06<01:43,  2.72it/s]Evaluating:  71%|███████   | 667/946 [04:06<01:42,  2.72it/s]Evaluating:  71%|███████   | 668/946 [04:06<01:42,  2.71it/s]Evaluating:  71%|███████   | 669/946 [04:07<01:42,  2.70it/s]Evaluating:  71%|███████   | 670/946 [04:07<01:42,  2.70it/s]Evaluating:  71%|███████   | 671/946 [04:08<01:42,  2.70it/s]Evaluating:  71%|███████   | 672/946 [04:08<01:41,  2.70it/s]Evaluating:  71%|███████   | 673/946 [04:08<01:40,  2.72it/s]Evaluating:  71%|███████   | 674/946 [04:09<01:40,  2.72it/s]Evaluating:  71%|███████▏  | 675/946 [04:09<01:40,  2.70it/s]Evaluating:  71%|███████▏  | 676/946 [04:09<01:40,  2.70it/s]Evaluating:  72%|███████▏  | 677/946 [04:10<01:39,  2.70it/s]Evaluating:  72%|███████▏  | 678/946 [04:10<01:39,  2.70it/s]Evaluating:  72%|███████▏  | 679/946 [04:10<01:38,  2.70it/s]Evaluating:  72%|███████▏  | 680/946 [04:11<01:39,  2.68it/s]Evaluating:  72%|███████▏  | 681/946 [04:11<01:38,  2.69it/s]Evaluating:  72%|███████▏  | 682/946 [04:12<01:37,  2.70it/s]Evaluating:  72%|███████▏  | 683/946 [04:12<01:37,  2.70it/s]Evaluating:  72%|███████▏  | 684/946 [04:12<01:37,  2.69it/s]Evaluating:  72%|███████▏  | 685/946 [04:13<01:36,  2.69it/s]Evaluating:  73%|███████▎  | 686/946 [04:13<01:36,  2.69it/s]Evaluating:  73%|███████▎  | 687/946 [04:13<01:36,  2.69it/s]Evaluating:  73%|███████▎  | 688/946 [04:14<01:35,  2.69it/s]Evaluating:  73%|███████▎  | 689/946 [04:14<01:35,  2.69it/s]Evaluating:  73%|███████▎  | 690/946 [04:15<01:35,  2.69it/s]Evaluating:  73%|███████▎  | 691/946 [04:15<01:34,  2.69it/s]Evaluating:  73%|███████▎  | 692/946 [04:15<01:34,  2.68it/s]Evaluating:  73%|███████▎  | 693/946 [04:16<01:33,  2.70it/s]Evaluating:  73%|███████▎  | 694/946 [04:16<01:33,  2.69it/s]Evaluating:  73%|███████▎  | 695/946 [04:16<01:32,  2.70it/s]Evaluating:  74%|███████▎  | 696/946 [04:17<01:32,  2.71it/s]Evaluating:  74%|███████▎  | 697/946 [04:17<01:31,  2.71it/s]Evaluating:  74%|███████▍  | 698/946 [04:18<01:31,  2.71it/s]Evaluating:  74%|███████▍  | 699/946 [04:18<01:30,  2.72it/s]Evaluating:  74%|███████▍  | 700/946 [04:18<01:30,  2.71it/s]Evaluating:  74%|███████▍  | 701/946 [04:19<01:30,  2.71it/s]Evaluating:  74%|███████▍  | 702/946 [04:19<01:29,  2.72it/s]Evaluating:  74%|███████▍  | 703/946 [04:19<01:29,  2.71it/s]Evaluating:  74%|███████▍  | 704/946 [04:20<01:29,  2.71it/s]Evaluating:  75%|███████▍  | 705/946 [04:20<01:29,  2.70it/s]Evaluating:  75%|███████▍  | 706/946 [04:20<01:29,  2.68it/s]Evaluating:  75%|███████▍  | 707/946 [04:21<01:28,  2.69it/s]Evaluating:  75%|███████▍  | 708/946 [04:21<01:28,  2.70it/s]Evaluating:  75%|███████▍  | 709/946 [04:22<01:28,  2.69it/s]Evaluating:  75%|███████▌  | 710/946 [04:22<01:27,  2.68it/s]Evaluating:  75%|███████▌  | 711/946 [04:22<01:27,  2.69it/s]Evaluating:  75%|███████▌  | 712/946 [04:23<01:26,  2.69it/s]Evaluating:  75%|███████▌  | 713/946 [04:23<01:26,  2.70it/s]Evaluating:  75%|███████▌  | 714/946 [04:23<01:25,  2.71it/s]Evaluating:  76%|███████▌  | 715/946 [04:24<01:25,  2.71it/s]Evaluating:  76%|███████▌  | 716/946 [04:24<01:24,  2.71it/s]Evaluating:  76%|███████▌  | 717/946 [04:25<01:24,  2.72it/s]Evaluating:  76%|███████▌  | 718/946 [04:25<01:23,  2.72it/s]Evaluating:  76%|███████▌  | 719/946 [04:25<01:23,  2.73it/s]Evaluating:  76%|███████▌  | 720/946 [04:26<01:23,  2.72it/s]Evaluating:  76%|███████▌  | 721/946 [04:26<01:22,  2.72it/s]Evaluating:  76%|███████▋  | 722/946 [04:26<01:22,  2.72it/s]Evaluating:  76%|███████▋  | 723/946 [04:27<01:22,  2.70it/s]Evaluating:  77%|███████▋  | 724/946 [04:27<01:22,  2.69it/s]Evaluating:  77%|███████▋  | 725/946 [04:28<01:22,  2.68it/s]Evaluating:  77%|███████▋  | 726/946 [04:28<01:22,  2.68it/s]Evaluating:  77%|███████▋  | 727/946 [04:28<01:22,  2.67it/s]Evaluating:  77%|███████▋  | 728/946 [04:29<01:21,  2.67it/s]Evaluating:  77%|███████▋  | 729/946 [04:29<01:21,  2.68it/s]Evaluating:  77%|███████▋  | 730/946 [04:29<01:20,  2.68it/s]Evaluating:  77%|███████▋  | 731/946 [04:30<01:20,  2.68it/s]Evaluating:  77%|███████▋  | 732/946 [04:30<01:19,  2.69it/s]Evaluating:  77%|███████▋  | 733/946 [04:31<01:19,  2.69it/s]Evaluating:  78%|███████▊  | 734/946 [04:31<01:18,  2.69it/s]Evaluating:  78%|███████▊  | 735/946 [04:31<01:18,  2.68it/s]Evaluating:  78%|███████▊  | 736/946 [04:32<01:17,  2.70it/s]Evaluating:  78%|███████▊  | 737/946 [04:32<01:17,  2.70it/s]Evaluating:  78%|███████▊  | 738/946 [04:32<01:16,  2.71it/s]Evaluating:  78%|███████▊  | 739/946 [04:33<01:16,  2.71it/s]Evaluating:  78%|███████▊  | 740/946 [04:33<01:16,  2.71it/s]Evaluating:  78%|███████▊  | 741/946 [04:33<01:15,  2.71it/s]Evaluating:  78%|███████▊  | 742/946 [04:34<01:15,  2.71it/s]Evaluating:  79%|███████▊  | 743/946 [04:34<01:14,  2.71it/s]Evaluating:  79%|███████▊  | 744/946 [04:35<01:14,  2.70it/s]Evaluating:  79%|███████▉  | 745/946 [04:35<01:14,  2.70it/s]Evaluating:  79%|███████▉  | 746/946 [04:35<01:14,  2.70it/s]Evaluating:  79%|███████▉  | 747/946 [04:36<01:13,  2.71it/s]Evaluating:  79%|███████▉  | 748/946 [04:36<01:13,  2.71it/s]Evaluating:  79%|███████▉  | 749/946 [04:36<01:12,  2.71it/s]Evaluating:  79%|███████▉  | 750/946 [04:37<01:12,  2.71it/s]Evaluating:  79%|███████▉  | 751/946 [04:37<01:11,  2.71it/s]Evaluating:  79%|███████▉  | 752/946 [04:38<01:11,  2.71it/s]Evaluating:  80%|███████▉  | 753/946 [04:38<01:11,  2.71it/s]Evaluating:  80%|███████▉  | 754/946 [04:38<01:11,  2.70it/s]Evaluating:  80%|███████▉  | 755/946 [04:39<01:10,  2.70it/s]Evaluating:  80%|███████▉  | 756/946 [04:39<01:10,  2.70it/s]Evaluating:  80%|████████  | 757/946 [04:39<01:10,  2.70it/s]Evaluating:  80%|████████  | 758/946 [04:40<01:10,  2.68it/s]Evaluating:  80%|████████  | 759/946 [04:40<01:09,  2.69it/s]Evaluating:  80%|████████  | 760/946 [04:41<01:09,  2.68it/s]Evaluating:  80%|████████  | 761/946 [04:41<01:08,  2.70it/s]Evaluating:  81%|████████  | 762/946 [04:41<01:08,  2.70it/s]Evaluating:  81%|████████  | 763/946 [04:42<01:07,  2.71it/s]Evaluating:  81%|████████  | 764/946 [04:42<01:07,  2.70it/s]Evaluating:  81%|████████  | 765/946 [04:42<01:06,  2.71it/s]Evaluating:  81%|████████  | 766/946 [04:43<01:06,  2.71it/s]Evaluating:  81%|████████  | 767/946 [04:43<01:06,  2.70it/s]Evaluating:  81%|████████  | 768/946 [04:43<01:05,  2.71it/s]Evaluating:  81%|████████▏ | 769/946 [04:44<01:05,  2.71it/s]Evaluating:  81%|████████▏ | 770/946 [04:44<01:04,  2.71it/s]Evaluating:  82%|████████▏ | 771/946 [04:45<01:04,  2.71it/s]Evaluating:  82%|████████▏ | 772/946 [04:45<01:04,  2.70it/s]Evaluating:  82%|████████▏ | 773/946 [04:45<01:04,  2.70it/s]Evaluating:  82%|████████▏ | 774/946 [04:46<01:03,  2.71it/s]Evaluating:  82%|████████▏ | 775/946 [04:46<01:03,  2.69it/s]Evaluating:  82%|████████▏ | 776/946 [04:46<01:03,  2.70it/s]Evaluating:  82%|████████▏ | 777/946 [04:47<01:02,  2.70it/s]Evaluating:  82%|████████▏ | 778/946 [04:47<01:02,  2.71it/s]Evaluating:  82%|████████▏ | 779/946 [04:48<01:01,  2.70it/s]Evaluating:  82%|████████▏ | 780/946 [04:48<01:01,  2.69it/s]Evaluating:  83%|████████▎ | 781/946 [04:48<01:01,  2.66it/s]Evaluating:  83%|████████▎ | 782/946 [04:49<01:01,  2.67it/s]Evaluating:  83%|████████▎ | 783/946 [04:49<01:01,  2.67it/s]Evaluating:  83%|████████▎ | 784/946 [04:49<01:00,  2.68it/s]Evaluating:  83%|████████▎ | 785/946 [04:50<00:59,  2.69it/s]Evaluating:  83%|████████▎ | 786/946 [04:50<00:59,  2.69it/s]Evaluating:  83%|████████▎ | 787/946 [04:51<00:58,  2.70it/s]Evaluating:  83%|████████▎ | 788/946 [04:51<00:58,  2.71it/s]Evaluating:  83%|████████▎ | 789/946 [04:51<00:58,  2.69it/s]Evaluating:  84%|████████▎ | 790/946 [04:52<00:57,  2.70it/s]Evaluating:  84%|████████▎ | 791/946 [04:52<00:57,  2.70it/s]Evaluating:  84%|████████▎ | 792/946 [04:52<00:57,  2.68it/s]Evaluating:  84%|████████▍ | 793/946 [04:53<00:56,  2.70it/s]Evaluating:  84%|████████▍ | 794/946 [04:53<00:56,  2.71it/s]Evaluating:  84%|████████▍ | 795/946 [04:53<00:56,  2.69it/s]Evaluating:  84%|████████▍ | 796/946 [04:54<00:55,  2.69it/s]Evaluating:  84%|████████▍ | 797/946 [04:54<00:55,  2.69it/s]Evaluating:  84%|████████▍ | 798/946 [04:55<00:54,  2.70it/s]Evaluating:  84%|████████▍ | 799/946 [04:55<00:54,  2.69it/s]Evaluating:  85%|████████▍ | 800/946 [04:55<00:54,  2.69it/s]Evaluating:  85%|████████▍ | 801/946 [04:56<00:54,  2.67it/s]Evaluating:  85%|████████▍ | 802/946 [04:56<00:53,  2.70it/s]Evaluating:  85%|████████▍ | 803/946 [04:56<00:52,  2.70it/s]Evaluating:  85%|████████▍ | 804/946 [04:57<00:52,  2.69it/s]Evaluating:  85%|████████▌ | 805/946 [04:57<00:52,  2.68it/s]Evaluating:  85%|████████▌ | 806/946 [04:58<00:51,  2.69it/s]Evaluating:  85%|████████▌ | 807/946 [04:58<00:51,  2.71it/s]Evaluating:  85%|████████▌ | 808/946 [04:58<00:50,  2.71it/s]Evaluating:  86%|████████▌ | 809/946 [04:59<00:50,  2.69it/s]Evaluating:  86%|████████▌ | 810/946 [04:59<00:50,  2.69it/s]Evaluating:  86%|████████▌ | 811/946 [04:59<00:50,  2.69it/s]Evaluating:  86%|████████▌ | 812/946 [05:00<00:49,  2.70it/s]Evaluating:  86%|████████▌ | 813/946 [05:00<00:49,  2.70it/s]Evaluating:  86%|████████▌ | 814/946 [05:01<00:49,  2.69it/s]Evaluating:  86%|████████▌ | 815/946 [05:01<00:48,  2.69it/s]Evaluating:  86%|████████▋ | 816/946 [05:01<00:48,  2.69it/s]Evaluating:  86%|████████▋ | 817/946 [05:02<00:48,  2.69it/s]Evaluating:  86%|████████▋ | 818/946 [05:02<00:47,  2.68it/s]Evaluating:  87%|████████▋ | 819/946 [05:02<00:47,  2.68it/s]Evaluating:  87%|████████▋ | 820/946 [05:03<00:46,  2.69it/s]Evaluating:  87%|████████▋ | 821/946 [05:03<00:46,  2.68it/s]Evaluating:  87%|████████▋ | 822/946 [05:04<00:46,  2.68it/s]Evaluating:  87%|████████▋ | 823/946 [05:04<00:46,  2.67it/s]Evaluating:  87%|████████▋ | 824/946 [05:04<00:45,  2.68it/s]Evaluating:  87%|████████▋ | 825/946 [05:05<00:44,  2.69it/s]Evaluating:  87%|████████▋ | 826/946 [05:05<00:44,  2.70it/s]Evaluating:  87%|████████▋ | 827/946 [05:05<00:44,  2.69it/s]Evaluating:  88%|████████▊ | 828/946 [05:06<00:43,  2.68it/s]Evaluating:  88%|████████▊ | 829/946 [05:06<00:43,  2.67it/s]Evaluating:  88%|████████▊ | 830/946 [05:07<00:43,  2.66it/s]Evaluating:  88%|████████▊ | 831/946 [05:07<00:43,  2.66it/s]Evaluating:  88%|████████▊ | 832/946 [05:07<00:42,  2.67it/s]Evaluating:  88%|████████▊ | 833/946 [05:08<00:42,  2.68it/s]Evaluating:  88%|████████▊ | 834/946 [05:08<00:41,  2.69it/s]Evaluating:  88%|████████▊ | 835/946 [05:08<00:41,  2.70it/s]Evaluating:  88%|████████▊ | 836/946 [05:09<00:40,  2.70it/s]Evaluating:  88%|████████▊ | 837/946 [05:09<00:40,  2.70it/s]Evaluating:  89%|████████▊ | 838/946 [05:09<00:40,  2.69it/s]Evaluating:  89%|████████▊ | 839/946 [05:10<00:39,  2.70it/s]Evaluating:  89%|████████▉ | 840/946 [05:10<00:39,  2.71it/s]Evaluating:  89%|████████▉ | 841/946 [05:11<00:38,  2.70it/s]Evaluating:  89%|████████▉ | 842/946 [05:11<00:38,  2.70it/s]Evaluating:  89%|████████▉ | 843/946 [05:11<00:38,  2.69it/s]Evaluating:  89%|████████▉ | 844/946 [05:12<00:37,  2.69it/s]Evaluating:  89%|████████▉ | 845/946 [05:12<00:37,  2.71it/s]Evaluating:  89%|████████▉ | 846/946 [05:12<00:36,  2.70it/s]Evaluating:  90%|████████▉ | 847/946 [05:13<00:36,  2.70it/s]Evaluating:  90%|████████▉ | 848/946 [05:13<00:36,  2.70it/s]Evaluating:  90%|████████▉ | 849/946 [05:14<00:35,  2.70it/s]Evaluating:  90%|████████▉ | 850/946 [05:14<00:35,  2.70it/s]Evaluating:  90%|████████▉ | 851/946 [05:14<00:35,  2.70it/s]Evaluating:  90%|█████████ | 852/946 [05:15<00:34,  2.71it/s]Evaluating:  90%|█████████ | 853/946 [05:15<00:34,  2.70it/s]Evaluating:  90%|█████████ | 854/946 [05:15<00:34,  2.70it/s]Evaluating:  90%|█████████ | 855/946 [05:16<00:33,  2.70it/s]Evaluating:  90%|█████████ | 856/946 [05:16<00:33,  2.69it/s]Evaluating:  91%|█████████ | 857/946 [05:17<00:33,  2.69it/s]Evaluating:  91%|█████████ | 858/946 [05:17<00:32,  2.68it/s]Evaluating:  91%|█████████ | 859/946 [05:17<00:32,  2.68it/s]Evaluating:  91%|█████████ | 860/946 [05:18<00:31,  2.69it/s]Evaluating:  91%|█████████ | 861/946 [05:18<00:31,  2.68it/s]Evaluating:  91%|█████████ | 862/946 [05:18<00:31,  2.67it/s]Evaluating:  91%|█████████ | 863/946 [05:19<00:30,  2.69it/s]Evaluating:  91%|█████████▏| 864/946 [05:19<00:30,  2.69it/s]Evaluating:  91%|█████████▏| 865/946 [05:19<00:30,  2.69it/s]Evaluating:  92%|█████████▏| 866/946 [05:20<00:29,  2.69it/s]Evaluating:  92%|█████████▏| 867/946 [05:20<00:29,  2.64it/s]Evaluating:  92%|█████████▏| 868/946 [05:21<00:29,  2.64it/s]Evaluating:  92%|█████████▏| 869/946 [05:21<00:28,  2.66it/s]Evaluating:  92%|█████████▏| 870/946 [05:21<00:28,  2.67it/s]Evaluating:  92%|█████████▏| 871/946 [05:22<00:27,  2.69it/s]Evaluating:  92%|█████████▏| 872/946 [05:22<00:27,  2.69it/s]Evaluating:  92%|█████████▏| 873/946 [05:22<00:26,  2.70it/s]Evaluating:  92%|█████████▏| 874/946 [05:23<00:26,  2.71it/s]Evaluating:  92%|█████████▏| 875/946 [05:23<00:26,  2.70it/s]Evaluating:  93%|█████████▎| 876/946 [05:24<00:25,  2.71it/s]Evaluating:  93%|█████████▎| 877/946 [05:24<00:25,  2.72it/s]Evaluating:  93%|█████████▎| 878/946 [05:24<00:25,  2.71it/s]Evaluating:  93%|█████████▎| 879/946 [05:25<00:24,  2.70it/s]Evaluating:  93%|█████████▎| 880/946 [05:25<00:24,  2.71it/s]Evaluating:  93%|█████████▎| 881/946 [05:25<00:24,  2.69it/s]Evaluating:  93%|█████████▎| 882/946 [05:26<00:23,  2.69it/s]Evaluating:  93%|█████████▎| 883/946 [05:26<00:23,  2.68it/s]Evaluating:  93%|█████████▎| 884/946 [05:27<00:23,  2.69it/s]Evaluating:  94%|█████████▎| 885/946 [05:28<00:35,  1.70it/s]Evaluating:  94%|█████████▎| 886/946 [05:28<00:31,  1.92it/s]Evaluating:  94%|█████████▍| 887/946 [05:28<00:28,  2.10it/s]Evaluating:  94%|█████████▍| 888/946 [05:29<00:25,  2.25it/s]Evaluating:  94%|█████████▍| 889/946 [05:29<00:24,  2.37it/s]Evaluating:  94%|█████████▍| 890/946 [05:29<00:22,  2.46it/s]Evaluating:  94%|█████████▍| 891/946 [05:30<00:21,  2.54it/s]Evaluating:  94%|█████████▍| 892/946 [05:30<00:20,  2.60it/s]Evaluating:  94%|█████████▍| 893/946 [05:31<00:20,  2.63it/s]Evaluating:  95%|█████████▍| 894/946 [05:31<00:19,  2.66it/s]Evaluating:  95%|█████████▍| 895/946 [05:31<00:19,  2.68it/s]Evaluating:  95%|█████████▍| 896/946 [05:32<00:18,  2.70it/s]Evaluating:  95%|█████████▍| 897/946 [05:32<00:18,  2.71it/s]Evaluating:  95%|█████████▍| 898/946 [05:32<00:17,  2.71it/s]Evaluating:  95%|█████████▌| 899/946 [05:33<00:17,  2.71it/s]Evaluating:  95%|█████████▌| 900/946 [05:33<00:17,  2.70it/s]Evaluating:  95%|█████████▌| 901/946 [05:34<00:16,  2.69it/s]Evaluating:  95%|█████████▌| 902/946 [05:34<00:16,  2.69it/s]Evaluating:  95%|█████████▌| 903/946 [05:34<00:15,  2.69it/s]Evaluating:  96%|█████████▌| 904/946 [05:35<00:15,  2.70it/s]Evaluating:  96%|█████████▌| 905/946 [05:35<00:15,  2.71it/s]Evaluating:  96%|█████████▌| 906/946 [05:35<00:14,  2.71it/s]Evaluating:  96%|█████████▌| 907/946 [05:36<00:14,  2.71it/s]Evaluating:  96%|█████████▌| 908/946 [05:36<00:13,  2.72it/s]Evaluating:  96%|█████████▌| 909/946 [05:36<00:13,  2.73it/s]Evaluating:  96%|█████████▌| 910/946 [05:37<00:13,  2.73it/s]Evaluating:  96%|█████████▋| 911/946 [05:37<00:12,  2.72it/s]Evaluating:  96%|█████████▋| 912/946 [05:38<00:12,  2.72it/s]Evaluating:  97%|█████████▋| 913/946 [05:38<00:12,  2.73it/s]Evaluating:  97%|█████████▋| 914/946 [05:38<00:11,  2.72it/s]Evaluating:  97%|█████████▋| 915/946 [05:39<00:11,  2.72it/s]Evaluating:  97%|█████████▋| 916/946 [05:39<00:11,  2.72it/s]Evaluating:  97%|█████████▋| 917/946 [05:39<00:10,  2.72it/s]Evaluating:  97%|█████████▋| 918/946 [05:40<00:10,  2.72it/s]Evaluating:  97%|█████████▋| 919/946 [05:40<00:09,  2.71it/s]Evaluating:  97%|█████████▋| 920/946 [05:41<00:09,  2.71it/s]Evaluating:  97%|█████████▋| 921/946 [05:41<00:09,  2.70it/s]Evaluating:  97%|█████████▋| 922/946 [05:41<00:08,  2.71it/s]Evaluating:  98%|█████████▊| 923/946 [05:42<00:08,  2.71it/s]Evaluating:  98%|█████████▊| 924/946 [05:42<00:08,  2.71it/s]Evaluating:  98%|█████████▊| 925/946 [05:42<00:07,  2.72it/s]Evaluating:  98%|█████████▊| 926/946 [05:43<00:07,  2.72it/s]Evaluating:  98%|█████████▊| 927/946 [05:43<00:06,  2.73it/s]Evaluating:  98%|█████████▊| 928/946 [05:43<00:06,  2.73it/s]Evaluating:  98%|█████████▊| 929/946 [05:44<00:06,  2.72it/s]Evaluating:  98%|█████████▊| 930/946 [05:44<00:05,  2.72it/s]Evaluating:  98%|█████████▊| 931/946 [05:45<00:05,  2.71it/s]Evaluating:  99%|█████████▊| 932/946 [05:45<00:05,  2.71it/s]Evaluating:  99%|█████████▊| 933/946 [05:45<00:04,  2.72it/s]Evaluating:  99%|█████████▊| 934/946 [05:46<00:04,  2.73it/s]Evaluating:  99%|█████████▉| 935/946 [05:46<00:04,  2.74it/s]Evaluating:  99%|█████████▉| 936/946 [05:46<00:03,  2.74it/s]Evaluating:  99%|█████████▉| 937/946 [05:47<00:03,  2.74it/s]Evaluating:  99%|█████████▉| 938/946 [05:47<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 939/946 [05:48<00:02,  2.74it/s]Evaluating:  99%|█████████▉| 940/946 [05:48<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 941/946 [05:48<00:01,  2.72it/s]Evaluating: 100%|█████████▉| 942/946 [05:49<00:01,  2.71it/s]Evaluating: 100%|█████████▉| 943/946 [05:49<00:01,  2.71it/s]Evaluating: 100%|█████████▉| 944/946 [05:49<00:00,  2.72it/s]Evaluating: 100%|█████████▉| 945/946 [05:50<00:00,  2.73it/s]Evaluating: 100%|██████████| 946/946 [05:50<00:00,  2.97it/s]Evaluating: 100%|██████████| 946/946 [05:50<00:00,  2.70it/s]
05/10/2022 00:23:25 - INFO - __main__ -     Evaluation done in total 350.485408 secs (0.046324 sec per example)
05/10/2022 00:23:48 - INFO - __main__ -   Results: {'exact': 41.312089971883786, 'f1': 61.406447449474584, 'total': 5335, 'HasAns_exact': 41.312089971883786, 'HasAns_f1': 61.406447449474584, 'HasAns_total': 5335, 'best_exact': 41.312089971883786, 'best_exact_thresh': 0.0, 'best_f1': 61.406447449474584, 'best_f1_thresh': 0.0}
  hi 
2022-05-10 00:23:52.034232: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/10/2022 00:23:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'qa_outputs.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'qa_outputs.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.15.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:24:18 - INFO - __main__ -   lang2id = None
05/10/2022 00:24:22 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='hi', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-hi-question-hi.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/10/2022 00:24:22 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/10/2022 00:24:22 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'qa_outputs.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'qa_outputs.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.15.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:24:50 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/2038 [00:00<?, ?it/s]  2%|▏         | 43/2038 [00:00<00:04, 424.17it/s]  5%|▌         | 108/2038 [00:00<00:03, 549.42it/s]  9%|▉         | 191/2038 [00:00<00:02, 672.21it/s] 13%|█▎        | 264/2038 [00:00<00:02, 693.80it/s] 17%|█▋        | 356/2038 [00:00<00:02, 772.56it/s] 22%|██▏       | 454/2038 [00:00<00:01, 841.69it/s] 28%|██▊       | 573/2038 [00:00<00:01, 953.29it/s] 33%|███▎      | 669/2038 [00:00<00:01, 926.66it/s] 37%|███▋      | 762/2038 [00:00<00:01, 833.53it/s] 43%|████▎     | 871/2038 [00:01<00:01, 902.88it/s] 47%|████▋     | 964/2038 [00:01<00:01, 887.70it/s] 52%|█████▏    | 1054/2038 [00:01<00:01, 822.31it/s] 56%|█████▌    | 1138/2038 [00:01<00:01, 789.45it/s] 60%|█████▉    | 1219/2038 [00:01<00:01, 780.71it/s] 64%|██████▍   | 1310/2038 [00:01<00:00, 811.36it/s] 70%|███████   | 1433/2038 [00:01<00:00, 924.21it/s] 77%|███████▋  | 1578/2038 [00:01<00:00, 1071.06it/s] 83%|████████▎ | 1700/2038 [00:01<00:00, 1113.13it/s] 93%|█████████▎| 1895/2038 [00:02<00:00, 1357.30it/s]100%|██████████| 2038/2038 [00:02<00:00, 983.06it/s] 
convert squad examples to features:   0%|          | 0/4918 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/4918 [00:00<18:54,  4.33it/s]convert squad examples to features:   1%|          | 33/4918 [00:00<01:02, 78.40it/s]convert squad examples to features:   1%|▏         | 65/4918 [00:00<00:49, 98.45it/s]convert squad examples to features:   2%|▏         | 97/4918 [00:01<01:04, 74.82it/s]convert squad examples to features:   3%|▎         | 129/4918 [00:01<00:58, 82.09it/s]convert squad examples to features:   3%|▎         | 161/4918 [00:02<00:57, 82.25it/s]convert squad examples to features:   4%|▍         | 193/4918 [00:02<00:53, 88.37it/s]convert squad examples to features:   5%|▍         | 225/4918 [00:02<00:59, 78.73it/s]convert squad examples to features:   5%|▌         | 257/4918 [00:03<00:52, 88.65it/s]convert squad examples to features:   6%|▌         | 289/4918 [00:03<00:46, 99.79it/s]convert squad examples to features:   7%|▋         | 321/4918 [00:03<00:42, 109.39it/s]convert squad examples to features:   7%|▋         | 353/4918 [00:03<00:43, 104.59it/s]convert squad examples to features:   8%|▊         | 385/4918 [00:04<00:42, 106.11it/s]convert squad examples to features:   8%|▊         | 417/4918 [00:04<00:33, 132.58it/s]convert squad examples to features:   9%|▉         | 449/4918 [00:04<00:30, 144.78it/s]convert squad examples to features:  10%|▉         | 481/4918 [00:04<00:29, 151.50it/s]convert squad examples to features:  10%|█         | 513/4918 [00:04<00:31, 141.15it/s]convert squad examples to features:  11%|█         | 545/4918 [00:05<00:32, 133.64it/s]convert squad examples to features:  12%|█▏        | 577/4918 [00:05<00:34, 124.14it/s]convert squad examples to features:  12%|█▏        | 609/4918 [00:05<00:33, 127.17it/s]convert squad examples to features:  13%|█▎        | 641/4918 [00:06<00:41, 102.47it/s]convert squad examples to features:  14%|█▎        | 673/4918 [00:06<00:38, 109.44it/s]convert squad examples to features:  14%|█▍        | 705/4918 [00:06<00:32, 130.28it/s]convert squad examples to features:  15%|█▍        | 737/4918 [00:06<00:35, 118.53it/s]convert squad examples to features:  16%|█▌        | 769/4918 [00:07<00:30, 136.44it/s]convert squad examples to features:  16%|█▋        | 801/4918 [00:07<00:37, 109.88it/s]convert squad examples to features:  17%|█▋        | 833/4918 [00:07<00:32, 126.39it/s]convert squad examples to features:  18%|█▊        | 865/4918 [00:07<00:27, 148.38it/s]convert squad examples to features:  18%|█▊        | 897/4918 [00:08<00:53, 75.13it/s] convert squad examples to features:  19%|█▉        | 929/4918 [00:08<00:46, 85.48it/s]convert squad examples to features:  20%|█▉        | 961/4918 [00:09<00:43, 90.74it/s]convert squad examples to features:  20%|██        | 993/4918 [00:09<00:36, 108.31it/s]convert squad examples to features:  21%|██        | 1025/4918 [00:09<00:36, 108.11it/s]convert squad examples to features:  21%|██▏       | 1057/4918 [00:10<00:52, 73.37it/s] convert squad examples to features:  22%|██▏       | 1089/4918 [00:10<00:43, 87.03it/s]convert squad examples to features:  23%|██▎       | 1121/4918 [00:10<00:42, 89.57it/s]convert squad examples to features:  23%|██▎       | 1153/4918 [00:11<00:36, 102.64it/s]convert squad examples to features:  24%|██▍       | 1185/4918 [00:11<00:34, 107.61it/s]convert squad examples to features:  25%|██▍       | 1217/4918 [00:11<00:29, 123.74it/s]convert squad examples to features:  25%|██▌       | 1249/4918 [00:11<00:27, 134.16it/s]convert squad examples to features:  26%|██▌       | 1281/4918 [00:11<00:23, 153.40it/s]convert squad examples to features:  27%|██▋       | 1313/4918 [00:12<00:25, 141.01it/s]convert squad examples to features:  27%|██▋       | 1345/4918 [00:12<00:23, 154.69it/s]convert squad examples to features:  28%|██▊       | 1377/4918 [00:12<00:22, 159.96it/s]convert squad examples to features:  29%|██▊       | 1409/4918 [00:12<00:22, 157.32it/s]convert squad examples to features:  29%|██▉       | 1441/4918 [00:12<00:22, 154.50it/s]convert squad examples to features:  30%|██▉       | 1473/4918 [00:13<00:21, 162.87it/s]convert squad examples to features:  31%|███       | 1505/4918 [00:13<00:25, 134.07it/s]convert squad examples to features:  31%|███▏      | 1537/4918 [00:13<00:24, 135.38it/s]convert squad examples to features:  32%|███▏      | 1569/4918 [00:13<00:24, 134.77it/s]convert squad examples to features:  33%|███▎      | 1601/4918 [00:14<00:23, 139.98it/s]convert squad examples to features:  33%|███▎      | 1633/4918 [00:14<00:24, 134.22it/s]convert squad examples to features:  34%|███▍      | 1665/4918 [00:14<00:22, 145.52it/s]convert squad examples to features:  35%|███▍      | 1697/4918 [00:14<00:22, 142.39it/s]convert squad examples to features:  35%|███▌      | 1729/4918 [00:15<00:21, 149.58it/s]convert squad examples to features:  36%|███▌      | 1761/4918 [00:15<00:22, 139.53it/s]convert squad examples to features:  36%|███▋      | 1793/4918 [00:15<00:27, 114.44it/s]convert squad examples to features:  37%|███▋      | 1825/4918 [00:16<00:35, 86.98it/s] convert squad examples to features:  38%|███▊      | 1889/4918 [00:16<00:24, 126.07it/s]convert squad examples to features:  39%|███▉      | 1921/4918 [00:16<00:21, 138.74it/s]convert squad examples to features:  40%|███▉      | 1953/4918 [00:17<00:34, 85.19it/s] convert squad examples to features:  40%|████      | 1985/4918 [00:17<00:30, 94.86it/s]convert squad examples to features:  41%|████      | 2017/4918 [00:17<00:27, 104.48it/s]convert squad examples to features:  42%|████▏     | 2049/4918 [00:18<00:25, 112.75it/s]convert squad examples to features:  42%|████▏     | 2081/4918 [00:18<00:27, 101.61it/s]convert squad examples to features:  43%|████▎     | 2113/4918 [00:18<00:27, 103.31it/s]convert squad examples to features:  44%|████▎     | 2145/4918 [00:19<00:24, 113.43it/s]convert squad examples to features:  44%|████▍     | 2177/4918 [00:19<00:22, 123.86it/s]convert squad examples to features:  45%|████▍     | 2209/4918 [00:19<00:22, 120.30it/s]convert squad examples to features:  46%|████▌     | 2241/4918 [00:19<00:20, 130.45it/s]convert squad examples to features:  46%|████▌     | 2273/4918 [00:19<00:18, 143.09it/s]convert squad examples to features:  47%|████▋     | 2305/4918 [00:20<00:17, 152.25it/s]convert squad examples to features:  48%|████▊     | 2337/4918 [00:20<00:22, 113.52it/s]convert squad examples to features:  48%|████▊     | 2369/4918 [00:20<00:24, 104.96it/s]convert squad examples to features:  49%|████▉     | 2401/4918 [00:21<00:20, 120.19it/s]convert squad examples to features:  49%|████▉     | 2433/4918 [00:21<00:19, 124.35it/s]convert squad examples to features:  50%|█████     | 2465/4918 [00:21<00:20, 119.62it/s]convert squad examples to features:  51%|█████     | 2497/4918 [00:22<00:33, 72.06it/s] convert squad examples to features:  51%|█████▏    | 2529/4918 [00:22<00:33, 70.86it/s]convert squad examples to features:  52%|█████▏    | 2561/4918 [00:23<00:31, 74.22it/s]convert squad examples to features:  53%|█████▎    | 2593/4918 [00:23<00:26, 88.18it/s]convert squad examples to features:  53%|█████▎    | 2625/4918 [00:23<00:21, 108.24it/s]convert squad examples to features:  54%|█████▍    | 2657/4918 [00:24<00:22, 98.59it/s] convert squad examples to features:  55%|█████▍    | 2689/4918 [00:24<00:20, 109.76it/s]convert squad examples to features:  55%|█████▌    | 2721/4918 [00:24<00:24, 90.61it/s] convert squad examples to features:  56%|█████▌    | 2753/4918 [00:25<00:22, 96.03it/s]convert squad examples to features:  57%|█████▋    | 2785/4918 [00:25<00:27, 77.86it/s]convert squad examples to features:  57%|█████▋    | 2817/4918 [00:26<00:27, 76.05it/s]convert squad examples to features:  58%|█████▊    | 2849/4918 [00:26<00:29, 71.20it/s]convert squad examples to features:  59%|█████▊    | 2881/4918 [00:26<00:24, 84.23it/s]convert squad examples to features:  59%|█████▉    | 2913/4918 [00:27<00:21, 93.36it/s]convert squad examples to features:  60%|█████▉    | 2945/4918 [00:27<00:19, 103.38it/s]convert squad examples to features:  61%|██████    | 2977/4918 [00:27<00:17, 112.32it/s]convert squad examples to features:  61%|██████    | 3009/4918 [00:27<00:15, 126.06it/s]convert squad examples to features:  62%|██████▏   | 3041/4918 [00:28<00:16, 115.73it/s]convert squad examples to features:  62%|██████▏   | 3073/4918 [00:29<00:28, 64.84it/s] convert squad examples to features:  63%|██████▎   | 3105/4918 [00:29<00:26, 68.81it/s]convert squad examples to features:  64%|██████▍   | 3137/4918 [00:29<00:20, 86.39it/s]convert squad examples to features:  64%|██████▍   | 3169/4918 [00:29<00:17, 102.72it/s]convert squad examples to features:  65%|██████▌   | 3201/4918 [00:29<00:15, 113.26it/s]convert squad examples to features:  66%|██████▌   | 3233/4918 [00:30<00:13, 126.53it/s]convert squad examples to features:  66%|██████▋   | 3265/4918 [00:30<00:14, 118.05it/s]convert squad examples to features:  67%|██████▋   | 3297/4918 [00:30<00:14, 109.93it/s]convert squad examples to features:  68%|██████▊   | 3329/4918 [00:31<00:16, 97.08it/s] convert squad examples to features:  68%|██████▊   | 3361/4918 [00:31<00:14, 109.31it/s]convert squad examples to features:  69%|██████▉   | 3393/4918 [00:31<00:13, 113.46it/s]convert squad examples to features:  70%|██████▉   | 3425/4918 [00:31<00:13, 114.13it/s]convert squad examples to features:  70%|███████   | 3457/4918 [00:32<00:10, 136.64it/s]convert squad examples to features:  71%|███████   | 3489/4918 [00:32<00:09, 150.40it/s]convert squad examples to features:  72%|███████▏  | 3521/4918 [00:32<00:08, 161.01it/s]convert squad examples to features:  72%|███████▏  | 3553/4918 [00:32<00:07, 173.52it/s]convert squad examples to features:  73%|███████▎  | 3585/4918 [00:32<00:08, 155.19it/s]convert squad examples to features:  74%|███████▎  | 3617/4918 [00:33<00:14, 88.20it/s] convert squad examples to features:  74%|███████▍  | 3649/4918 [00:33<00:12, 103.96it/s]convert squad examples to features:  75%|███████▍  | 3681/4918 [00:33<00:10, 117.32it/s]convert squad examples to features:  75%|███████▌  | 3713/4918 [00:34<00:08, 138.27it/s]convert squad examples to features:  76%|███████▌  | 3745/4918 [00:34<00:08, 142.14it/s]convert squad examples to features:  77%|███████▋  | 3777/4918 [00:34<00:07, 147.25it/s]convert squad examples to features:  77%|███████▋  | 3809/4918 [00:34<00:08, 125.73it/s]convert squad examples to features:  78%|███████▊  | 3841/4918 [00:34<00:07, 139.93it/s]convert squad examples to features:  79%|███████▉  | 3873/4918 [00:35<00:08, 123.70it/s]convert squad examples to features:  79%|███████▉  | 3905/4918 [00:35<00:08, 117.68it/s]convert squad examples to features:  80%|████████  | 3937/4918 [00:35<00:07, 135.07it/s]convert squad examples to features:  81%|████████  | 3969/4918 [00:36<00:07, 129.08it/s]convert squad examples to features:  81%|████████▏ | 4001/4918 [00:36<00:07, 128.05it/s]convert squad examples to features:  82%|████████▏ | 4033/4918 [00:36<00:05, 151.12it/s]convert squad examples to features:  83%|████████▎ | 4065/4918 [00:36<00:05, 144.80it/s]convert squad examples to features:  83%|████████▎ | 4097/4918 [00:36<00:06, 135.16it/s]convert squad examples to features:  84%|████████▍ | 4129/4918 [00:37<00:05, 139.57it/s]convert squad examples to features:  85%|████████▍ | 4161/4918 [00:37<00:05, 136.01it/s]convert squad examples to features:  85%|████████▌ | 4193/4918 [00:37<00:05, 132.19it/s]convert squad examples to features:  86%|████████▌ | 4225/4918 [00:37<00:05, 136.50it/s]convert squad examples to features:  87%|████████▋ | 4257/4918 [00:38<00:04, 146.05it/s]convert squad examples to features:  87%|████████▋ | 4289/4918 [00:38<00:04, 155.20it/s]convert squad examples to features:  88%|████████▊ | 4321/4918 [00:38<00:03, 168.92it/s]convert squad examples to features:  89%|████████▊ | 4353/4918 [00:38<00:03, 186.56it/s]convert squad examples to features:  89%|████████▉ | 4385/4918 [00:38<00:03, 160.68it/s]convert squad examples to features:  90%|████████▉ | 4417/4918 [00:39<00:03, 137.38it/s]convert squad examples to features:  90%|█████████ | 4449/4918 [00:39<00:03, 137.82it/s]convert squad examples to features:  91%|█████████ | 4481/4918 [00:39<00:03, 111.50it/s]convert squad examples to features:  92%|█████████▏| 4513/4918 [00:39<00:03, 132.09it/s]convert squad examples to features:  92%|█████████▏| 4545/4918 [00:40<00:03, 114.49it/s]convert squad examples to features:  93%|█████████▎| 4577/4918 [00:40<00:02, 137.21it/s]convert squad examples to features:  94%|█████████▍| 4641/4918 [00:40<00:01, 164.27it/s]convert squad examples to features:  95%|█████████▌| 4673/4918 [00:40<00:01, 173.93it/s]convert squad examples to features:  96%|█████████▌| 4705/4918 [00:40<00:01, 182.66it/s]convert squad examples to features:  96%|█████████▋| 4737/4918 [00:41<00:00, 197.58it/s]convert squad examples to features:  97%|█████████▋| 4769/4918 [00:41<00:00, 199.83it/s]convert squad examples to features:  98%|█████████▊| 4801/4918 [00:41<00:00, 204.00it/s]convert squad examples to features:  98%|█████████▊| 4833/4918 [00:41<00:00, 215.18it/s]convert squad examples to features:  99%|█████████▉| 4865/4918 [00:41<00:00, 206.65it/s]convert squad examples to features: 100%|█████████▉| 4897/4918 [00:41<00:00, 214.76it/s]convert squad examples to features: 100%|██████████| 4918/4918 [00:41<00:00, 117.55it/s]
add example index and unique id:   0%|          | 0/4918 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 4918/4918 [00:00<00:00, 647525.96it/s]
05/10/2022 00:25:35 - INFO - __main__ -   Saving features into cached file ./cached_test-context-hi-question-hi.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_hi
05/10/2022 00:25:45 - INFO - __main__ -   ***** Running evaluation  *****
05/10/2022 00:25:45 - INFO - __main__ -     Num examples = 7084
05/10/2022 00:25:45 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/886 [00:00<?, ?it/s]Evaluating:   0%|          | 1/886 [00:01<15:08,  1.03s/it]Evaluating:   0%|          | 2/886 [00:01<09:21,  1.57it/s]Evaluating:   0%|          | 3/886 [00:01<07:30,  1.96it/s]Evaluating:   0%|          | 4/886 [00:02<06:37,  2.22it/s]Evaluating:   1%|          | 5/886 [00:02<06:09,  2.38it/s]Evaluating:   1%|          | 6/886 [00:02<05:53,  2.49it/s]Evaluating:   1%|          | 7/886 [00:03<05:42,  2.57it/s]Evaluating:   1%|          | 8/886 [00:03<05:35,  2.62it/s]Evaluating:   1%|          | 9/886 [00:03<05:29,  2.66it/s]Evaluating:   1%|          | 10/886 [00:04<05:25,  2.69it/s]Evaluating:   1%|          | 11/886 [00:04<05:21,  2.72it/s]Evaluating:   1%|▏         | 12/886 [00:05<05:21,  2.72it/s]Evaluating:   1%|▏         | 13/886 [00:05<05:20,  2.73it/s]Evaluating:   2%|▏         | 14/886 [00:05<05:19,  2.73it/s]Evaluating:   2%|▏         | 15/886 [00:06<05:18,  2.74it/s]Evaluating:   2%|▏         | 16/886 [00:06<05:18,  2.74it/s]Evaluating:   2%|▏         | 17/886 [00:06<05:18,  2.73it/s]Evaluating:   2%|▏         | 18/886 [00:07<05:17,  2.73it/s]Evaluating:   2%|▏         | 19/886 [00:07<05:19,  2.71it/s]Evaluating:   2%|▏         | 20/886 [00:07<05:18,  2.72it/s]Evaluating:   2%|▏         | 21/886 [00:08<05:18,  2.71it/s]Evaluating:   2%|▏         | 22/886 [00:08<05:17,  2.72it/s]Evaluating:   3%|▎         | 23/886 [00:09<05:18,  2.71it/s]Evaluating:   3%|▎         | 24/886 [00:09<05:17,  2.72it/s]Evaluating:   3%|▎         | 25/886 [00:09<05:17,  2.71it/s]Evaluating:   3%|▎         | 26/886 [00:10<05:15,  2.72it/s]Evaluating:   3%|▎         | 27/886 [00:10<05:15,  2.73it/s]Evaluating:   3%|▎         | 28/886 [00:10<05:13,  2.74it/s]Evaluating:   3%|▎         | 29/886 [00:11<05:12,  2.74it/s]Evaluating:   3%|▎         | 30/886 [00:11<05:12,  2.74it/s]Evaluating:   3%|▎         | 31/886 [00:11<05:13,  2.72it/s]Evaluating:   4%|▎         | 32/886 [00:12<05:14,  2.72it/s]Evaluating:   4%|▎         | 33/886 [00:12<05:13,  2.72it/s]Evaluating:   4%|▍         | 34/886 [00:13<05:12,  2.73it/s]Evaluating:   4%|▍         | 35/886 [00:13<05:10,  2.74it/s]Evaluating:   4%|▍         | 36/886 [00:13<05:11,  2.73it/s]Evaluating:   4%|▍         | 37/886 [00:14<05:12,  2.71it/s]Evaluating:   4%|▍         | 38/886 [00:14<05:11,  2.72it/s]Evaluating:   4%|▍         | 39/886 [00:14<05:10,  2.73it/s]Evaluating:   5%|▍         | 40/886 [00:15<05:11,  2.71it/s]Evaluating:   5%|▍         | 41/886 [00:15<05:10,  2.72it/s]Evaluating:   5%|▍         | 42/886 [00:16<05:10,  2.72it/s]Evaluating:   5%|▍         | 43/886 [00:16<05:09,  2.73it/s]Evaluating:   5%|▍         | 44/886 [00:16<05:09,  2.72it/s]Evaluating:   5%|▌         | 45/886 [00:17<05:07,  2.73it/s]Evaluating:   5%|▌         | 46/886 [00:17<05:08,  2.72it/s]Evaluating:   5%|▌         | 47/886 [00:17<05:09,  2.71it/s]Evaluating:   5%|▌         | 48/886 [00:18<05:09,  2.71it/s]Evaluating:   6%|▌         | 49/886 [00:18<05:09,  2.71it/s]Evaluating:   6%|▌         | 50/886 [00:18<05:07,  2.72it/s]Evaluating:   6%|▌         | 51/886 [00:19<05:04,  2.74it/s]Evaluating:   6%|▌         | 52/886 [00:19<05:05,  2.73it/s]Evaluating:   6%|▌         | 53/886 [00:20<05:06,  2.72it/s]Evaluating:   6%|▌         | 54/886 [00:20<05:05,  2.72it/s]Evaluating:   6%|▌         | 55/886 [00:20<05:05,  2.72it/s]Evaluating:   6%|▋         | 56/886 [00:21<05:05,  2.71it/s]Evaluating:   6%|▋         | 57/886 [00:21<05:03,  2.73it/s]Evaluating:   7%|▋         | 58/886 [00:21<05:02,  2.74it/s]Evaluating:   7%|▋         | 59/886 [00:22<05:03,  2.73it/s]Evaluating:   7%|▋         | 60/886 [00:22<05:05,  2.71it/s]Evaluating:   7%|▋         | 61/886 [00:23<05:03,  2.72it/s]Evaluating:   7%|▋         | 62/886 [00:23<05:04,  2.71it/s]Evaluating:   7%|▋         | 63/886 [00:23<05:03,  2.71it/s]Evaluating:   7%|▋         | 64/886 [00:24<05:03,  2.71it/s]Evaluating:   7%|▋         | 65/886 [00:24<05:03,  2.71it/s]Evaluating:   7%|▋         | 66/886 [00:24<05:03,  2.70it/s]Evaluating:   8%|▊         | 67/886 [00:25<05:02,  2.71it/s]Evaluating:   8%|▊         | 68/886 [00:25<05:00,  2.72it/s]Evaluating:   8%|▊         | 69/886 [00:25<05:01,  2.71it/s]Evaluating:   8%|▊         | 70/886 [00:26<05:10,  2.63it/s]Evaluating:   8%|▊         | 71/886 [00:26<05:07,  2.65it/s]Evaluating:   8%|▊         | 72/886 [00:27<05:05,  2.67it/s]Evaluating:   8%|▊         | 73/886 [00:27<05:02,  2.69it/s]Evaluating:   8%|▊         | 74/886 [00:27<05:01,  2.69it/s]Evaluating:   8%|▊         | 75/886 [00:28<05:01,  2.69it/s]Evaluating:   9%|▊         | 76/886 [00:28<04:59,  2.70it/s]Evaluating:   9%|▊         | 77/886 [00:28<04:57,  2.72it/s]Evaluating:   9%|▉         | 78/886 [00:29<04:58,  2.71it/s]Evaluating:   9%|▉         | 79/886 [00:29<04:57,  2.71it/s]Evaluating:   9%|▉         | 80/886 [00:30<04:57,  2.71it/s]Evaluating:   9%|▉         | 81/886 [00:30<04:57,  2.71it/s]Evaluating:   9%|▉         | 82/886 [00:30<04:56,  2.71it/s]Evaluating:   9%|▉         | 83/886 [00:31<04:56,  2.71it/s]Evaluating:   9%|▉         | 84/886 [00:31<04:55,  2.72it/s]Evaluating:  10%|▉         | 85/886 [00:31<04:54,  2.72it/s]Evaluating:  10%|▉         | 86/886 [00:32<04:52,  2.74it/s]Evaluating:  10%|▉         | 87/886 [00:32<04:51,  2.74it/s]Evaluating:  10%|▉         | 88/886 [00:32<04:52,  2.73it/s]Evaluating:  10%|█         | 89/886 [00:33<04:53,  2.72it/s]Evaluating:  10%|█         | 90/886 [00:33<04:50,  2.74it/s]Evaluating:  10%|█         | 91/886 [00:34<04:51,  2.73it/s]Evaluating:  10%|█         | 92/886 [00:34<04:49,  2.74it/s]Evaluating:  10%|█         | 93/886 [00:34<04:49,  2.74it/s]Evaluating:  11%|█         | 94/886 [00:35<04:49,  2.74it/s]Evaluating:  11%|█         | 95/886 [00:35<04:48,  2.74it/s]Evaluating:  11%|█         | 96/886 [00:35<04:49,  2.73it/s]Evaluating:  11%|█         | 97/886 [00:36<04:49,  2.73it/s]Evaluating:  11%|█         | 98/886 [00:36<04:48,  2.73it/s]Evaluating:  11%|█         | 99/886 [00:37<04:49,  2.72it/s]Evaluating:  11%|█▏        | 100/886 [00:37<04:50,  2.71it/s]Evaluating:  11%|█▏        | 101/886 [00:37<04:48,  2.72it/s]Evaluating:  12%|█▏        | 102/886 [00:38<04:47,  2.73it/s]Evaluating:  12%|█▏        | 103/886 [00:38<04:46,  2.73it/s]Evaluating:  12%|█▏        | 104/886 [00:38<04:48,  2.71it/s]Evaluating:  12%|█▏        | 105/886 [00:39<04:47,  2.72it/s]Evaluating:  12%|█▏        | 106/886 [00:39<04:47,  2.71it/s]Evaluating:  12%|█▏        | 107/886 [00:39<04:47,  2.71it/s]Evaluating:  12%|█▏        | 108/886 [00:40<04:46,  2.72it/s]Evaluating:  12%|█▏        | 109/886 [00:40<04:45,  2.72it/s]Evaluating:  12%|█▏        | 110/886 [00:41<04:43,  2.73it/s]Evaluating:  13%|█▎        | 111/886 [00:41<04:43,  2.73it/s]Evaluating:  13%|█▎        | 112/886 [00:41<04:43,  2.73it/s]Evaluating:  13%|█▎        | 113/886 [00:42<04:43,  2.73it/s]Evaluating:  13%|█▎        | 114/886 [00:42<04:44,  2.72it/s]Evaluating:  13%|█▎        | 115/886 [00:42<04:46,  2.70it/s]Evaluating:  13%|█▎        | 116/886 [00:43<04:44,  2.71it/s]Evaluating:  13%|█▎        | 117/886 [00:43<04:43,  2.71it/s]Evaluating:  13%|█▎        | 118/886 [00:44<04:42,  2.72it/s]Evaluating:  13%|█▎        | 119/886 [00:44<04:42,  2.72it/s]Evaluating:  14%|█▎        | 120/886 [00:44<04:42,  2.71it/s]Evaluating:  14%|█▎        | 121/886 [00:45<04:43,  2.70it/s]Evaluating:  14%|█▍        | 122/886 [00:45<04:42,  2.70it/s]Evaluating:  14%|█▍        | 123/886 [00:45<04:42,  2.70it/s]Evaluating:  14%|█▍        | 124/886 [00:46<04:42,  2.69it/s]Evaluating:  14%|█▍        | 125/886 [00:46<04:40,  2.71it/s]Evaluating:  14%|█▍        | 126/886 [00:46<04:39,  2.72it/s]Evaluating:  14%|█▍        | 127/886 [00:47<04:40,  2.71it/s]Evaluating:  14%|█▍        | 128/886 [00:47<04:41,  2.70it/s]Evaluating:  15%|█▍        | 129/886 [00:48<04:39,  2.70it/s]Evaluating:  15%|█▍        | 130/886 [00:48<04:39,  2.71it/s]Evaluating:  15%|█▍        | 131/886 [00:48<04:38,  2.71it/s]Evaluating:  15%|█▍        | 132/886 [00:49<04:38,  2.71it/s]Evaluating:  15%|█▌        | 133/886 [00:49<04:37,  2.72it/s]Evaluating:  15%|█▌        | 134/886 [00:49<04:37,  2.71it/s]Evaluating:  15%|█▌        | 135/886 [00:50<04:35,  2.72it/s]Evaluating:  15%|█▌        | 136/886 [00:50<04:35,  2.72it/s]Evaluating:  15%|█▌        | 137/886 [00:51<04:36,  2.71it/s]Evaluating:  16%|█▌        | 138/886 [00:51<04:35,  2.71it/s]Evaluating:  16%|█▌        | 139/886 [00:51<04:36,  2.71it/s]Evaluating:  16%|█▌        | 140/886 [00:52<04:35,  2.70it/s]Evaluating:  16%|█▌        | 141/886 [00:52<04:34,  2.71it/s]Evaluating:  16%|█▌        | 142/886 [00:52<04:34,  2.71it/s]Evaluating:  16%|█▌        | 143/886 [00:53<04:33,  2.72it/s]Evaluating:  16%|█▋        | 144/886 [00:53<04:32,  2.72it/s]Evaluating:  16%|█▋        | 145/886 [00:53<04:31,  2.73it/s]Evaluating:  16%|█▋        | 146/886 [00:54<04:30,  2.73it/s]Evaluating:  17%|█▋        | 147/886 [00:54<04:30,  2.73it/s]Evaluating:  17%|█▋        | 148/886 [00:55<04:31,  2.72it/s]Evaluating:  17%|█▋        | 149/886 [00:55<04:30,  2.72it/s]Evaluating:  17%|█▋        | 150/886 [00:55<04:31,  2.71it/s]Evaluating:  17%|█▋        | 151/886 [00:56<04:31,  2.71it/s]Evaluating:  17%|█▋        | 152/886 [00:56<04:30,  2.72it/s]Evaluating:  17%|█▋        | 153/886 [00:56<04:29,  2.72it/s]Evaluating:  17%|█▋        | 154/886 [00:57<04:28,  2.72it/s]Evaluating:  17%|█▋        | 155/886 [00:57<04:28,  2.72it/s]Evaluating:  18%|█▊        | 156/886 [00:58<04:26,  2.73it/s]Evaluating:  18%|█▊        | 157/886 [00:58<04:27,  2.72it/s]Evaluating:  18%|█▊        | 158/886 [00:58<04:26,  2.74it/s]Evaluating:  18%|█▊        | 159/886 [00:59<04:26,  2.73it/s]Evaluating:  18%|█▊        | 160/886 [00:59<04:25,  2.73it/s]Evaluating:  18%|█▊        | 161/886 [00:59<04:26,  2.72it/s]Evaluating:  18%|█▊        | 162/886 [01:00<04:25,  2.72it/s]Evaluating:  18%|█▊        | 163/886 [01:00<04:25,  2.72it/s]Evaluating:  19%|█▊        | 164/886 [01:00<04:26,  2.71it/s]Evaluating:  19%|█▊        | 165/886 [01:01<04:28,  2.69it/s]Evaluating:  19%|█▊        | 166/886 [01:01<04:27,  2.69it/s]Evaluating:  19%|█▉        | 167/886 [01:02<04:27,  2.69it/s]Evaluating:  19%|█▉        | 168/886 [01:02<04:25,  2.70it/s]Evaluating:  19%|█▉        | 169/886 [01:02<04:25,  2.70it/s]Evaluating:  19%|█▉        | 170/886 [01:03<04:24,  2.71it/s]Evaluating:  19%|█▉        | 171/886 [01:03<04:23,  2.71it/s]Evaluating:  19%|█▉        | 172/886 [01:03<04:23,  2.71it/s]Evaluating:  20%|█▉        | 173/886 [01:04<04:23,  2.71it/s]Evaluating:  20%|█▉        | 174/886 [01:04<04:24,  2.69it/s]Evaluating:  20%|█▉        | 175/886 [01:05<04:26,  2.67it/s]Evaluating:  20%|█▉        | 176/886 [01:05<04:25,  2.67it/s]Evaluating:  20%|█▉        | 177/886 [01:05<04:24,  2.68it/s]Evaluating:  20%|██        | 178/886 [01:06<04:23,  2.69it/s]Evaluating:  20%|██        | 179/886 [01:06<04:21,  2.70it/s]Evaluating:  20%|██        | 180/886 [01:06<04:20,  2.71it/s]Evaluating:  20%|██        | 181/886 [01:07<04:21,  2.70it/s]Evaluating:  21%|██        | 182/886 [01:07<04:20,  2.70it/s]Evaluating:  21%|██        | 183/886 [01:08<04:22,  2.68it/s]Evaluating:  21%|██        | 184/886 [01:08<04:23,  2.67it/s]Evaluating:  21%|██        | 185/886 [01:08<04:20,  2.69it/s]Evaluating:  21%|██        | 186/886 [01:09<04:19,  2.69it/s]Evaluating:  21%|██        | 187/886 [01:09<04:18,  2.71it/s]Evaluating:  21%|██        | 188/886 [01:09<04:17,  2.71it/s]Evaluating:  21%|██▏       | 189/886 [01:10<04:16,  2.72it/s]Evaluating:  21%|██▏       | 190/886 [01:10<04:15,  2.72it/s]Evaluating:  22%|██▏       | 191/886 [01:10<04:16,  2.71it/s]Evaluating:  22%|██▏       | 192/886 [01:11<04:14,  2.73it/s]Evaluating:  22%|██▏       | 193/886 [01:11<04:14,  2.73it/s]Evaluating:  22%|██▏       | 194/886 [01:12<04:15,  2.71it/s]Evaluating:  22%|██▏       | 195/886 [01:12<04:14,  2.71it/s]Evaluating:  22%|██▏       | 196/886 [01:12<04:14,  2.71it/s]Evaluating:  22%|██▏       | 197/886 [01:13<04:14,  2.71it/s]Evaluating:  22%|██▏       | 198/886 [01:13<04:13,  2.71it/s]Evaluating:  22%|██▏       | 199/886 [01:13<04:13,  2.71it/s]Evaluating:  23%|██▎       | 200/886 [01:14<04:14,  2.70it/s]Evaluating:  23%|██▎       | 201/886 [01:14<04:15,  2.68it/s]Evaluating:  23%|██▎       | 202/886 [01:15<04:14,  2.69it/s]Evaluating:  23%|██▎       | 203/886 [01:15<04:13,  2.69it/s]Evaluating:  23%|██▎       | 204/886 [01:15<04:13,  2.69it/s]Evaluating:  23%|██▎       | 205/886 [01:16<04:12,  2.70it/s]Evaluating:  23%|██▎       | 206/886 [01:16<04:12,  2.70it/s]Evaluating:  23%|██▎       | 207/886 [01:16<04:13,  2.68it/s]Evaluating:  23%|██▎       | 208/886 [01:17<04:11,  2.69it/s]Evaluating:  24%|██▎       | 209/886 [01:17<04:11,  2.69it/s]Evaluating:  24%|██▎       | 210/886 [01:18<04:10,  2.69it/s]Evaluating:  24%|██▍       | 211/886 [01:18<04:10,  2.69it/s]Evaluating:  24%|██▍       | 212/886 [01:18<04:09,  2.70it/s]Evaluating:  24%|██▍       | 213/886 [01:19<04:08,  2.71it/s]Evaluating:  24%|██▍       | 214/886 [01:19<04:07,  2.72it/s]Evaluating:  24%|██▍       | 215/886 [01:19<04:09,  2.69it/s]Evaluating:  24%|██▍       | 216/886 [01:20<04:08,  2.70it/s]Evaluating:  24%|██▍       | 217/886 [01:20<04:08,  2.70it/s]Evaluating:  25%|██▍       | 218/886 [01:20<04:07,  2.69it/s]Evaluating:  25%|██▍       | 219/886 [01:21<04:07,  2.69it/s]Evaluating:  25%|██▍       | 220/886 [01:21<04:08,  2.68it/s]Evaluating:  25%|██▍       | 221/886 [01:22<04:06,  2.70it/s]Evaluating:  25%|██▌       | 222/886 [01:22<04:06,  2.70it/s]Evaluating:  25%|██▌       | 223/886 [01:22<04:06,  2.69it/s]Evaluating:  25%|██▌       | 224/886 [01:23<04:05,  2.69it/s]Evaluating:  25%|██▌       | 225/886 [01:23<04:05,  2.70it/s]Evaluating:  26%|██▌       | 226/886 [01:23<04:04,  2.70it/s]Evaluating:  26%|██▌       | 227/886 [01:24<04:04,  2.70it/s]Evaluating:  26%|██▌       | 228/886 [01:24<04:03,  2.71it/s]Evaluating:  26%|██▌       | 229/886 [01:25<04:01,  2.72it/s]Evaluating:  26%|██▌       | 230/886 [01:25<04:01,  2.72it/s]Evaluating:  26%|██▌       | 231/886 [01:25<04:01,  2.71it/s]Evaluating:  26%|██▌       | 232/886 [01:26<04:00,  2.72it/s]Evaluating:  26%|██▋       | 233/886 [01:26<04:00,  2.71it/s]Evaluating:  26%|██▋       | 234/886 [01:26<04:01,  2.70it/s]Evaluating:  27%|██▋       | 235/886 [01:27<03:59,  2.71it/s]Evaluating:  27%|██▋       | 236/886 [01:27<03:58,  2.73it/s]Evaluating:  27%|██▋       | 237/886 [01:27<03:58,  2.72it/s]Evaluating:  27%|██▋       | 238/886 [01:28<03:59,  2.71it/s]Evaluating:  27%|██▋       | 239/886 [01:28<03:58,  2.71it/s]Evaluating:  27%|██▋       | 240/886 [01:29<03:58,  2.71it/s]Evaluating:  27%|██▋       | 241/886 [01:29<03:57,  2.72it/s]Evaluating:  27%|██▋       | 242/886 [01:29<03:55,  2.73it/s]Evaluating:  27%|██▋       | 243/886 [01:30<03:56,  2.72it/s]Evaluating:  28%|██▊       | 244/886 [01:30<03:55,  2.73it/s]Evaluating:  28%|██▊       | 245/886 [01:30<03:55,  2.73it/s]Evaluating:  28%|██▊       | 246/886 [01:31<03:54,  2.73it/s]Evaluating:  28%|██▊       | 247/886 [01:31<03:55,  2.71it/s]Evaluating:  28%|██▊       | 248/886 [01:32<03:55,  2.71it/s]Evaluating:  28%|██▊       | 249/886 [01:32<03:55,  2.70it/s]Evaluating:  28%|██▊       | 250/886 [01:32<03:55,  2.70it/s]Evaluating:  28%|██▊       | 251/886 [01:33<03:54,  2.71it/s]Evaluating:  28%|██▊       | 252/886 [01:33<03:53,  2.71it/s]Evaluating:  29%|██▊       | 253/886 [01:33<03:53,  2.72it/s]Evaluating:  29%|██▊       | 254/886 [01:34<03:52,  2.72it/s]Evaluating:  29%|██▉       | 255/886 [01:34<04:00,  2.63it/s]Evaluating:  29%|██▉       | 256/886 [01:35<03:58,  2.65it/s]Evaluating:  29%|██▉       | 257/886 [01:35<03:55,  2.67it/s]Evaluating:  29%|██▉       | 258/886 [01:35<03:54,  2.68it/s]Evaluating:  29%|██▉       | 259/886 [01:36<03:54,  2.67it/s]Evaluating:  29%|██▉       | 260/886 [01:36<03:52,  2.69it/s]Evaluating:  29%|██▉       | 261/886 [01:36<03:51,  2.70it/s]Evaluating:  30%|██▉       | 262/886 [01:37<03:50,  2.71it/s]Evaluating:  30%|██▉       | 263/886 [01:37<03:50,  2.70it/s]Evaluating:  30%|██▉       | 264/886 [01:37<03:49,  2.71it/s]Evaluating:  30%|██▉       | 265/886 [01:38<03:48,  2.72it/s]Evaluating:  30%|███       | 266/886 [01:38<03:47,  2.72it/s]Evaluating:  30%|███       | 267/886 [01:39<03:48,  2.70it/s]Evaluating:  30%|███       | 268/886 [01:39<03:49,  2.70it/s]Evaluating:  30%|███       | 269/886 [01:39<03:47,  2.71it/s]Evaluating:  30%|███       | 270/886 [01:40<03:47,  2.71it/s]Evaluating:  31%|███       | 271/886 [01:40<03:45,  2.72it/s]Evaluating:  31%|███       | 272/886 [01:40<03:44,  2.74it/s]Evaluating:  31%|███       | 273/886 [01:41<03:44,  2.74it/s]Evaluating:  31%|███       | 274/886 [01:41<03:43,  2.73it/s]Evaluating:  31%|███       | 275/886 [01:42<03:43,  2.74it/s]Evaluating:  31%|███       | 276/886 [01:42<03:43,  2.73it/s]Evaluating:  31%|███▏      | 277/886 [01:42<03:42,  2.74it/s]Evaluating:  31%|███▏      | 278/886 [01:43<03:43,  2.72it/s]Evaluating:  31%|███▏      | 279/886 [01:43<03:43,  2.71it/s]Evaluating:  32%|███▏      | 280/886 [01:43<03:43,  2.72it/s]Evaluating:  32%|███▏      | 281/886 [01:44<03:42,  2.72it/s]Evaluating:  32%|███▏      | 282/886 [01:44<03:42,  2.71it/s]Evaluating:  32%|███▏      | 283/886 [01:44<03:41,  2.72it/s]Evaluating:  32%|███▏      | 284/886 [01:45<03:42,  2.71it/s]Evaluating:  32%|███▏      | 285/886 [01:45<03:42,  2.70it/s]Evaluating:  32%|███▏      | 286/886 [01:46<03:42,  2.70it/s]Evaluating:  32%|███▏      | 287/886 [01:46<03:41,  2.70it/s]Evaluating:  33%|███▎      | 288/886 [01:46<03:40,  2.71it/s]Evaluating:  33%|███▎      | 289/886 [01:47<03:40,  2.71it/s]Evaluating:  33%|███▎      | 290/886 [01:47<03:39,  2.71it/s]Evaluating:  33%|███▎      | 291/886 [01:47<03:41,  2.69it/s]Evaluating:  33%|███▎      | 292/886 [01:48<03:40,  2.69it/s]Evaluating:  33%|███▎      | 293/886 [01:48<03:39,  2.70it/s]Evaluating:  33%|███▎      | 294/886 [01:49<03:38,  2.71it/s]Evaluating:  33%|███▎      | 295/886 [01:49<03:37,  2.72it/s]Evaluating:  33%|███▎      | 296/886 [01:49<03:38,  2.71it/s]Evaluating:  34%|███▎      | 297/886 [01:50<03:36,  2.72it/s]Evaluating:  34%|███▎      | 298/886 [01:50<03:35,  2.73it/s]Evaluating:  34%|███▎      | 299/886 [01:50<03:35,  2.73it/s]Evaluating:  34%|███▍      | 300/886 [01:51<03:35,  2.73it/s]Evaluating:  34%|███▍      | 301/886 [01:51<03:35,  2.72it/s]Evaluating:  34%|███▍      | 302/886 [01:51<03:34,  2.72it/s]Evaluating:  34%|███▍      | 303/886 [01:52<03:35,  2.71it/s]Evaluating:  34%|███▍      | 304/886 [01:52<03:34,  2.72it/s]Evaluating:  34%|███▍      | 305/886 [01:53<03:34,  2.71it/s]Evaluating:  35%|███▍      | 306/886 [01:53<03:33,  2.71it/s]Evaluating:  35%|███▍      | 307/886 [01:53<03:33,  2.71it/s]Evaluating:  35%|███▍      | 308/886 [01:54<03:32,  2.72it/s]Evaluating:  35%|███▍      | 309/886 [01:54<03:33,  2.70it/s]Evaluating:  35%|███▍      | 310/886 [01:54<03:32,  2.71it/s]Evaluating:  35%|███▌      | 311/886 [01:55<03:32,  2.71it/s]Evaluating:  35%|███▌      | 312/886 [01:55<03:30,  2.72it/s]Evaluating:  35%|███▌      | 313/886 [01:56<03:31,  2.71it/s]Evaluating:  35%|███▌      | 314/886 [01:56<03:31,  2.71it/s]Evaluating:  36%|███▌      | 315/886 [01:56<03:30,  2.72it/s]Evaluating:  36%|███▌      | 316/886 [01:57<03:30,  2.71it/s]Evaluating:  36%|███▌      | 317/886 [01:57<03:29,  2.71it/s]Evaluating:  36%|███▌      | 318/886 [01:57<03:29,  2.71it/s]Evaluating:  36%|███▌      | 319/886 [01:58<03:29,  2.70it/s]Evaluating:  36%|███▌      | 320/886 [01:58<03:29,  2.70it/s]Evaluating:  36%|███▌      | 321/886 [01:58<03:28,  2.70it/s]Evaluating:  36%|███▋      | 322/886 [01:59<03:28,  2.70it/s]Evaluating:  36%|███▋      | 323/886 [01:59<03:28,  2.69it/s]Evaluating:  37%|███▋      | 324/886 [02:00<03:28,  2.69it/s]Evaluating:  37%|███▋      | 325/886 [02:00<03:28,  2.70it/s]Evaluating:  37%|███▋      | 326/886 [02:00<03:27,  2.70it/s]Evaluating:  37%|███▋      | 327/886 [02:01<03:27,  2.70it/s]Evaluating:  37%|███▋      | 328/886 [02:01<03:27,  2.69it/s]Evaluating:  37%|███▋      | 329/886 [02:01<03:26,  2.70it/s]Evaluating:  37%|███▋      | 330/886 [02:02<03:26,  2.70it/s]Evaluating:  37%|███▋      | 331/886 [02:02<03:25,  2.70it/s]Evaluating:  37%|███▋      | 332/886 [02:03<03:25,  2.69it/s]Evaluating:  38%|███▊      | 333/886 [02:03<03:24,  2.70it/s]Evaluating:  38%|███▊      | 334/886 [02:03<03:24,  2.70it/s]Evaluating:  38%|███▊      | 335/886 [02:04<03:24,  2.70it/s]Evaluating:  38%|███▊      | 336/886 [02:04<03:23,  2.70it/s]Evaluating:  38%|███▊      | 337/886 [02:04<03:23,  2.69it/s]Evaluating:  38%|███▊      | 338/886 [02:05<03:23,  2.69it/s]Evaluating:  38%|███▊      | 339/886 [02:05<03:23,  2.69it/s]Evaluating:  38%|███▊      | 340/886 [02:06<03:23,  2.69it/s]Evaluating:  38%|███▊      | 341/886 [02:06<03:22,  2.69it/s]Evaluating:  39%|███▊      | 342/886 [02:06<03:20,  2.71it/s]Evaluating:  39%|███▊      | 343/886 [02:07<03:19,  2.72it/s]Evaluating:  39%|███▉      | 344/886 [02:07<03:19,  2.72it/s]Evaluating:  39%|███▉      | 345/886 [02:07<03:20,  2.70it/s]Evaluating:  39%|███▉      | 346/886 [02:08<03:19,  2.70it/s]Evaluating:  39%|███▉      | 347/886 [02:08<03:20,  2.69it/s]Evaluating:  39%|███▉      | 348/886 [02:08<03:18,  2.71it/s]Evaluating:  39%|███▉      | 349/886 [02:09<03:18,  2.70it/s]Evaluating:  40%|███▉      | 350/886 [02:09<03:18,  2.70it/s]Evaluating:  40%|███▉      | 351/886 [02:10<03:18,  2.69it/s]Evaluating:  40%|███▉      | 352/886 [02:10<03:18,  2.68it/s]Evaluating:  40%|███▉      | 353/886 [02:10<03:19,  2.68it/s]Evaluating:  40%|███▉      | 354/886 [02:11<03:18,  2.69it/s]Evaluating:  40%|████      | 355/886 [02:11<03:17,  2.69it/s]Evaluating:  40%|████      | 356/886 [02:11<03:16,  2.69it/s]Evaluating:  40%|████      | 357/886 [02:12<03:16,  2.70it/s]Evaluating:  40%|████      | 358/886 [02:12<03:16,  2.69it/s]Evaluating:  41%|████      | 359/886 [02:13<03:16,  2.69it/s]Evaluating:  41%|████      | 360/886 [02:13<03:15,  2.68it/s]Evaluating:  41%|████      | 361/886 [02:13<03:15,  2.69it/s]Evaluating:  41%|████      | 362/886 [02:14<03:14,  2.69it/s]Evaluating:  41%|████      | 363/886 [02:14<03:15,  2.68it/s]Evaluating:  41%|████      | 364/886 [02:14<03:14,  2.68it/s]Evaluating:  41%|████      | 365/886 [02:15<03:13,  2.69it/s]Evaluating:  41%|████▏     | 366/886 [02:15<03:12,  2.70it/s]Evaluating:  41%|████▏     | 367/886 [02:16<03:11,  2.70it/s]Evaluating:  42%|████▏     | 368/886 [02:16<03:12,  2.70it/s]Evaluating:  42%|████▏     | 369/886 [02:16<03:11,  2.70it/s]Evaluating:  42%|████▏     | 370/886 [02:17<03:11,  2.70it/s]Evaluating:  42%|████▏     | 371/886 [02:17<03:11,  2.69it/s]Evaluating:  42%|████▏     | 372/886 [02:17<03:10,  2.70it/s]Evaluating:  42%|████▏     | 373/886 [02:18<03:10,  2.69it/s]Evaluating:  42%|████▏     | 374/886 [02:18<03:09,  2.71it/s]Evaluating:  42%|████▏     | 375/886 [02:19<03:08,  2.70it/s]Evaluating:  42%|████▏     | 376/886 [02:19<03:08,  2.70it/s]Evaluating:  43%|████▎     | 377/886 [02:19<03:07,  2.71it/s]Evaluating:  43%|████▎     | 378/886 [02:20<03:08,  2.70it/s]Evaluating:  43%|████▎     | 379/886 [02:20<03:08,  2.69it/s]Evaluating:  43%|████▎     | 380/886 [02:20<03:08,  2.69it/s]Evaluating:  43%|████▎     | 381/886 [02:21<03:08,  2.68it/s]Evaluating:  43%|████▎     | 382/886 [02:21<03:07,  2.68it/s]Evaluating:  43%|████▎     | 383/886 [02:22<03:07,  2.69it/s]Evaluating:  43%|████▎     | 384/886 [02:22<03:06,  2.69it/s]Evaluating:  43%|████▎     | 385/886 [02:22<03:06,  2.68it/s]Evaluating:  44%|████▎     | 386/886 [02:23<03:06,  2.68it/s]Evaluating:  44%|████▎     | 387/886 [02:23<03:06,  2.68it/s]Evaluating:  44%|████▍     | 388/886 [02:23<03:04,  2.69it/s]Evaluating:  44%|████▍     | 389/886 [02:24<03:04,  2.69it/s]Evaluating:  44%|████▍     | 390/886 [02:24<03:04,  2.69it/s]Evaluating:  44%|████▍     | 391/886 [02:24<03:03,  2.70it/s]Evaluating:  44%|████▍     | 392/886 [02:25<03:02,  2.70it/s]Evaluating:  44%|████▍     | 393/886 [02:25<03:02,  2.70it/s]Evaluating:  44%|████▍     | 394/886 [02:26<03:02,  2.70it/s]Evaluating:  45%|████▍     | 395/886 [02:26<03:02,  2.69it/s]Evaluating:  45%|████▍     | 396/886 [02:26<03:01,  2.70it/s]Evaluating:  45%|████▍     | 397/886 [02:27<03:01,  2.69it/s]Evaluating:  45%|████▍     | 398/886 [02:27<03:01,  2.69it/s]Evaluating:  45%|████▌     | 399/886 [02:27<03:00,  2.70it/s]Evaluating:  45%|████▌     | 400/886 [02:28<02:59,  2.71it/s]Evaluating:  45%|████▌     | 401/886 [02:28<02:59,  2.71it/s]Evaluating:  45%|████▌     | 402/886 [02:29<02:59,  2.70it/s]Evaluating:  45%|████▌     | 403/886 [02:29<02:59,  2.70it/s]Evaluating:  46%|████▌     | 404/886 [02:29<02:59,  2.68it/s]Evaluating:  46%|████▌     | 405/886 [02:30<02:58,  2.69it/s]Evaluating:  46%|████▌     | 406/886 [02:30<02:58,  2.70it/s]Evaluating:  46%|████▌     | 407/886 [02:30<02:56,  2.71it/s]Evaluating:  46%|████▌     | 408/886 [02:31<02:55,  2.72it/s]Evaluating:  46%|████▌     | 409/886 [02:31<02:55,  2.72it/s]Evaluating:  46%|████▋     | 410/886 [02:31<02:54,  2.72it/s]Evaluating:  46%|████▋     | 411/886 [02:32<02:54,  2.72it/s]Evaluating:  47%|████▋     | 412/886 [02:32<02:53,  2.73it/s]Evaluating:  47%|████▋     | 413/886 [02:33<02:53,  2.72it/s]Evaluating:  47%|████▋     | 414/886 [02:33<02:54,  2.71it/s]Evaluating:  47%|████▋     | 415/886 [02:33<02:53,  2.71it/s]Evaluating:  47%|████▋     | 416/886 [02:34<02:53,  2.71it/s]Evaluating:  47%|████▋     | 417/886 [02:34<02:54,  2.68it/s]Evaluating:  47%|████▋     | 418/886 [02:34<02:53,  2.69it/s]Evaluating:  47%|████▋     | 419/886 [02:35<02:53,  2.69it/s]Evaluating:  47%|████▋     | 420/886 [02:35<02:53,  2.69it/s]Evaluating:  48%|████▊     | 421/886 [02:36<02:52,  2.69it/s]Evaluating:  48%|████▊     | 422/886 [02:36<02:53,  2.68it/s]Evaluating:  48%|████▊     | 423/886 [02:36<02:52,  2.68it/s]Evaluating:  48%|████▊     | 424/886 [02:37<02:52,  2.68it/s]Evaluating:  48%|████▊     | 425/886 [02:37<02:51,  2.69it/s]Evaluating:  48%|████▊     | 426/886 [02:37<02:50,  2.70it/s]Evaluating:  48%|████▊     | 427/886 [02:38<02:49,  2.71it/s]Evaluating:  48%|████▊     | 428/886 [02:38<02:49,  2.70it/s]Evaluating:  48%|████▊     | 429/886 [02:39<02:48,  2.71it/s]Evaluating:  49%|████▊     | 430/886 [02:39<02:48,  2.71it/s]Evaluating:  49%|████▊     | 431/886 [02:39<02:47,  2.72it/s]Evaluating:  49%|████▉     | 432/886 [02:40<02:47,  2.71it/s]Evaluating:  49%|████▉     | 433/886 [02:40<02:47,  2.70it/s]Evaluating:  49%|████▉     | 434/886 [02:40<02:48,  2.69it/s]Evaluating:  49%|████▉     | 435/886 [02:41<02:48,  2.68it/s]Evaluating:  49%|████▉     | 436/886 [02:41<02:48,  2.68it/s]Evaluating:  49%|████▉     | 437/886 [02:42<02:47,  2.68it/s]Evaluating:  49%|████▉     | 438/886 [02:42<02:46,  2.69it/s]Evaluating:  50%|████▉     | 439/886 [02:42<02:45,  2.70it/s]Evaluating:  50%|████▉     | 440/886 [02:43<02:45,  2.70it/s]Evaluating:  50%|████▉     | 441/886 [02:43<02:44,  2.70it/s]Evaluating:  50%|████▉     | 442/886 [02:43<02:44,  2.70it/s]Evaluating:  50%|█████     | 443/886 [02:44<02:43,  2.71it/s]Evaluating:  50%|█████     | 444/886 [02:44<02:43,  2.70it/s]Evaluating:  50%|█████     | 445/886 [02:44<02:43,  2.69it/s]Evaluating:  50%|█████     | 446/886 [02:45<02:42,  2.70it/s]Evaluating:  50%|█████     | 447/886 [02:45<02:41,  2.71it/s]Evaluating:  51%|█████     | 448/886 [02:46<02:42,  2.70it/s]Evaluating:  51%|█████     | 449/886 [02:46<02:42,  2.70it/s]Evaluating:  51%|█████     | 450/886 [02:46<02:41,  2.70it/s]Evaluating:  51%|█████     | 451/886 [02:47<02:40,  2.71it/s]Evaluating:  51%|█████     | 452/886 [02:47<02:40,  2.71it/s]Evaluating:  51%|█████     | 453/886 [02:47<02:41,  2.68it/s]Evaluating:  51%|█████     | 454/886 [02:48<02:41,  2.67it/s]Evaluating:  51%|█████▏    | 455/886 [02:48<02:41,  2.67it/s]Evaluating:  51%|█████▏    | 456/886 [02:49<02:40,  2.68it/s]Evaluating:  52%|█████▏    | 457/886 [02:49<02:40,  2.67it/s]Evaluating:  52%|█████▏    | 458/886 [02:49<02:39,  2.68it/s]Evaluating:  52%|█████▏    | 459/886 [02:50<02:39,  2.68it/s]Evaluating:  52%|█████▏    | 460/886 [02:50<02:38,  2.69it/s]Evaluating:  52%|█████▏    | 461/886 [02:50<02:37,  2.70it/s]Evaluating:  52%|█████▏    | 462/886 [02:51<02:36,  2.70it/s]Evaluating:  52%|█████▏    | 463/886 [02:51<02:36,  2.70it/s]Evaluating:  52%|█████▏    | 464/886 [02:52<02:36,  2.70it/s]Evaluating:  52%|█████▏    | 465/886 [02:52<02:35,  2.70it/s]Evaluating:  53%|█████▎    | 466/886 [02:52<02:36,  2.69it/s]Evaluating:  53%|█████▎    | 467/886 [02:53<02:35,  2.70it/s]Evaluating:  53%|█████▎    | 468/886 [02:53<02:34,  2.70it/s]Evaluating:  53%|█████▎    | 469/886 [02:53<02:33,  2.71it/s]Evaluating:  53%|█████▎    | 470/886 [02:54<02:33,  2.71it/s]Evaluating:  53%|█████▎    | 471/886 [02:54<02:37,  2.63it/s]Evaluating:  53%|█████▎    | 472/886 [02:55<02:36,  2.65it/s]Evaluating:  53%|█████▎    | 473/886 [02:55<02:35,  2.66it/s]Evaluating:  53%|█████▎    | 474/886 [02:55<02:33,  2.68it/s]Evaluating:  54%|█████▎    | 475/886 [02:56<02:33,  2.68it/s]Evaluating:  54%|█████▎    | 476/886 [02:56<02:32,  2.68it/s]Evaluating:  54%|█████▍    | 477/886 [02:56<02:32,  2.69it/s]Evaluating:  54%|█████▍    | 478/886 [02:57<02:31,  2.70it/s]Evaluating:  54%|█████▍    | 479/886 [02:57<02:30,  2.71it/s]Evaluating:  54%|█████▍    | 480/886 [02:57<02:29,  2.71it/s]Evaluating:  54%|█████▍    | 481/886 [02:58<02:30,  2.70it/s]Evaluating:  54%|█████▍    | 482/886 [02:58<02:29,  2.70it/s]Evaluating:  55%|█████▍    | 483/886 [02:59<02:29,  2.70it/s]Evaluating:  55%|█████▍    | 484/886 [02:59<02:28,  2.71it/s]Evaluating:  55%|█████▍    | 485/886 [02:59<02:27,  2.72it/s]Evaluating:  55%|█████▍    | 486/886 [03:00<02:27,  2.72it/s]Evaluating:  55%|█████▍    | 487/886 [03:00<02:27,  2.70it/s]Evaluating:  55%|█████▌    | 488/886 [03:00<02:27,  2.70it/s]Evaluating:  55%|█████▌    | 489/886 [03:01<02:27,  2.70it/s]Evaluating:  55%|█████▌    | 490/886 [03:01<02:26,  2.70it/s]Evaluating:  55%|█████▌    | 491/886 [03:02<02:26,  2.70it/s]Evaluating:  56%|█████▌    | 492/886 [03:02<02:25,  2.70it/s]Evaluating:  56%|█████▌    | 493/886 [03:02<02:25,  2.70it/s]Evaluating:  56%|█████▌    | 494/886 [03:03<02:25,  2.70it/s]Evaluating:  56%|█████▌    | 495/886 [03:03<02:24,  2.70it/s]Evaluating:  56%|█████▌    | 496/886 [03:03<02:23,  2.71it/s]Evaluating:  56%|█████▌    | 497/886 [03:04<02:22,  2.72it/s]Evaluating:  56%|█████▌    | 498/886 [03:04<02:22,  2.72it/s]Evaluating:  56%|█████▋    | 499/886 [03:05<02:22,  2.71it/s]Evaluating:  56%|█████▋    | 500/886 [03:05<02:22,  2.70it/s]Evaluating:  57%|█████▋    | 501/886 [03:05<02:22,  2.70it/s]Evaluating:  57%|█████▋    | 502/886 [03:06<02:22,  2.70it/s]Evaluating:  57%|█████▋    | 503/886 [03:06<02:22,  2.69it/s]Evaluating:  57%|█████▋    | 504/886 [03:06<02:22,  2.68it/s]Evaluating:  57%|█████▋    | 505/886 [03:07<02:22,  2.67it/s]Evaluating:  57%|█████▋    | 506/886 [03:07<02:22,  2.68it/s]Evaluating:  57%|█████▋    | 507/886 [03:07<02:21,  2.67it/s]Evaluating:  57%|█████▋    | 508/886 [03:08<02:20,  2.69it/s]Evaluating:  57%|█████▋    | 509/886 [03:08<02:19,  2.70it/s]Evaluating:  58%|█████▊    | 510/886 [03:09<02:19,  2.70it/s]Evaluating:  58%|█████▊    | 511/886 [03:09<02:18,  2.70it/s]Evaluating:  58%|█████▊    | 512/886 [03:09<02:18,  2.71it/s]Evaluating:  58%|█████▊    | 513/886 [03:10<02:17,  2.71it/s]Evaluating:  58%|█████▊    | 514/886 [03:10<02:16,  2.72it/s]Evaluating:  58%|█████▊    | 515/886 [03:10<02:16,  2.71it/s]Evaluating:  58%|█████▊    | 516/886 [03:11<02:16,  2.70it/s]Evaluating:  58%|█████▊    | 517/886 [03:11<02:16,  2.70it/s]Evaluating:  58%|█████▊    | 518/886 [03:12<02:17,  2.68it/s]Evaluating:  59%|█████▊    | 519/886 [03:12<02:17,  2.68it/s]Evaluating:  59%|█████▊    | 520/886 [03:12<02:16,  2.68it/s]Evaluating:  59%|█████▉    | 521/886 [03:13<02:15,  2.69it/s]Evaluating:  59%|█████▉    | 522/886 [03:13<02:15,  2.69it/s]Evaluating:  59%|█████▉    | 523/886 [03:13<02:14,  2.69it/s]Evaluating:  59%|█████▉    | 524/886 [03:14<02:13,  2.70it/s]Evaluating:  59%|█████▉    | 525/886 [03:14<02:14,  2.69it/s]Evaluating:  59%|█████▉    | 526/886 [03:15<02:13,  2.70it/s]Evaluating:  59%|█████▉    | 527/886 [03:15<02:13,  2.69it/s]Evaluating:  60%|█████▉    | 528/886 [03:15<02:13,  2.69it/s]Evaluating:  60%|█████▉    | 529/886 [03:16<02:12,  2.69it/s]Evaluating:  60%|█████▉    | 530/886 [03:16<02:11,  2.70it/s]Evaluating:  60%|█████▉    | 531/886 [03:16<02:11,  2.70it/s]Evaluating:  60%|██████    | 532/886 [03:17<02:10,  2.71it/s]Evaluating:  60%|██████    | 533/886 [03:17<02:11,  2.69it/s]Evaluating:  60%|██████    | 534/886 [03:18<02:10,  2.69it/s]Evaluating:  60%|██████    | 535/886 [03:18<02:11,  2.67it/s]Evaluating:  60%|██████    | 536/886 [03:18<02:10,  2.68it/s]Evaluating:  61%|██████    | 537/886 [03:19<02:10,  2.68it/s]Evaluating:  61%|██████    | 538/886 [03:19<02:09,  2.69it/s]Evaluating:  61%|██████    | 539/886 [03:19<02:08,  2.70it/s]Evaluating:  61%|██████    | 540/886 [03:20<02:08,  2.70it/s]Evaluating:  61%|██████    | 541/886 [03:20<02:08,  2.69it/s]Evaluating:  61%|██████    | 542/886 [03:20<02:07,  2.69it/s]Evaluating:  61%|██████▏   | 543/886 [03:21<02:07,  2.69it/s]Evaluating:  61%|██████▏   | 544/886 [03:21<02:07,  2.69it/s]Evaluating:  62%|██████▏   | 545/886 [03:22<02:06,  2.69it/s]Evaluating:  62%|██████▏   | 546/886 [03:22<02:06,  2.70it/s]Evaluating:  62%|██████▏   | 547/886 [03:22<02:05,  2.70it/s]Evaluating:  62%|██████▏   | 548/886 [03:23<02:05,  2.69it/s]Evaluating:  62%|██████▏   | 549/886 [03:23<02:05,  2.69it/s]Evaluating:  62%|██████▏   | 550/886 [03:23<02:04,  2.70it/s]Evaluating:  62%|██████▏   | 551/886 [03:24<02:04,  2.69it/s]Evaluating:  62%|██████▏   | 552/886 [03:24<02:03,  2.69it/s]Evaluating:  62%|██████▏   | 553/886 [03:25<02:03,  2.69it/s]Evaluating:  63%|██████▎   | 554/886 [03:25<02:03,  2.69it/s]Evaluating:  63%|██████▎   | 555/886 [03:25<02:02,  2.70it/s]Evaluating:  63%|██████▎   | 556/886 [03:26<02:02,  2.70it/s]Evaluating:  63%|██████▎   | 557/886 [03:26<02:02,  2.69it/s]Evaluating:  63%|██████▎   | 558/886 [03:26<02:01,  2.69it/s]Evaluating:  63%|██████▎   | 559/886 [03:27<02:00,  2.70it/s]Evaluating:  63%|██████▎   | 560/886 [03:27<02:00,  2.70it/s]Evaluating:  63%|██████▎   | 561/886 [03:28<02:01,  2.68it/s]Evaluating:  63%|██████▎   | 562/886 [03:28<02:00,  2.69it/s]Evaluating:  64%|██████▎   | 563/886 [03:28<02:00,  2.69it/s]Evaluating:  64%|██████▎   | 564/886 [03:29<01:59,  2.69it/s]Evaluating:  64%|██████▍   | 565/886 [03:29<01:59,  2.69it/s]Evaluating:  64%|██████▍   | 566/886 [03:29<01:58,  2.70it/s]Evaluating:  64%|██████▍   | 567/886 [03:30<01:58,  2.68it/s]Evaluating:  64%|██████▍   | 568/886 [03:30<01:58,  2.69it/s]Evaluating:  64%|██████▍   | 569/886 [03:31<01:58,  2.68it/s]Evaluating:  64%|██████▍   | 570/886 [03:31<01:57,  2.68it/s]Evaluating:  64%|██████▍   | 571/886 [03:31<01:57,  2.68it/s]Evaluating:  65%|██████▍   | 572/886 [03:32<01:56,  2.69it/s]Evaluating:  65%|██████▍   | 573/886 [03:32<01:56,  2.69it/s]Evaluating:  65%|██████▍   | 574/886 [03:32<01:56,  2.69it/s]Evaluating:  65%|██████▍   | 575/886 [03:33<01:56,  2.68it/s]Evaluating:  65%|██████▌   | 576/886 [03:33<01:55,  2.69it/s]Evaluating:  65%|██████▌   | 577/886 [03:33<01:55,  2.68it/s]Evaluating:  65%|██████▌   | 578/886 [03:34<01:55,  2.67it/s]Evaluating:  65%|██████▌   | 579/886 [03:34<01:55,  2.65it/s]Evaluating:  65%|██████▌   | 580/886 [03:35<01:54,  2.67it/s]Evaluating:  66%|██████▌   | 581/886 [03:35<01:53,  2.68it/s]Evaluating:  66%|██████▌   | 582/886 [03:35<01:53,  2.68it/s]Evaluating:  66%|██████▌   | 583/886 [03:36<01:52,  2.68it/s]Evaluating:  66%|██████▌   | 584/886 [03:36<01:52,  2.69it/s]Evaluating:  66%|██████▌   | 585/886 [03:36<01:51,  2.69it/s]Evaluating:  66%|██████▌   | 586/886 [03:37<01:51,  2.70it/s]Evaluating:  66%|██████▋   | 587/886 [03:37<01:50,  2.70it/s]Evaluating:  66%|██████▋   | 588/886 [03:38<01:50,  2.70it/s]Evaluating:  66%|██████▋   | 589/886 [03:38<01:50,  2.70it/s]Evaluating:  67%|██████▋   | 590/886 [03:38<01:49,  2.70it/s]Evaluating:  67%|██████▋   | 591/886 [03:39<01:49,  2.70it/s]Evaluating:  67%|██████▋   | 592/886 [03:39<01:49,  2.69it/s]Evaluating:  67%|██████▋   | 593/886 [03:39<01:48,  2.69it/s]Evaluating:  67%|██████▋   | 594/886 [03:40<01:47,  2.71it/s]Evaluating:  67%|██████▋   | 595/886 [03:40<01:47,  2.70it/s]Evaluating:  67%|██████▋   | 596/886 [03:41<01:47,  2.71it/s]Evaluating:  67%|██████▋   | 597/886 [03:41<01:46,  2.71it/s]Evaluating:  67%|██████▋   | 598/886 [03:41<01:46,  2.71it/s]Evaluating:  68%|██████▊   | 599/886 [03:42<01:45,  2.72it/s]Evaluating:  68%|██████▊   | 600/886 [03:42<01:44,  2.73it/s]Evaluating:  68%|██████▊   | 601/886 [03:42<01:45,  2.71it/s]Evaluating:  68%|██████▊   | 602/886 [03:43<01:44,  2.71it/s]Evaluating:  68%|██████▊   | 603/886 [03:43<01:44,  2.72it/s]Evaluating:  68%|██████▊   | 604/886 [03:43<01:43,  2.72it/s]Evaluating:  68%|██████▊   | 605/886 [03:44<01:43,  2.72it/s]Evaluating:  68%|██████▊   | 606/886 [03:44<01:43,  2.72it/s]Evaluating:  69%|██████▊   | 607/886 [03:45<01:42,  2.72it/s]Evaluating:  69%|██████▊   | 608/886 [03:45<01:42,  2.72it/s]Evaluating:  69%|██████▊   | 609/886 [03:45<01:42,  2.71it/s]Evaluating:  69%|██████▉   | 610/886 [03:46<01:41,  2.72it/s]Evaluating:  69%|██████▉   | 611/886 [03:46<01:40,  2.73it/s]Evaluating:  69%|██████▉   | 612/886 [03:46<01:40,  2.73it/s]Evaluating:  69%|██████▉   | 613/886 [03:47<01:40,  2.73it/s]Evaluating:  69%|██████▉   | 614/886 [03:47<01:39,  2.73it/s]Evaluating:  69%|██████▉   | 615/886 [03:48<01:39,  2.72it/s]Evaluating:  70%|██████▉   | 616/886 [03:48<01:39,  2.70it/s]Evaluating:  70%|██████▉   | 617/886 [03:48<01:39,  2.70it/s]Evaluating:  70%|██████▉   | 618/886 [03:49<01:39,  2.70it/s]Evaluating:  70%|██████▉   | 619/886 [03:49<01:39,  2.68it/s]Evaluating:  70%|██████▉   | 620/886 [03:49<01:38,  2.69it/s]Evaluating:  70%|███████   | 621/886 [03:50<01:38,  2.70it/s]Evaluating:  70%|███████   | 622/886 [03:50<01:37,  2.71it/s]Evaluating:  70%|███████   | 623/886 [03:50<01:36,  2.72it/s]Evaluating:  70%|███████   | 624/886 [03:51<01:36,  2.72it/s]Evaluating:  71%|███████   | 625/886 [03:51<01:36,  2.70it/s]Evaluating:  71%|███████   | 626/886 [03:52<01:36,  2.70it/s]Evaluating:  71%|███████   | 627/886 [03:52<01:35,  2.71it/s]Evaluating:  71%|███████   | 628/886 [03:52<01:35,  2.71it/s]Evaluating:  71%|███████   | 629/886 [03:53<01:34,  2.71it/s]Evaluating:  71%|███████   | 630/886 [03:53<01:34,  2.71it/s]Evaluating:  71%|███████   | 631/886 [03:53<01:33,  2.72it/s]Evaluating:  71%|███████▏  | 632/886 [03:54<01:34,  2.70it/s]Evaluating:  71%|███████▏  | 633/886 [03:54<01:33,  2.70it/s]Evaluating:  72%|███████▏  | 634/886 [03:55<01:33,  2.70it/s]Evaluating:  72%|███████▏  | 635/886 [03:55<01:32,  2.70it/s]Evaluating:  72%|███████▏  | 636/886 [03:55<01:32,  2.71it/s]Evaluating:  72%|███████▏  | 637/886 [03:56<01:32,  2.70it/s]Evaluating:  72%|███████▏  | 638/886 [03:56<01:31,  2.70it/s]Evaluating:  72%|███████▏  | 639/886 [03:56<01:31,  2.70it/s]Evaluating:  72%|███████▏  | 640/886 [03:57<01:31,  2.70it/s]Evaluating:  72%|███████▏  | 641/886 [03:57<01:30,  2.70it/s]Evaluating:  72%|███████▏  | 642/886 [03:58<01:30,  2.70it/s]Evaluating:  73%|███████▎  | 643/886 [03:58<01:30,  2.70it/s]Evaluating:  73%|███████▎  | 644/886 [03:58<01:29,  2.70it/s]Evaluating:  73%|███████▎  | 645/886 [03:59<01:29,  2.70it/s]Evaluating:  73%|███████▎  | 646/886 [03:59<01:28,  2.71it/s]Evaluating:  73%|███████▎  | 647/886 [03:59<01:28,  2.71it/s]Evaluating:  73%|███████▎  | 648/886 [04:00<01:28,  2.70it/s]Evaluating:  73%|███████▎  | 649/886 [04:00<01:27,  2.70it/s]Evaluating:  73%|███████▎  | 650/886 [04:00<01:27,  2.69it/s]Evaluating:  73%|███████▎  | 651/886 [04:01<01:27,  2.69it/s]Evaluating:  74%|███████▎  | 652/886 [04:01<01:26,  2.70it/s]Evaluating:  74%|███████▎  | 653/886 [04:02<01:26,  2.69it/s]Evaluating:  74%|███████▍  | 654/886 [04:02<01:25,  2.70it/s]Evaluating:  74%|███████▍  | 655/886 [04:02<01:25,  2.72it/s]Evaluating:  74%|███████▍  | 656/886 [04:03<01:24,  2.72it/s]Evaluating:  74%|███████▍  | 657/886 [04:03<01:24,  2.72it/s]Evaluating:  74%|███████▍  | 658/886 [04:03<01:23,  2.72it/s]Evaluating:  74%|███████▍  | 659/886 [04:04<01:23,  2.72it/s]Evaluating:  74%|███████▍  | 660/886 [04:04<01:23,  2.72it/s]Evaluating:  75%|███████▍  | 661/886 [04:05<01:22,  2.72it/s]Evaluating:  75%|███████▍  | 662/886 [04:05<01:22,  2.72it/s]Evaluating:  75%|███████▍  | 663/886 [04:05<01:21,  2.72it/s]Evaluating:  75%|███████▍  | 664/886 [04:06<01:21,  2.72it/s]Evaluating:  75%|███████▌  | 665/886 [04:06<01:21,  2.71it/s]Evaluating:  75%|███████▌  | 666/886 [04:06<01:20,  2.72it/s]Evaluating:  75%|███████▌  | 667/886 [04:07<01:20,  2.71it/s]Evaluating:  75%|███████▌  | 668/886 [04:07<01:20,  2.72it/s]Evaluating:  76%|███████▌  | 669/886 [04:07<01:20,  2.71it/s]Evaluating:  76%|███████▌  | 670/886 [04:08<01:20,  2.70it/s]Evaluating:  76%|███████▌  | 671/886 [04:08<01:19,  2.70it/s]Evaluating:  76%|███████▌  | 672/886 [04:09<01:19,  2.70it/s]Evaluating:  76%|███████▌  | 673/886 [04:09<01:18,  2.71it/s]Evaluating:  76%|███████▌  | 674/886 [04:09<01:18,  2.70it/s]Evaluating:  76%|███████▌  | 675/886 [04:10<01:17,  2.72it/s]Evaluating:  76%|███████▋  | 676/886 [04:10<01:17,  2.72it/s]Evaluating:  76%|███████▋  | 677/886 [04:10<01:16,  2.72it/s]Evaluating:  77%|███████▋  | 678/886 [04:11<01:16,  2.71it/s]Evaluating:  77%|███████▋  | 679/886 [04:11<01:16,  2.69it/s]Evaluating:  77%|███████▋  | 680/886 [04:12<01:16,  2.70it/s]Evaluating:  77%|███████▋  | 681/886 [04:12<01:16,  2.68it/s]Evaluating:  77%|███████▋  | 682/886 [04:12<01:16,  2.68it/s]Evaluating:  77%|███████▋  | 683/886 [04:13<01:15,  2.67it/s]Evaluating:  77%|███████▋  | 684/886 [04:13<01:15,  2.68it/s]Evaluating:  77%|███████▋  | 685/886 [04:13<01:14,  2.69it/s]Evaluating:  77%|███████▋  | 686/886 [04:14<01:14,  2.70it/s]Evaluating:  78%|███████▊  | 687/886 [04:14<01:15,  2.62it/s]Evaluating:  78%|███████▊  | 688/886 [04:15<01:15,  2.64it/s]Evaluating:  78%|███████▊  | 689/886 [04:15<01:14,  2.65it/s]Evaluating:  78%|███████▊  | 690/886 [04:15<01:13,  2.66it/s]Evaluating:  78%|███████▊  | 691/886 [04:16<01:12,  2.67it/s]Evaluating:  78%|███████▊  | 692/886 [04:16<01:12,  2.69it/s]Evaluating:  78%|███████▊  | 693/886 [04:16<01:11,  2.70it/s]Evaluating:  78%|███████▊  | 694/886 [04:17<01:10,  2.71it/s]Evaluating:  78%|███████▊  | 695/886 [04:17<01:10,  2.71it/s]Evaluating:  79%|███████▊  | 696/886 [04:18<01:10,  2.71it/s]Evaluating:  79%|███████▊  | 697/886 [04:18<01:09,  2.72it/s]Evaluating:  79%|███████▉  | 698/886 [04:18<01:09,  2.72it/s]Evaluating:  79%|███████▉  | 699/886 [04:19<01:08,  2.72it/s]Evaluating:  79%|███████▉  | 700/886 [04:19<01:08,  2.71it/s]Evaluating:  79%|███████▉  | 701/886 [04:19<01:08,  2.72it/s]Evaluating:  79%|███████▉  | 702/886 [04:20<01:08,  2.70it/s]Evaluating:  79%|███████▉  | 703/886 [04:20<01:07,  2.70it/s]Evaluating:  79%|███████▉  | 704/886 [04:20<01:07,  2.70it/s]Evaluating:  80%|███████▉  | 705/886 [04:21<01:06,  2.71it/s]Evaluating:  80%|███████▉  | 706/886 [04:21<01:06,  2.72it/s]Evaluating:  80%|███████▉  | 707/886 [04:22<01:06,  2.71it/s]Evaluating:  80%|███████▉  | 708/886 [04:22<01:05,  2.71it/s]Evaluating:  80%|████████  | 709/886 [04:22<01:05,  2.71it/s]Evaluating:  80%|████████  | 710/886 [04:23<01:05,  2.70it/s]Evaluating:  80%|████████  | 711/886 [04:23<01:04,  2.70it/s]Evaluating:  80%|████████  | 712/886 [04:23<01:04,  2.71it/s]Evaluating:  80%|████████  | 713/886 [04:24<01:04,  2.69it/s]Evaluating:  81%|████████  | 714/886 [04:24<01:03,  2.69it/s]Evaluating:  81%|████████  | 715/886 [04:25<01:03,  2.69it/s]Evaluating:  81%|████████  | 716/886 [04:25<01:03,  2.69it/s]Evaluating:  81%|████████  | 717/886 [04:25<01:02,  2.70it/s]Evaluating:  81%|████████  | 718/886 [04:26<01:02,  2.70it/s]Evaluating:  81%|████████  | 719/886 [04:26<01:01,  2.70it/s]Evaluating:  81%|████████▏ | 720/886 [04:26<01:01,  2.70it/s]Evaluating:  81%|████████▏ | 721/886 [04:27<01:01,  2.70it/s]Evaluating:  81%|████████▏ | 722/886 [04:27<01:00,  2.71it/s]Evaluating:  82%|████████▏ | 723/886 [04:28<01:00,  2.70it/s]Evaluating:  82%|████████▏ | 724/886 [04:28<00:59,  2.71it/s]Evaluating:  82%|████████▏ | 725/886 [04:28<00:59,  2.71it/s]Evaluating:  82%|████████▏ | 726/886 [04:29<00:59,  2.69it/s]Evaluating:  82%|████████▏ | 727/886 [04:29<00:59,  2.69it/s]Evaluating:  82%|████████▏ | 728/886 [04:29<00:58,  2.68it/s]Evaluating:  82%|████████▏ | 729/886 [04:30<00:58,  2.69it/s]Evaluating:  82%|████████▏ | 730/886 [04:30<00:57,  2.70it/s]Evaluating:  83%|████████▎ | 731/886 [04:30<00:57,  2.69it/s]Evaluating:  83%|████████▎ | 732/886 [04:31<00:57,  2.69it/s]Evaluating:  83%|████████▎ | 733/886 [04:31<00:56,  2.70it/s]Evaluating:  83%|████████▎ | 734/886 [04:32<00:56,  2.70it/s]Evaluating:  83%|████████▎ | 735/886 [04:32<00:56,  2.69it/s]Evaluating:  83%|████████▎ | 736/886 [04:32<00:55,  2.70it/s]Evaluating:  83%|████████▎ | 737/886 [04:33<00:55,  2.71it/s]Evaluating:  83%|████████▎ | 738/886 [04:33<00:54,  2.71it/s]Evaluating:  83%|████████▎ | 739/886 [04:33<00:54,  2.70it/s]Evaluating:  84%|████████▎ | 740/886 [04:34<00:53,  2.71it/s]Evaluating:  84%|████████▎ | 741/886 [04:34<00:53,  2.71it/s]Evaluating:  84%|████████▎ | 742/886 [04:35<00:53,  2.70it/s]Evaluating:  84%|████████▍ | 743/886 [04:35<00:52,  2.70it/s]Evaluating:  84%|████████▍ | 744/886 [04:35<00:52,  2.71it/s]Evaluating:  84%|████████▍ | 745/886 [04:36<00:51,  2.72it/s]Evaluating:  84%|████████▍ | 746/886 [04:36<00:51,  2.72it/s]Evaluating:  84%|████████▍ | 747/886 [04:36<00:51,  2.71it/s]Evaluating:  84%|████████▍ | 748/886 [04:37<00:51,  2.70it/s]Evaluating:  85%|████████▍ | 749/886 [04:37<00:50,  2.70it/s]Evaluating:  85%|████████▍ | 750/886 [04:38<00:50,  2.70it/s]Evaluating:  85%|████████▍ | 751/886 [04:38<00:49,  2.71it/s]Evaluating:  85%|████████▍ | 752/886 [04:38<00:49,  2.72it/s]Evaluating:  85%|████████▍ | 753/886 [04:39<00:48,  2.73it/s]Evaluating:  85%|████████▌ | 754/886 [04:39<00:48,  2.73it/s]Evaluating:  85%|████████▌ | 755/886 [04:39<00:48,  2.72it/s]Evaluating:  85%|████████▌ | 756/886 [04:40<00:48,  2.70it/s]Evaluating:  85%|████████▌ | 757/886 [04:40<00:48,  2.68it/s]Evaluating:  86%|████████▌ | 758/886 [04:40<00:47,  2.70it/s]Evaluating:  86%|████████▌ | 759/886 [04:41<00:47,  2.69it/s]Evaluating:  86%|████████▌ | 760/886 [04:41<00:47,  2.68it/s]Evaluating:  86%|████████▌ | 761/886 [04:42<00:46,  2.69it/s]Evaluating:  86%|████████▌ | 762/886 [04:42<00:45,  2.70it/s]Evaluating:  86%|████████▌ | 763/886 [04:42<00:45,  2.69it/s]Evaluating:  86%|████████▌ | 764/886 [04:43<00:45,  2.70it/s]Evaluating:  86%|████████▋ | 765/886 [04:43<00:44,  2.70it/s]Evaluating:  86%|████████▋ | 766/886 [04:43<00:44,  2.69it/s]Evaluating:  87%|████████▋ | 767/886 [04:44<00:44,  2.70it/s]Evaluating:  87%|████████▋ | 768/886 [04:44<00:43,  2.69it/s]Evaluating:  87%|████████▋ | 769/886 [04:45<00:43,  2.68it/s]Evaluating:  87%|████████▋ | 770/886 [04:45<00:43,  2.69it/s]Evaluating:  87%|████████▋ | 771/886 [04:45<00:42,  2.70it/s]Evaluating:  87%|████████▋ | 772/886 [04:46<00:42,  2.71it/s]Evaluating:  87%|████████▋ | 773/886 [04:46<00:41,  2.71it/s]Evaluating:  87%|████████▋ | 774/886 [04:46<00:41,  2.70it/s]Evaluating:  87%|████████▋ | 775/886 [04:47<00:41,  2.69it/s]Evaluating:  88%|████████▊ | 776/886 [04:47<00:40,  2.70it/s]Evaluating:  88%|████████▊ | 777/886 [04:48<00:40,  2.70it/s]Evaluating:  88%|████████▊ | 778/886 [04:48<00:40,  2.68it/s]Evaluating:  88%|████████▊ | 779/886 [04:48<00:39,  2.69it/s]Evaluating:  88%|████████▊ | 780/886 [04:49<00:39,  2.70it/s]Evaluating:  88%|████████▊ | 781/886 [04:49<00:38,  2.70it/s]Evaluating:  88%|████████▊ | 782/886 [04:49<00:38,  2.70it/s]Evaluating:  88%|████████▊ | 783/886 [04:50<00:38,  2.70it/s]Evaluating:  88%|████████▊ | 784/886 [04:50<00:37,  2.71it/s]Evaluating:  89%|████████▊ | 785/886 [04:50<00:37,  2.71it/s]Evaluating:  89%|████████▊ | 786/886 [04:51<00:36,  2.71it/s]Evaluating:  89%|████████▉ | 787/886 [04:51<00:36,  2.71it/s]Evaluating:  89%|████████▉ | 788/886 [04:52<00:36,  2.71it/s]Evaluating:  89%|████████▉ | 789/886 [04:52<00:35,  2.71it/s]Evaluating:  89%|████████▉ | 790/886 [04:52<00:35,  2.71it/s]Evaluating:  89%|████████▉ | 791/886 [04:53<00:35,  2.69it/s]Evaluating:  89%|████████▉ | 792/886 [04:53<00:34,  2.69it/s]Evaluating:  90%|████████▉ | 793/886 [04:53<00:34,  2.70it/s]Evaluating:  90%|████████▉ | 794/886 [04:54<00:33,  2.71it/s]Evaluating:  90%|████████▉ | 795/886 [04:54<00:33,  2.71it/s]Evaluating:  90%|████████▉ | 796/886 [04:55<00:33,  2.70it/s]Evaluating:  90%|████████▉ | 797/886 [04:55<00:32,  2.70it/s]Evaluating:  90%|█████████ | 798/886 [04:55<00:32,  2.70it/s]Evaluating:  90%|█████████ | 799/886 [04:56<00:32,  2.70it/s]Evaluating:  90%|█████████ | 800/886 [04:56<00:31,  2.70it/s]Evaluating:  90%|█████████ | 801/886 [04:56<00:31,  2.70it/s]Evaluating:  91%|█████████ | 802/886 [04:57<00:31,  2.70it/s]Evaluating:  91%|█████████ | 803/886 [04:57<00:30,  2.71it/s]Evaluating:  91%|█████████ | 804/886 [04:57<00:30,  2.71it/s]Evaluating:  91%|█████████ | 805/886 [04:58<00:29,  2.71it/s]Evaluating:  91%|█████████ | 806/886 [04:58<00:29,  2.71it/s]Evaluating:  91%|█████████ | 807/886 [04:59<00:29,  2.71it/s]Evaluating:  91%|█████████ | 808/886 [04:59<00:28,  2.71it/s]Evaluating:  91%|█████████▏| 809/886 [04:59<00:28,  2.71it/s]Evaluating:  91%|█████████▏| 810/886 [05:00<00:28,  2.71it/s]Evaluating:  92%|█████████▏| 811/886 [05:00<00:27,  2.69it/s]Evaluating:  92%|█████████▏| 812/886 [05:00<00:27,  2.69it/s]Evaluating:  92%|█████████▏| 813/886 [05:01<00:27,  2.68it/s]Evaluating:  92%|█████████▏| 814/886 [05:01<00:26,  2.69it/s]Evaluating:  92%|█████████▏| 815/886 [05:02<00:26,  2.70it/s]Evaluating:  92%|█████████▏| 816/886 [05:02<00:25,  2.71it/s]Evaluating:  92%|█████████▏| 817/886 [05:02<00:25,  2.71it/s]Evaluating:  92%|█████████▏| 818/886 [05:03<00:25,  2.70it/s]Evaluating:  92%|█████████▏| 819/886 [05:03<00:24,  2.70it/s]Evaluating:  93%|█████████▎| 820/886 [05:03<00:24,  2.70it/s]Evaluating:  93%|█████████▎| 821/886 [05:04<00:24,  2.70it/s]Evaluating:  93%|█████████▎| 822/886 [05:04<00:23,  2.69it/s]Evaluating:  93%|█████████▎| 823/886 [05:05<00:23,  2.68it/s]Evaluating:  93%|█████████▎| 824/886 [05:05<00:23,  2.68it/s]Evaluating:  93%|█████████▎| 825/886 [05:05<00:22,  2.68it/s]Evaluating:  93%|█████████▎| 826/886 [05:06<00:22,  2.68it/s]Evaluating:  93%|█████████▎| 827/886 [05:06<00:22,  2.67it/s]Evaluating:  93%|█████████▎| 828/886 [05:06<00:21,  2.68it/s]Evaluating:  94%|█████████▎| 829/886 [05:07<00:21,  2.68it/s]Evaluating:  94%|█████████▎| 830/886 [05:07<00:20,  2.69it/s]Evaluating:  94%|█████████▍| 831/886 [05:08<00:20,  2.69it/s]Evaluating:  94%|█████████▍| 832/886 [05:08<00:19,  2.70it/s]Evaluating:  94%|█████████▍| 833/886 [05:08<00:19,  2.71it/s]Evaluating:  94%|█████████▍| 834/886 [05:09<00:19,  2.69it/s]Evaluating:  94%|█████████▍| 835/886 [05:09<00:18,  2.69it/s]Evaluating:  94%|█████████▍| 836/886 [05:09<00:18,  2.70it/s]Evaluating:  94%|█████████▍| 837/886 [05:10<00:18,  2.69it/s]Evaluating:  95%|█████████▍| 838/886 [05:10<00:17,  2.70it/s]Evaluating:  95%|█████████▍| 839/886 [05:10<00:17,  2.69it/s]Evaluating:  95%|█████████▍| 840/886 [05:11<00:17,  2.70it/s]Evaluating:  95%|█████████▍| 841/886 [05:11<00:16,  2.70it/s]Evaluating:  95%|█████████▌| 842/886 [05:12<00:16,  2.70it/s]Evaluating:  95%|█████████▌| 843/886 [05:12<00:15,  2.70it/s]Evaluating:  95%|█████████▌| 844/886 [05:12<00:15,  2.70it/s]Evaluating:  95%|█████████▌| 845/886 [05:13<00:15,  2.71it/s]Evaluating:  95%|█████████▌| 846/886 [05:13<00:14,  2.72it/s]Evaluating:  96%|█████████▌| 847/886 [05:13<00:14,  2.72it/s]Evaluating:  96%|█████████▌| 848/886 [05:14<00:14,  2.71it/s]Evaluating:  96%|█████████▌| 849/886 [05:14<00:13,  2.71it/s]Evaluating:  96%|█████████▌| 850/886 [05:15<00:13,  2.71it/s]Evaluating:  96%|█████████▌| 851/886 [05:15<00:12,  2.70it/s]Evaluating:  96%|█████████▌| 852/886 [05:15<00:12,  2.71it/s]Evaluating:  96%|█████████▋| 853/886 [05:16<00:12,  2.70it/s]Evaluating:  96%|█████████▋| 854/886 [05:16<00:11,  2.71it/s]Evaluating:  97%|█████████▋| 855/886 [05:16<00:11,  2.71it/s]Evaluating:  97%|█████████▋| 856/886 [05:17<00:11,  2.72it/s]Evaluating:  97%|█████████▋| 857/886 [05:17<00:10,  2.73it/s]Evaluating:  97%|█████████▋| 858/886 [05:17<00:10,  2.72it/s]Evaluating:  97%|█████████▋| 859/886 [05:18<00:09,  2.72it/s]Evaluating:  97%|█████████▋| 860/886 [05:18<00:09,  2.72it/s]Evaluating:  97%|█████████▋| 861/886 [05:19<00:09,  2.73it/s]Evaluating:  97%|█████████▋| 862/886 [05:19<00:08,  2.73it/s]Evaluating:  97%|█████████▋| 863/886 [05:19<00:08,  2.72it/s]Evaluating:  98%|█████████▊| 864/886 [05:20<00:08,  2.72it/s]Evaluating:  98%|█████████▊| 865/886 [05:20<00:07,  2.72it/s]Evaluating:  98%|█████████▊| 866/886 [05:20<00:07,  2.71it/s]Evaluating:  98%|█████████▊| 867/886 [05:21<00:07,  2.71it/s]Evaluating:  98%|█████████▊| 868/886 [05:21<00:06,  2.72it/s]Evaluating:  98%|█████████▊| 869/886 [05:22<00:06,  2.72it/s]Evaluating:  98%|█████████▊| 870/886 [05:22<00:05,  2.72it/s]Evaluating:  98%|█████████▊| 871/886 [05:22<00:05,  2.71it/s]Evaluating:  98%|█████████▊| 872/886 [05:23<00:05,  2.70it/s]Evaluating:  99%|█████████▊| 873/886 [05:23<00:04,  2.71it/s]Evaluating:  99%|█████████▊| 874/886 [05:23<00:04,  2.71it/s]Evaluating:  99%|█████████▉| 875/886 [05:24<00:04,  2.72it/s]Evaluating:  99%|█████████▉| 876/886 [05:24<00:03,  2.72it/s]Evaluating:  99%|█████████▉| 877/886 [05:24<00:03,  2.72it/s]Evaluating:  99%|█████████▉| 878/886 [05:25<00:02,  2.72it/s]Evaluating:  99%|█████████▉| 879/886 [05:25<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 880/886 [05:26<00:02,  2.72it/s]Evaluating:  99%|█████████▉| 881/886 [05:26<00:01,  2.73it/s]Evaluating: 100%|█████████▉| 882/886 [05:26<00:01,  2.73it/s]Evaluating: 100%|█████████▉| 883/886 [05:27<00:01,  2.71it/s]Evaluating: 100%|█████████▉| 884/886 [05:27<00:00,  2.71it/s]Evaluating: 100%|█████████▉| 885/886 [05:27<00:00,  2.71it/s]Evaluating: 100%|██████████| 886/886 [05:28<00:00,  3.15it/s]Evaluating: 100%|██████████| 886/886 [05:28<00:00,  2.70it/s]
05/10/2022 00:31:13 - INFO - __main__ -     Evaluation done in total 328.128611 secs (0.046320 sec per example)
05/10/2022 00:31:35 - INFO - __main__ -   Results: {'exact': 51.362342415616105, 'f1': 69.33947990018237, 'total': 4918, 'HasAns_exact': 51.362342415616105, 'HasAns_f1': 69.33947990018237, 'HasAns_total': 4918, 'best_exact': 51.362342415616105, 'best_exact_thresh': 0.0, 'best_f1': 69.33947990018237, 'best_f1_thresh': 0.0}
  vi 
2022-05-10 00:31:39.324189: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/10/2022 00:31:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.value.weight', 'qa_outputs.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.21.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.18.attention.self.key.weight', 'pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:32:07 - INFO - __main__ -   lang2id = None
05/10/2022 00:32:11 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='vi', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-vi-question-vi.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/10/2022 00:32:11 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/10/2022 00:32:11 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.value.weight', 'qa_outputs.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.21.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.18.attention.self.key.weight', 'pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:32:37 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/2454 [00:00<?, ?it/s]  3%|▎         | 66/2454 [00:00<00:04, 525.45it/s]  7%|▋         | 171/2454 [00:00<00:02, 783.78it/s] 11%|█         | 271/2454 [00:00<00:02, 853.38it/s] 15%|█▍        | 358/2454 [00:00<00:03, 663.77it/s] 18%|█▊        | 433/2454 [00:00<00:02, 682.33it/s] 21%|██        | 510/2454 [00:00<00:02, 689.77it/s] 26%|██▌       | 626/2454 [00:00<00:02, 824.72it/s] 30%|███       | 737/2454 [00:00<00:01, 904.79it/s] 34%|███▍      | 831/2454 [00:01<00:02, 640.54it/s] 37%|███▋      | 908/2454 [00:01<00:02, 627.52it/s] 40%|████      | 987/2454 [00:01<00:02, 663.45it/s] 43%|████▎     | 1061/2454 [00:01<00:02, 656.44it/s] 46%|████▌     | 1132/2454 [00:01<00:02, 623.14it/s] 49%|████▉     | 1204/2454 [00:01<00:01, 646.26it/s] 52%|█████▏    | 1273/2454 [00:01<00:01, 657.72it/s] 56%|█████▌    | 1371/2454 [00:01<00:01, 728.75it/s] 59%|█████▉    | 1446/2454 [00:02<00:01, 730.45it/s] 65%|██████▍   | 1586/2454 [00:02<00:00, 907.35it/s] 70%|██████▉   | 1717/2454 [00:02<00:00, 1021.53it/s] 74%|███████▍  | 1821/2454 [00:02<00:00, 744.52it/s]  81%|████████  | 1984/2454 [00:02<00:00, 939.04it/s] 85%|████████▌ | 2092/2454 [00:02<00:00, 916.47it/s] 93%|█████████▎| 2287/2454 [00:02<00:00, 1172.46it/s]100%|██████████| 2454/2454 [00:02<00:00, 843.50it/s] 
convert squad examples to features:   0%|          | 0/5495 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/5495 [00:00<21:14,  4.31it/s]convert squad examples to features:   1%|          | 33/5495 [00:00<00:59, 91.95it/s]convert squad examples to features:   1%|          | 65/5495 [00:00<00:54, 99.50it/s]convert squad examples to features:   2%|▏         | 97/5495 [00:00<00:44, 122.62it/s]convert squad examples to features:   2%|▏         | 129/5495 [00:01<00:38, 137.68it/s]convert squad examples to features:   3%|▎         | 161/5495 [00:01<00:58, 91.52it/s] convert squad examples to features:   4%|▎         | 193/5495 [00:02<01:01, 85.86it/s]convert squad examples to features:   4%|▍         | 225/5495 [00:02<00:53, 98.85it/s]convert squad examples to features:   5%|▍         | 257/5495 [00:02<00:53, 98.24it/s]convert squad examples to features:   5%|▌         | 289/5495 [00:02<00:51, 101.04it/s]convert squad examples to features:   6%|▌         | 321/5495 [00:03<00:43, 119.41it/s]convert squad examples to features:   6%|▋         | 353/5495 [00:03<00:37, 136.21it/s]convert squad examples to features:   7%|▋         | 385/5495 [00:03<00:40, 126.60it/s]convert squad examples to features:   8%|▊         | 417/5495 [00:03<00:37, 135.35it/s]convert squad examples to features:   8%|▊         | 449/5495 [00:03<00:37, 133.65it/s]convert squad examples to features:   9%|▉         | 481/5495 [00:04<00:42, 117.95it/s]convert squad examples to features:   9%|▉         | 513/5495 [00:04<00:45, 110.65it/s]convert squad examples to features:  10%|▉         | 545/5495 [00:05<00:47, 105.30it/s]convert squad examples to features:  11%|█         | 577/5495 [00:05<00:43, 111.91it/s]convert squad examples to features:  11%|█         | 609/5495 [00:05<00:47, 103.35it/s]convert squad examples to features:  12%|█▏        | 641/5495 [00:05<00:38, 126.45it/s]convert squad examples to features:  12%|█▏        | 673/5495 [00:05<00:31, 151.04it/s]convert squad examples to features:  13%|█▎        | 705/5495 [00:06<00:32, 149.24it/s]convert squad examples to features:  13%|█▎        | 737/5495 [00:06<00:38, 123.83it/s]convert squad examples to features:  14%|█▍        | 769/5495 [00:06<00:38, 123.14it/s]convert squad examples to features:  15%|█▍        | 801/5495 [00:07<00:53, 87.19it/s] convert squad examples to features:  15%|█▌        | 833/5495 [00:07<00:51, 91.28it/s]convert squad examples to features:  16%|█▌        | 865/5495 [00:07<00:40, 115.60it/s]convert squad examples to features:  16%|█▋        | 897/5495 [00:07<00:35, 128.98it/s]convert squad examples to features:  17%|█▋        | 929/5495 [00:08<01:06, 68.58it/s] convert squad examples to features:  17%|█▋        | 961/5495 [00:09<01:01, 73.57it/s]convert squad examples to features:  18%|█▊        | 993/5495 [00:09<00:55, 80.97it/s]convert squad examples to features:  19%|█▊        | 1025/5495 [00:09<00:46, 95.90it/s]convert squad examples to features:  19%|█▉        | 1057/5495 [00:10<00:49, 89.19it/s]convert squad examples to features:  20%|█▉        | 1089/5495 [00:10<00:41, 105.89it/s]convert squad examples to features:  20%|██        | 1121/5495 [00:10<00:45, 95.15it/s] convert squad examples to features:  21%|██        | 1153/5495 [00:10<00:41, 103.47it/s]convert squad examples to features:  22%|██▏       | 1185/5495 [00:11<00:39, 109.89it/s]convert squad examples to features:  22%|██▏       | 1217/5495 [00:11<00:44, 96.31it/s] convert squad examples to features:  23%|██▎       | 1249/5495 [00:12<00:50, 84.64it/s]convert squad examples to features:  23%|██▎       | 1281/5495 [00:12<00:40, 103.31it/s]convert squad examples to features:  24%|██▍       | 1313/5495 [00:12<00:36, 115.44it/s]convert squad examples to features:  24%|██▍       | 1345/5495 [00:12<00:35, 116.80it/s]convert squad examples to features:  25%|██▌       | 1377/5495 [00:13<00:48, 85.31it/s] convert squad examples to features:  26%|██▌       | 1409/5495 [00:13<00:42, 95.97it/s]convert squad examples to features:  26%|██▌       | 1441/5495 [00:13<00:40, 100.74it/s]convert squad examples to features:  27%|██▋       | 1473/5495 [00:14<00:41, 96.67it/s] convert squad examples to features:  27%|██▋       | 1505/5495 [00:14<00:33, 117.65it/s]convert squad examples to features:  28%|██▊       | 1537/5495 [00:14<00:37, 106.25it/s]convert squad examples to features:  29%|██▊       | 1569/5495 [00:14<00:31, 122.73it/s]convert squad examples to features:  29%|██▉       | 1601/5495 [00:15<00:30, 127.74it/s]convert squad examples to features:  30%|██▉       | 1633/5495 [00:15<00:27, 138.67it/s]convert squad examples to features:  30%|███       | 1665/5495 [00:15<00:30, 126.19it/s]convert squad examples to features:  31%|███       | 1697/5495 [00:15<00:31, 121.42it/s]convert squad examples to features:  31%|███▏      | 1729/5495 [00:16<00:31, 119.36it/s]convert squad examples to features:  32%|███▏      | 1761/5495 [00:16<00:28, 132.46it/s]convert squad examples to features:  33%|███▎      | 1793/5495 [00:16<00:32, 112.71it/s]convert squad examples to features:  33%|███▎      | 1825/5495 [00:17<00:50, 72.56it/s] convert squad examples to features:  34%|███▍      | 1857/5495 [00:17<00:49, 74.14it/s]convert squad examples to features:  34%|███▍      | 1889/5495 [00:18<00:42, 83.89it/s]convert squad examples to features:  35%|███▍      | 1921/5495 [00:18<00:50, 70.85it/s]convert squad examples to features:  36%|███▌      | 1953/5495 [00:19<00:43, 80.83it/s]convert squad examples to features:  36%|███▌      | 1985/5495 [00:19<00:39, 89.95it/s]convert squad examples to features:  37%|███▋      | 2017/5495 [00:19<00:38, 90.13it/s]convert squad examples to features:  37%|███▋      | 2049/5495 [00:20<00:39, 87.93it/s]convert squad examples to features:  38%|███▊      | 2081/5495 [00:20<00:39, 87.05it/s]convert squad examples to features:  38%|███▊      | 2113/5495 [00:20<00:38, 87.30it/s]convert squad examples to features:  39%|███▉      | 2145/5495 [00:21<00:56, 58.79it/s]convert squad examples to features:  40%|███▉      | 2177/5495 [00:22<00:49, 67.22it/s]convert squad examples to features:  40%|████      | 2209/5495 [00:22<00:43, 75.69it/s]convert squad examples to features:  41%|████      | 2241/5495 [00:22<00:34, 93.33it/s]convert squad examples to features:  41%|████▏     | 2273/5495 [00:22<00:28, 113.21it/s]convert squad examples to features:  42%|████▏     | 2305/5495 [00:23<00:30, 105.76it/s]convert squad examples to features:  43%|████▎     | 2337/5495 [00:23<00:29, 108.42it/s]convert squad examples to features:  43%|████▎     | 2369/5495 [00:23<00:35, 87.46it/s] convert squad examples to features:  44%|████▎     | 2401/5495 [00:24<00:28, 107.22it/s]convert squad examples to features:  44%|████▍     | 2433/5495 [00:24<00:32, 94.29it/s] convert squad examples to features:  45%|████▍     | 2465/5495 [00:24<00:28, 107.71it/s]convert squad examples to features:  45%|████▌     | 2497/5495 [00:25<00:29, 102.17it/s]convert squad examples to features:  46%|████▌     | 2529/5495 [00:25<00:25, 118.48it/s]convert squad examples to features:  47%|████▋     | 2561/5495 [00:25<00:29, 100.99it/s]convert squad examples to features:  47%|████▋     | 2593/5495 [00:26<00:32, 89.90it/s] convert squad examples to features:  48%|████▊     | 2625/5495 [00:26<00:32, 88.05it/s]convert squad examples to features:  48%|████▊     | 2657/5495 [00:26<00:31, 90.40it/s]convert squad examples to features:  49%|████▉     | 2689/5495 [00:27<00:37, 73.91it/s]convert squad examples to features:  50%|████▉     | 2721/5495 [00:27<00:31, 86.98it/s]convert squad examples to features:  50%|█████     | 2753/5495 [00:28<00:35, 77.85it/s]convert squad examples to features:  51%|█████     | 2785/5495 [00:28<00:34, 79.30it/s]convert squad examples to features:  51%|█████▏    | 2817/5495 [00:28<00:31, 84.82it/s]convert squad examples to features:  52%|█████▏    | 2849/5495 [00:29<00:27, 95.80it/s]convert squad examples to features:  52%|█████▏    | 2881/5495 [00:29<00:24, 105.45it/s]convert squad examples to features:  53%|█████▎    | 2913/5495 [00:29<00:28, 89.24it/s] convert squad examples to features:  54%|█████▎    | 2945/5495 [00:30<00:29, 87.17it/s]convert squad examples to features:  54%|█████▍    | 2977/5495 [00:30<00:24, 103.49it/s]convert squad examples to features:  55%|█████▍    | 3009/5495 [00:30<00:25, 97.48it/s] convert squad examples to features:  55%|█████▌    | 3041/5495 [00:30<00:22, 108.99it/s]convert squad examples to features:  56%|█████▌    | 3073/5495 [00:31<00:24, 99.69it/s] convert squad examples to features:  57%|█████▋    | 3105/5495 [00:31<00:26, 88.95it/s]convert squad examples to features:  57%|█████▋    | 3137/5495 [00:32<00:23, 99.64it/s]convert squad examples to features:  58%|█████▊    | 3169/5495 [00:32<00:24, 95.77it/s]convert squad examples to features:  58%|█████▊    | 3201/5495 [00:32<00:20, 109.73it/s]convert squad examples to features:  59%|█████▉    | 3233/5495 [00:32<00:20, 107.92it/s]convert squad examples to features:  59%|█████▉    | 3265/5495 [00:33<00:20, 107.28it/s]convert squad examples to features:  60%|██████    | 3297/5495 [00:33<00:19, 110.55it/s]convert squad examples to features:  61%|██████    | 3329/5495 [00:33<00:24, 89.26it/s] convert squad examples to features:  61%|██████    | 3361/5495 [00:34<00:24, 86.84it/s]convert squad examples to features:  62%|██████▏   | 3393/5495 [00:34<00:26, 78.12it/s]convert squad examples to features:  62%|██████▏   | 3425/5495 [00:35<00:23, 89.24it/s]convert squad examples to features:  63%|██████▎   | 3457/5495 [00:35<00:19, 105.28it/s]convert squad examples to features:  63%|██████▎   | 3489/5495 [00:35<00:21, 93.88it/s] convert squad examples to features:  64%|██████▍   | 3521/5495 [00:35<00:19, 102.52it/s]convert squad examples to features:  65%|██████▍   | 3553/5495 [00:36<00:19, 97.80it/s] convert squad examples to features:  65%|██████▌   | 3585/5495 [00:36<00:18, 102.06it/s]convert squad examples to features:  66%|██████▌   | 3617/5495 [00:36<00:17, 108.09it/s]convert squad examples to features:  66%|██████▋   | 3649/5495 [00:37<00:17, 105.80it/s]convert squad examples to features:  67%|██████▋   | 3681/5495 [00:38<00:35, 50.80it/s] convert squad examples to features:  68%|██████▊   | 3713/5495 [00:38<00:28, 63.23it/s]convert squad examples to features:  68%|██████▊   | 3745/5495 [00:39<00:24, 72.85it/s]convert squad examples to features:  69%|██████▊   | 3777/5495 [00:39<00:22, 77.98it/s]convert squad examples to features:  69%|██████▉   | 3809/5495 [00:39<00:20, 80.32it/s]convert squad examples to features:  70%|██████▉   | 3841/5495 [00:40<00:22, 72.95it/s]convert squad examples to features:  70%|███████   | 3873/5495 [00:40<00:20, 81.08it/s]convert squad examples to features:  71%|███████   | 3905/5495 [00:40<00:16, 99.37it/s]convert squad examples to features:  72%|███████▏  | 3937/5495 [00:40<00:14, 106.53it/s]convert squad examples to features:  72%|███████▏  | 3969/5495 [00:41<00:12, 121.45it/s]convert squad examples to features:  73%|███████▎  | 4001/5495 [00:41<00:11, 132.95it/s]convert squad examples to features:  73%|███████▎  | 4033/5495 [00:41<00:11, 125.22it/s]convert squad examples to features:  74%|███████▍  | 4065/5495 [00:41<00:12, 115.17it/s]convert squad examples to features:  75%|███████▍  | 4097/5495 [00:42<00:11, 120.66it/s]convert squad examples to features:  75%|███████▌  | 4129/5495 [00:42<00:11, 115.41it/s]convert squad examples to features:  76%|███████▌  | 4161/5495 [00:42<00:10, 123.08it/s]convert squad examples to features:  76%|███████▋  | 4193/5495 [00:43<00:10, 120.96it/s]convert squad examples to features:  77%|███████▋  | 4225/5495 [00:43<00:12, 104.93it/s]convert squad examples to features:  77%|███████▋  | 4257/5495 [00:43<00:10, 113.60it/s]convert squad examples to features:  78%|███████▊  | 4289/5495 [00:43<00:08, 134.78it/s]convert squad examples to features:  79%|███████▊  | 4321/5495 [00:44<00:08, 133.88it/s]convert squad examples to features:  79%|███████▉  | 4353/5495 [00:44<00:07, 156.13it/s]convert squad examples to features:  80%|███████▉  | 4385/5495 [00:44<00:09, 111.59it/s]convert squad examples to features:  80%|████████  | 4417/5495 [00:45<00:11, 90.95it/s] convert squad examples to features:  81%|████████  | 4449/5495 [00:46<00:16, 62.43it/s]convert squad examples to features:  82%|████████▏ | 4481/5495 [00:46<00:14, 69.20it/s]convert squad examples to features:  82%|████████▏ | 4513/5495 [00:46<00:12, 81.67it/s]convert squad examples to features:  83%|████████▎ | 4545/5495 [00:46<00:10, 88.72it/s]convert squad examples to features:  83%|████████▎ | 4577/5495 [00:47<00:09, 91.87it/s]convert squad examples to features:  84%|████████▍ | 4609/5495 [00:47<00:07, 113.77it/s]convert squad examples to features:  84%|████████▍ | 4641/5495 [00:47<00:06, 130.81it/s]convert squad examples to features:  85%|████████▌ | 4673/5495 [00:47<00:06, 131.74it/s]convert squad examples to features:  86%|████████▌ | 4705/5495 [00:47<00:05, 143.90it/s]convert squad examples to features:  86%|████████▌ | 4737/5495 [00:48<00:04, 157.92it/s]convert squad examples to features:  87%|████████▋ | 4769/5495 [00:48<00:04, 153.65it/s]convert squad examples to features:  87%|████████▋ | 4801/5495 [00:48<00:04, 160.40it/s]convert squad examples to features:  88%|████████▊ | 4833/5495 [00:48<00:03, 166.16it/s]convert squad examples to features:  89%|████████▊ | 4865/5495 [00:48<00:04, 153.03it/s]convert squad examples to features:  89%|████████▉ | 4897/5495 [00:49<00:05, 119.44it/s]convert squad examples to features:  90%|████████▉ | 4929/5495 [00:49<00:04, 120.67it/s]convert squad examples to features:  90%|█████████ | 4961/5495 [00:49<00:04, 114.85it/s]convert squad examples to features:  91%|█████████ | 4993/5495 [00:50<00:04, 113.85it/s]convert squad examples to features:  91%|█████████▏| 5025/5495 [00:50<00:03, 120.53it/s]convert squad examples to features:  92%|█████████▏| 5057/5495 [00:50<00:03, 134.10it/s]convert squad examples to features:  93%|█████████▎| 5089/5495 [00:50<00:02, 151.42it/s]convert squad examples to features:  93%|█████████▎| 5121/5495 [00:50<00:02, 161.03it/s]convert squad examples to features:  94%|█████████▍| 5153/5495 [00:51<00:02, 153.25it/s]convert squad examples to features:  94%|█████████▍| 5185/5495 [00:51<00:01, 164.88it/s]convert squad examples to features:  95%|█████████▍| 5217/5495 [00:51<00:01, 164.59it/s]convert squad examples to features:  96%|█████████▌| 5249/5495 [00:51<00:01, 183.51it/s]convert squad examples to features:  96%|█████████▌| 5281/5495 [00:51<00:01, 152.39it/s]convert squad examples to features:  97%|█████████▋| 5313/5495 [00:52<00:01, 151.96it/s]convert squad examples to features:  97%|█████████▋| 5345/5495 [00:52<00:00, 174.24it/s]convert squad examples to features:  98%|█████████▊| 5377/5495 [00:52<00:00, 190.96it/s]convert squad examples to features:  98%|█████████▊| 5409/5495 [00:52<00:00, 205.15it/s]convert squad examples to features:  99%|█████████▉| 5441/5495 [00:52<00:00, 201.37it/s]convert squad examples to features: 100%|█████████▉| 5473/5495 [00:52<00:00, 218.94it/s]convert squad examples to features: 100%|██████████| 5495/5495 [00:52<00:00, 104.20it/s]
add example index and unique id:   0%|          | 0/5495 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 5495/5495 [00:00<00:00, 471564.20it/s]
05/10/2022 00:33:33 - INFO - __main__ -   Saving features into cached file ./cached_test-context-vi-question-vi.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_vi
05/10/2022 00:33:44 - INFO - __main__ -   ***** Running evaluation  *****
05/10/2022 00:33:44 - INFO - __main__ -     Num examples = 8315
05/10/2022 00:33:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/1040 [00:00<?, ?it/s]Evaluating:   0%|          | 1/1040 [00:01<17:59,  1.04s/it]Evaluating:   0%|          | 2/1040 [00:01<11:05,  1.56it/s]Evaluating:   0%|          | 3/1040 [00:01<08:53,  1.94it/s]Evaluating:   0%|          | 4/1040 [00:02<07:52,  2.19it/s]Evaluating:   0%|          | 5/1040 [00:02<07:18,  2.36it/s]Evaluating:   1%|          | 6/1040 [00:02<06:56,  2.48it/s]Evaluating:   1%|          | 7/1040 [00:03<06:42,  2.56it/s]Evaluating:   1%|          | 8/1040 [00:03<06:35,  2.61it/s]Evaluating:   1%|          | 9/1040 [00:03<06:29,  2.65it/s]Evaluating:   1%|          | 10/1040 [00:04<06:25,  2.67it/s]Evaluating:   1%|          | 11/1040 [00:04<06:21,  2.70it/s]Evaluating:   1%|          | 12/1040 [00:05<06:18,  2.71it/s]Evaluating:   1%|▏         | 13/1040 [00:05<06:17,  2.72it/s]Evaluating:   1%|▏         | 14/1040 [00:05<06:16,  2.72it/s]Evaluating:   1%|▏         | 15/1040 [00:06<06:14,  2.74it/s]Evaluating:   2%|▏         | 16/1040 [00:06<06:13,  2.74it/s]Evaluating:   2%|▏         | 17/1040 [00:06<06:13,  2.74it/s]Evaluating:   2%|▏         | 18/1040 [00:07<06:12,  2.74it/s]Evaluating:   2%|▏         | 19/1040 [00:07<06:11,  2.75it/s]Evaluating:   2%|▏         | 20/1040 [00:07<06:11,  2.75it/s]Evaluating:   2%|▏         | 21/1040 [00:08<06:10,  2.75it/s]Evaluating:   2%|▏         | 22/1040 [00:08<06:08,  2.76it/s]Evaluating:   2%|▏         | 23/1040 [00:09<06:08,  2.76it/s]Evaluating:   2%|▏         | 24/1040 [00:09<06:08,  2.76it/s]Evaluating:   2%|▏         | 25/1040 [00:09<06:23,  2.64it/s]Evaluating:   2%|▎         | 26/1040 [00:10<06:18,  2.68it/s]Evaluating:   3%|▎         | 27/1040 [00:10<06:15,  2.70it/s]Evaluating:   3%|▎         | 28/1040 [00:10<06:13,  2.71it/s]Evaluating:   3%|▎         | 29/1040 [00:11<06:11,  2.72it/s]Evaluating:   3%|▎         | 30/1040 [00:11<06:10,  2.73it/s]Evaluating:   3%|▎         | 31/1040 [00:12<06:09,  2.73it/s]Evaluating:   3%|▎         | 32/1040 [00:12<06:08,  2.74it/s]Evaluating:   3%|▎         | 33/1040 [00:12<06:07,  2.74it/s]Evaluating:   3%|▎         | 34/1040 [00:13<06:08,  2.73it/s]Evaluating:   3%|▎         | 35/1040 [00:13<06:08,  2.72it/s]Evaluating:   3%|▎         | 36/1040 [00:13<06:07,  2.73it/s]Evaluating:   4%|▎         | 37/1040 [00:14<06:07,  2.73it/s]Evaluating:   4%|▎         | 38/1040 [00:14<06:06,  2.74it/s]Evaluating:   4%|▍         | 39/1040 [00:14<06:04,  2.74it/s]Evaluating:   4%|▍         | 40/1040 [00:15<06:03,  2.75it/s]Evaluating:   4%|▍         | 41/1040 [00:15<06:03,  2.75it/s]Evaluating:   4%|▍         | 42/1040 [00:16<06:04,  2.73it/s]Evaluating:   4%|▍         | 43/1040 [00:16<06:04,  2.74it/s]Evaluating:   4%|▍         | 44/1040 [00:16<06:02,  2.75it/s]Evaluating:   4%|▍         | 45/1040 [00:17<06:00,  2.76it/s]Evaluating:   4%|▍         | 46/1040 [00:17<06:00,  2.76it/s]Evaluating:   5%|▍         | 47/1040 [00:17<06:01,  2.75it/s]Evaluating:   5%|▍         | 48/1040 [00:18<06:00,  2.75it/s]Evaluating:   5%|▍         | 49/1040 [00:18<06:00,  2.75it/s]Evaluating:   5%|▍         | 50/1040 [00:18<06:00,  2.75it/s]Evaluating:   5%|▍         | 51/1040 [00:19<06:01,  2.74it/s]Evaluating:   5%|▌         | 52/1040 [00:19<06:01,  2.74it/s]Evaluating:   5%|▌         | 53/1040 [00:20<06:02,  2.73it/s]Evaluating:   5%|▌         | 54/1040 [00:20<06:00,  2.74it/s]Evaluating:   5%|▌         | 55/1040 [00:20<05:57,  2.75it/s]Evaluating:   5%|▌         | 56/1040 [00:21<05:57,  2.76it/s]Evaluating:   5%|▌         | 57/1040 [00:21<05:56,  2.75it/s]Evaluating:   6%|▌         | 58/1040 [00:21<05:57,  2.75it/s]Evaluating:   6%|▌         | 59/1040 [00:22<05:56,  2.75it/s]Evaluating:   6%|▌         | 60/1040 [00:22<05:55,  2.75it/s]Evaluating:   6%|▌         | 61/1040 [00:22<05:55,  2.76it/s]Evaluating:   6%|▌         | 62/1040 [00:23<05:55,  2.75it/s]Evaluating:   6%|▌         | 63/1040 [00:23<05:54,  2.76it/s]Evaluating:   6%|▌         | 64/1040 [00:24<05:54,  2.75it/s]Evaluating:   6%|▋         | 65/1040 [00:24<05:53,  2.76it/s]Evaluating:   6%|▋         | 66/1040 [00:24<05:53,  2.75it/s]Evaluating:   6%|▋         | 67/1040 [00:25<05:52,  2.76it/s]Evaluating:   7%|▋         | 68/1040 [00:25<05:55,  2.74it/s]Evaluating:   7%|▋         | 69/1040 [00:25<05:54,  2.74it/s]Evaluating:   7%|▋         | 70/1040 [00:26<05:54,  2.74it/s]Evaluating:   7%|▋         | 71/1040 [00:26<05:54,  2.73it/s]Evaluating:   7%|▋         | 72/1040 [00:26<05:54,  2.73it/s]Evaluating:   7%|▋         | 73/1040 [00:27<05:53,  2.74it/s]Evaluating:   7%|▋         | 74/1040 [00:27<05:54,  2.73it/s]Evaluating:   7%|▋         | 75/1040 [00:28<05:52,  2.73it/s]Evaluating:   7%|▋         | 76/1040 [00:28<05:53,  2.73it/s]Evaluating:   7%|▋         | 77/1040 [00:28<05:52,  2.73it/s]Evaluating:   8%|▊         | 78/1040 [00:29<05:52,  2.73it/s]Evaluating:   8%|▊         | 79/1040 [00:29<05:51,  2.73it/s]Evaluating:   8%|▊         | 80/1040 [00:29<05:51,  2.73it/s]Evaluating:   8%|▊         | 81/1040 [00:30<05:51,  2.73it/s]Evaluating:   8%|▊         | 82/1040 [00:30<05:51,  2.73it/s]Evaluating:   8%|▊         | 83/1040 [00:30<05:50,  2.73it/s]Evaluating:   8%|▊         | 84/1040 [00:31<05:50,  2.73it/s]Evaluating:   8%|▊         | 85/1040 [00:31<05:50,  2.73it/s]Evaluating:   8%|▊         | 86/1040 [00:32<05:50,  2.72it/s]Evaluating:   8%|▊         | 87/1040 [00:32<05:50,  2.72it/s]Evaluating:   8%|▊         | 88/1040 [00:32<05:50,  2.72it/s]Evaluating:   9%|▊         | 89/1040 [00:33<05:48,  2.73it/s]Evaluating:   9%|▊         | 90/1040 [00:33<05:47,  2.74it/s]Evaluating:   9%|▉         | 91/1040 [00:33<05:48,  2.73it/s]Evaluating:   9%|▉         | 92/1040 [00:34<05:47,  2.73it/s]Evaluating:   9%|▉         | 93/1040 [00:34<05:46,  2.74it/s]Evaluating:   9%|▉         | 94/1040 [00:35<05:45,  2.74it/s]Evaluating:   9%|▉         | 95/1040 [00:35<05:46,  2.73it/s]Evaluating:   9%|▉         | 96/1040 [00:35<05:45,  2.74it/s]Evaluating:   9%|▉         | 97/1040 [00:36<05:44,  2.74it/s]Evaluating:   9%|▉         | 98/1040 [00:36<05:46,  2.72it/s]Evaluating:  10%|▉         | 99/1040 [00:36<05:44,  2.73it/s]Evaluating:  10%|▉         | 100/1040 [00:37<05:45,  2.72it/s]Evaluating:  10%|▉         | 101/1040 [00:37<05:45,  2.72it/s]Evaluating:  10%|▉         | 102/1040 [00:37<05:44,  2.72it/s]Evaluating:  10%|▉         | 103/1040 [00:38<05:42,  2.73it/s]Evaluating:  10%|█         | 104/1040 [00:38<05:41,  2.74it/s]Evaluating:  10%|█         | 105/1040 [00:39<05:43,  2.72it/s]Evaluating:  10%|█         | 106/1040 [00:39<05:43,  2.72it/s]Evaluating:  10%|█         | 107/1040 [00:39<05:43,  2.72it/s]Evaluating:  10%|█         | 108/1040 [00:40<05:43,  2.71it/s]Evaluating:  10%|█         | 109/1040 [00:40<05:42,  2.72it/s]Evaluating:  11%|█         | 110/1040 [00:40<05:41,  2.73it/s]Evaluating:  11%|█         | 111/1040 [00:41<05:39,  2.73it/s]Evaluating:  11%|█         | 112/1040 [00:41<05:39,  2.74it/s]Evaluating:  11%|█         | 113/1040 [00:41<05:38,  2.74it/s]Evaluating:  11%|█         | 114/1040 [00:42<05:38,  2.74it/s]Evaluating:  11%|█         | 115/1040 [00:42<05:40,  2.72it/s]Evaluating:  11%|█         | 116/1040 [00:43<05:41,  2.70it/s]Evaluating:  11%|█▏        | 117/1040 [00:43<05:40,  2.71it/s]Evaluating:  11%|█▏        | 118/1040 [00:43<05:39,  2.71it/s]Evaluating:  11%|█▏        | 119/1040 [00:44<05:37,  2.73it/s]Evaluating:  12%|█▏        | 120/1040 [00:44<05:36,  2.74it/s]Evaluating:  12%|█▏        | 121/1040 [00:44<05:35,  2.74it/s]Evaluating:  12%|█▏        | 122/1040 [00:45<05:35,  2.74it/s]Evaluating:  12%|█▏        | 123/1040 [00:45<05:34,  2.74it/s]Evaluating:  12%|█▏        | 124/1040 [00:46<05:34,  2.74it/s]Evaluating:  12%|█▏        | 125/1040 [00:46<05:32,  2.75it/s]Evaluating:  12%|█▏        | 126/1040 [00:46<05:33,  2.74it/s]Evaluating:  12%|█▏        | 127/1040 [00:47<05:32,  2.75it/s]Evaluating:  12%|█▏        | 128/1040 [00:47<05:32,  2.75it/s]Evaluating:  12%|█▏        | 129/1040 [00:47<05:31,  2.75it/s]Evaluating:  12%|█▎        | 130/1040 [00:48<05:31,  2.75it/s]Evaluating:  13%|█▎        | 131/1040 [00:48<05:31,  2.74it/s]Evaluating:  13%|█▎        | 132/1040 [00:48<05:32,  2.73it/s]Evaluating:  13%|█▎        | 133/1040 [00:49<05:32,  2.73it/s]Evaluating:  13%|█▎        | 134/1040 [00:49<05:34,  2.71it/s]Evaluating:  13%|█▎        | 135/1040 [00:50<05:33,  2.71it/s]Evaluating:  13%|█▎        | 136/1040 [00:50<05:34,  2.70it/s]Evaluating:  13%|█▎        | 137/1040 [00:50<05:33,  2.71it/s]Evaluating:  13%|█▎        | 138/1040 [00:51<05:32,  2.71it/s]Evaluating:  13%|█▎        | 139/1040 [00:51<05:31,  2.72it/s]Evaluating:  13%|█▎        | 140/1040 [00:51<05:30,  2.72it/s]Evaluating:  14%|█▎        | 141/1040 [00:52<05:29,  2.73it/s]Evaluating:  14%|█▎        | 142/1040 [00:52<05:28,  2.74it/s]Evaluating:  14%|█▍        | 143/1040 [00:52<05:28,  2.73it/s]Evaluating:  14%|█▍        | 144/1040 [00:53<05:28,  2.73it/s]Evaluating:  14%|█▍        | 145/1040 [00:53<05:29,  2.71it/s]Evaluating:  14%|█▍        | 146/1040 [00:54<05:30,  2.71it/s]Evaluating:  14%|█▍        | 147/1040 [00:54<05:29,  2.71it/s]Evaluating:  14%|█▍        | 148/1040 [00:54<05:29,  2.71it/s]Evaluating:  14%|█▍        | 149/1040 [00:55<05:28,  2.71it/s]Evaluating:  14%|█▍        | 150/1040 [00:55<05:27,  2.72it/s]Evaluating:  15%|█▍        | 151/1040 [00:55<05:26,  2.73it/s]Evaluating:  15%|█▍        | 152/1040 [00:56<05:25,  2.72it/s]Evaluating:  15%|█▍        | 153/1040 [00:56<05:25,  2.72it/s]Evaluating:  15%|█▍        | 154/1040 [00:57<05:25,  2.72it/s]Evaluating:  15%|█▍        | 155/1040 [00:57<05:24,  2.73it/s]Evaluating:  15%|█▌        | 156/1040 [00:57<05:23,  2.74it/s]Evaluating:  15%|█▌        | 157/1040 [00:58<05:22,  2.74it/s]Evaluating:  15%|█▌        | 158/1040 [00:58<05:23,  2.72it/s]Evaluating:  15%|█▌        | 159/1040 [00:58<05:23,  2.72it/s]Evaluating:  15%|█▌        | 160/1040 [00:59<05:23,  2.72it/s]Evaluating:  15%|█▌        | 161/1040 [00:59<05:22,  2.72it/s]Evaluating:  16%|█▌        | 162/1040 [00:59<05:20,  2.74it/s]Evaluating:  16%|█▌        | 163/1040 [01:00<05:19,  2.75it/s]Evaluating:  16%|█▌        | 164/1040 [01:00<05:18,  2.75it/s]Evaluating:  16%|█▌        | 165/1040 [01:01<05:19,  2.74it/s]Evaluating:  16%|█▌        | 166/1040 [01:01<05:18,  2.74it/s]Evaluating:  16%|█▌        | 167/1040 [01:01<05:17,  2.75it/s]Evaluating:  16%|█▌        | 168/1040 [01:02<05:16,  2.75it/s]Evaluating:  16%|█▋        | 169/1040 [01:02<05:17,  2.75it/s]Evaluating:  16%|█▋        | 170/1040 [01:02<05:17,  2.74it/s]Evaluating:  16%|█▋        | 171/1040 [01:03<05:18,  2.73it/s]Evaluating:  17%|█▋        | 172/1040 [01:03<05:18,  2.73it/s]Evaluating:  17%|█▋        | 173/1040 [01:03<05:18,  2.73it/s]Evaluating:  17%|█▋        | 174/1040 [01:04<05:17,  2.72it/s]Evaluating:  17%|█▋        | 175/1040 [01:04<05:18,  2.71it/s]Evaluating:  17%|█▋        | 176/1040 [01:05<05:19,  2.71it/s]Evaluating:  17%|█▋        | 177/1040 [01:05<05:18,  2.71it/s]Evaluating:  17%|█▋        | 178/1040 [01:05<05:19,  2.70it/s]Evaluating:  17%|█▋        | 179/1040 [01:06<05:19,  2.70it/s]Evaluating:  17%|█▋        | 180/1040 [01:06<05:18,  2.70it/s]Evaluating:  17%|█▋        | 181/1040 [01:06<05:17,  2.70it/s]Evaluating:  18%|█▊        | 182/1040 [01:07<05:17,  2.70it/s]Evaluating:  18%|█▊        | 183/1040 [01:07<05:17,  2.70it/s]Evaluating:  18%|█▊        | 184/1040 [01:08<05:14,  2.72it/s]Evaluating:  18%|█▊        | 185/1040 [01:08<05:13,  2.73it/s]Evaluating:  18%|█▊        | 186/1040 [01:08<05:14,  2.71it/s]Evaluating:  18%|█▊        | 187/1040 [01:09<05:14,  2.71it/s]Evaluating:  18%|█▊        | 188/1040 [01:09<05:13,  2.72it/s]Evaluating:  18%|█▊        | 189/1040 [01:09<05:11,  2.73it/s]Evaluating:  18%|█▊        | 190/1040 [01:10<05:12,  2.72it/s]Evaluating:  18%|█▊        | 191/1040 [01:10<05:11,  2.72it/s]Evaluating:  18%|█▊        | 192/1040 [01:10<05:12,  2.71it/s]Evaluating:  19%|█▊        | 193/1040 [01:11<05:11,  2.72it/s]Evaluating:  19%|█▊        | 194/1040 [01:11<05:10,  2.73it/s]Evaluating:  19%|█▉        | 195/1040 [01:12<05:09,  2.73it/s]Evaluating:  19%|█▉        | 196/1040 [01:12<05:08,  2.73it/s]Evaluating:  19%|█▉        | 197/1040 [01:12<05:07,  2.74it/s]Evaluating:  19%|█▉        | 198/1040 [01:13<05:07,  2.74it/s]Evaluating:  19%|█▉        | 199/1040 [01:13<05:07,  2.74it/s]Evaluating:  19%|█▉        | 200/1040 [01:13<05:08,  2.72it/s]Evaluating:  19%|█▉        | 201/1040 [01:14<05:09,  2.71it/s]Evaluating:  19%|█▉        | 202/1040 [01:14<05:08,  2.71it/s]Evaluating:  20%|█▉        | 203/1040 [01:15<05:07,  2.72it/s]Evaluating:  20%|█▉        | 204/1040 [01:15<05:07,  2.72it/s]Evaluating:  20%|█▉        | 205/1040 [01:15<05:14,  2.65it/s]Evaluating:  20%|█▉        | 206/1040 [01:16<05:11,  2.67it/s]Evaluating:  20%|█▉        | 207/1040 [01:16<05:09,  2.70it/s]Evaluating:  20%|██        | 208/1040 [01:16<05:07,  2.71it/s]Evaluating:  20%|██        | 209/1040 [01:17<05:06,  2.72it/s]Evaluating:  20%|██        | 210/1040 [01:17<05:05,  2.71it/s]Evaluating:  20%|██        | 211/1040 [01:17<05:06,  2.71it/s]Evaluating:  20%|██        | 212/1040 [01:18<05:05,  2.71it/s]Evaluating:  20%|██        | 213/1040 [01:18<05:05,  2.71it/s]Evaluating:  21%|██        | 214/1040 [01:19<05:05,  2.70it/s]Evaluating:  21%|██        | 215/1040 [01:19<05:05,  2.70it/s]Evaluating:  21%|██        | 216/1040 [01:19<05:04,  2.71it/s]Evaluating:  21%|██        | 217/1040 [01:20<05:02,  2.72it/s]Evaluating:  21%|██        | 218/1040 [01:20<05:01,  2.72it/s]Evaluating:  21%|██        | 219/1040 [01:20<05:01,  2.72it/s]Evaluating:  21%|██        | 220/1040 [01:21<05:00,  2.73it/s]Evaluating:  21%|██▏       | 221/1040 [01:21<04:59,  2.73it/s]Evaluating:  21%|██▏       | 222/1040 [01:22<04:59,  2.73it/s]Evaluating:  21%|██▏       | 223/1040 [01:22<04:59,  2.73it/s]Evaluating:  22%|██▏       | 224/1040 [01:22<04:59,  2.73it/s]Evaluating:  22%|██▏       | 225/1040 [01:23<04:58,  2.73it/s]Evaluating:  22%|██▏       | 226/1040 [01:23<04:57,  2.74it/s]Evaluating:  22%|██▏       | 227/1040 [01:23<04:57,  2.73it/s]Evaluating:  22%|██▏       | 228/1040 [01:24<04:58,  2.72it/s]Evaluating:  22%|██▏       | 229/1040 [01:24<04:59,  2.71it/s]Evaluating:  22%|██▏       | 230/1040 [01:24<04:58,  2.71it/s]Evaluating:  22%|██▏       | 231/1040 [01:25<04:58,  2.71it/s]Evaluating:  22%|██▏       | 232/1040 [01:25<04:58,  2.71it/s]Evaluating:  22%|██▏       | 233/1040 [01:26<04:56,  2.72it/s]Evaluating:  22%|██▎       | 234/1040 [01:26<04:56,  2.72it/s]Evaluating:  23%|██▎       | 235/1040 [01:26<04:56,  2.71it/s]Evaluating:  23%|██▎       | 236/1040 [01:27<04:55,  2.72it/s]Evaluating:  23%|██▎       | 237/1040 [01:27<04:54,  2.72it/s]Evaluating:  23%|██▎       | 238/1040 [01:27<04:53,  2.73it/s]Evaluating:  23%|██▎       | 239/1040 [01:28<04:55,  2.71it/s]Evaluating:  23%|██▎       | 240/1040 [01:28<04:55,  2.71it/s]Evaluating:  23%|██▎       | 241/1040 [01:29<04:55,  2.70it/s]Evaluating:  23%|██▎       | 242/1040 [01:29<04:54,  2.71it/s]Evaluating:  23%|██▎       | 243/1040 [01:29<04:52,  2.73it/s]Evaluating:  23%|██▎       | 244/1040 [01:30<04:51,  2.73it/s]Evaluating:  24%|██▎       | 245/1040 [01:30<04:51,  2.73it/s]Evaluating:  24%|██▎       | 246/1040 [01:30<04:51,  2.72it/s]Evaluating:  24%|██▍       | 247/1040 [01:31<04:51,  2.72it/s]Evaluating:  24%|██▍       | 248/1040 [01:31<04:50,  2.73it/s]Evaluating:  24%|██▍       | 249/1040 [01:31<04:49,  2.73it/s]Evaluating:  24%|██▍       | 250/1040 [01:32<04:49,  2.73it/s]Evaluating:  24%|██▍       | 251/1040 [01:32<04:49,  2.73it/s]Evaluating:  24%|██▍       | 252/1040 [01:33<04:49,  2.72it/s]Evaluating:  24%|██▍       | 253/1040 [01:33<04:49,  2.72it/s]Evaluating:  24%|██▍       | 254/1040 [01:33<04:48,  2.72it/s]Evaluating:  25%|██▍       | 255/1040 [01:34<04:49,  2.72it/s]Evaluating:  25%|██▍       | 256/1040 [01:34<04:47,  2.73it/s]Evaluating:  25%|██▍       | 257/1040 [01:34<04:47,  2.73it/s]Evaluating:  25%|██▍       | 258/1040 [01:35<04:46,  2.72it/s]Evaluating:  25%|██▍       | 259/1040 [01:35<04:45,  2.73it/s]Evaluating:  25%|██▌       | 260/1040 [01:35<04:44,  2.74it/s]Evaluating:  25%|██▌       | 261/1040 [01:36<04:44,  2.74it/s]Evaluating:  25%|██▌       | 262/1040 [01:36<04:45,  2.72it/s]Evaluating:  25%|██▌       | 263/1040 [01:37<04:46,  2.71it/s]Evaluating:  25%|██▌       | 264/1040 [01:37<04:46,  2.71it/s]Evaluating:  25%|██▌       | 265/1040 [01:37<04:46,  2.71it/s]Evaluating:  26%|██▌       | 266/1040 [01:38<04:46,  2.70it/s]Evaluating:  26%|██▌       | 267/1040 [01:38<04:45,  2.71it/s]Evaluating:  26%|██▌       | 268/1040 [01:38<04:44,  2.71it/s]Evaluating:  26%|██▌       | 269/1040 [01:39<04:43,  2.72it/s]Evaluating:  26%|██▌       | 270/1040 [01:39<04:44,  2.70it/s]Evaluating:  26%|██▌       | 271/1040 [01:40<04:44,  2.71it/s]Evaluating:  26%|██▌       | 272/1040 [01:40<04:42,  2.72it/s]Evaluating:  26%|██▋       | 273/1040 [01:40<04:42,  2.72it/s]Evaluating:  26%|██▋       | 274/1040 [01:41<04:41,  2.72it/s]Evaluating:  26%|██▋       | 275/1040 [01:41<04:41,  2.72it/s]Evaluating:  27%|██▋       | 276/1040 [01:41<04:41,  2.71it/s]Evaluating:  27%|██▋       | 277/1040 [01:42<04:41,  2.71it/s]Evaluating:  27%|██▋       | 278/1040 [01:42<04:41,  2.71it/s]Evaluating:  27%|██▋       | 279/1040 [01:42<04:40,  2.71it/s]Evaluating:  27%|██▋       | 280/1040 [01:43<04:39,  2.72it/s]Evaluating:  27%|██▋       | 281/1040 [01:43<04:38,  2.72it/s]Evaluating:  27%|██▋       | 282/1040 [01:44<04:38,  2.73it/s]Evaluating:  27%|██▋       | 283/1040 [01:44<04:38,  2.72it/s]Evaluating:  27%|██▋       | 284/1040 [01:44<04:38,  2.71it/s]Evaluating:  27%|██▋       | 285/1040 [01:45<04:38,  2.72it/s]Evaluating:  28%|██▊       | 286/1040 [01:45<04:36,  2.72it/s]Evaluating:  28%|██▊       | 287/1040 [01:45<04:35,  2.73it/s]Evaluating:  28%|██▊       | 288/1040 [01:46<04:35,  2.73it/s]Evaluating:  28%|██▊       | 289/1040 [01:46<04:36,  2.72it/s]Evaluating:  28%|██▊       | 290/1040 [01:47<04:35,  2.72it/s]Evaluating:  28%|██▊       | 291/1040 [01:47<04:35,  2.72it/s]Evaluating:  28%|██▊       | 292/1040 [01:47<04:35,  2.71it/s]Evaluating:  28%|██▊       | 293/1040 [01:48<04:36,  2.70it/s]Evaluating:  28%|██▊       | 294/1040 [01:48<04:36,  2.70it/s]Evaluating:  28%|██▊       | 295/1040 [01:48<04:36,  2.69it/s]Evaluating:  28%|██▊       | 296/1040 [01:49<04:35,  2.70it/s]Evaluating:  29%|██▊       | 297/1040 [01:49<04:34,  2.70it/s]Evaluating:  29%|██▊       | 298/1040 [01:49<04:34,  2.70it/s]Evaluating:  29%|██▉       | 299/1040 [01:50<04:34,  2.70it/s]Evaluating:  29%|██▉       | 300/1040 [01:50<04:32,  2.71it/s]Evaluating:  29%|██▉       | 301/1040 [01:51<04:32,  2.72it/s]Evaluating:  29%|██▉       | 302/1040 [01:51<04:31,  2.72it/s]Evaluating:  29%|██▉       | 303/1040 [01:51<04:30,  2.72it/s]Evaluating:  29%|██▉       | 304/1040 [01:52<04:30,  2.72it/s]Evaluating:  29%|██▉       | 305/1040 [01:52<04:29,  2.73it/s]Evaluating:  29%|██▉       | 306/1040 [01:52<04:29,  2.72it/s]Evaluating:  30%|██▉       | 307/1040 [01:53<04:29,  2.72it/s]Evaluating:  30%|██▉       | 308/1040 [01:53<04:28,  2.73it/s]Evaluating:  30%|██▉       | 309/1040 [01:54<04:27,  2.73it/s]Evaluating:  30%|██▉       | 310/1040 [01:54<04:27,  2.72it/s]Evaluating:  30%|██▉       | 311/1040 [01:54<04:27,  2.72it/s]Evaluating:  30%|███       | 312/1040 [01:55<04:26,  2.73it/s]Evaluating:  30%|███       | 313/1040 [01:55<04:27,  2.72it/s]Evaluating:  30%|███       | 314/1040 [01:55<04:26,  2.72it/s]Evaluating:  30%|███       | 315/1040 [01:56<04:26,  2.72it/s]Evaluating:  30%|███       | 316/1040 [01:56<04:27,  2.71it/s]Evaluating:  30%|███       | 317/1040 [01:56<04:27,  2.70it/s]Evaluating:  31%|███       | 318/1040 [01:57<04:27,  2.70it/s]Evaluating:  31%|███       | 319/1040 [01:57<04:25,  2.72it/s]Evaluating:  31%|███       | 320/1040 [01:58<04:24,  2.72it/s]Evaluating:  31%|███       | 321/1040 [01:58<04:24,  2.72it/s]Evaluating:  31%|███       | 322/1040 [01:58<04:24,  2.72it/s]Evaluating:  31%|███       | 323/1040 [01:59<04:25,  2.70it/s]Evaluating:  31%|███       | 324/1040 [01:59<04:23,  2.72it/s]Evaluating:  31%|███▏      | 325/1040 [01:59<04:22,  2.72it/s]Evaluating:  31%|███▏      | 326/1040 [02:00<04:22,  2.72it/s]Evaluating:  31%|███▏      | 327/1040 [02:00<04:22,  2.72it/s]Evaluating:  32%|███▏      | 328/1040 [02:01<04:22,  2.72it/s]Evaluating:  32%|███▏      | 329/1040 [02:01<04:21,  2.72it/s]Evaluating:  32%|███▏      | 330/1040 [02:01<04:20,  2.72it/s]Evaluating:  32%|███▏      | 331/1040 [02:02<04:19,  2.73it/s]Evaluating:  32%|███▏      | 332/1040 [02:02<04:19,  2.73it/s]Evaluating:  32%|███▏      | 333/1040 [02:02<04:18,  2.73it/s]Evaluating:  32%|███▏      | 334/1040 [02:03<04:19,  2.72it/s]Evaluating:  32%|███▏      | 335/1040 [02:03<04:18,  2.72it/s]Evaluating:  32%|███▏      | 336/1040 [02:03<04:19,  2.72it/s]Evaluating:  32%|███▏      | 337/1040 [02:04<04:19,  2.71it/s]Evaluating:  32%|███▎      | 338/1040 [02:04<04:18,  2.72it/s]Evaluating:  33%|███▎      | 339/1040 [02:05<04:18,  2.71it/s]Evaluating:  33%|███▎      | 340/1040 [02:05<04:17,  2.71it/s]Evaluating:  33%|███▎      | 341/1040 [02:05<04:16,  2.73it/s]Evaluating:  33%|███▎      | 342/1040 [02:06<04:15,  2.73it/s]Evaluating:  33%|███▎      | 343/1040 [02:06<04:16,  2.71it/s]Evaluating:  33%|███▎      | 344/1040 [02:06<04:16,  2.71it/s]Evaluating:  33%|███▎      | 345/1040 [02:07<04:17,  2.70it/s]Evaluating:  33%|███▎      | 346/1040 [02:07<04:16,  2.70it/s]Evaluating:  33%|███▎      | 347/1040 [02:08<04:16,  2.70it/s]Evaluating:  33%|███▎      | 348/1040 [02:08<04:15,  2.71it/s]Evaluating:  34%|███▎      | 349/1040 [02:08<04:15,  2.70it/s]Evaluating:  34%|███▎      | 350/1040 [02:09<04:14,  2.71it/s]Evaluating:  34%|███▍      | 351/1040 [02:09<04:13,  2.72it/s]Evaluating:  34%|███▍      | 352/1040 [02:09<04:13,  2.71it/s]Evaluating:  34%|███▍      | 353/1040 [02:10<04:12,  2.73it/s]Evaluating:  34%|███▍      | 354/1040 [02:10<04:11,  2.73it/s]Evaluating:  34%|███▍      | 355/1040 [02:10<04:12,  2.71it/s]Evaluating:  34%|███▍      | 356/1040 [02:11<04:11,  2.72it/s]Evaluating:  34%|███▍      | 357/1040 [02:11<04:10,  2.72it/s]Evaluating:  34%|███▍      | 358/1040 [02:12<04:10,  2.72it/s]Evaluating:  35%|███▍      | 359/1040 [02:12<04:10,  2.72it/s]Evaluating:  35%|███▍      | 360/1040 [02:12<04:09,  2.72it/s]Evaluating:  35%|███▍      | 361/1040 [02:13<04:09,  2.72it/s]Evaluating:  35%|███▍      | 362/1040 [02:13<04:09,  2.72it/s]Evaluating:  35%|███▍      | 363/1040 [02:13<04:08,  2.72it/s]Evaluating:  35%|███▌      | 364/1040 [02:14<04:09,  2.71it/s]Evaluating:  35%|███▌      | 365/1040 [02:14<04:08,  2.72it/s]Evaluating:  35%|███▌      | 366/1040 [02:15<04:07,  2.72it/s]Evaluating:  35%|███▌      | 367/1040 [02:15<04:06,  2.73it/s]Evaluating:  35%|███▌      | 368/1040 [02:15<04:06,  2.72it/s]Evaluating:  35%|███▌      | 369/1040 [02:16<04:07,  2.71it/s]Evaluating:  36%|███▌      | 370/1040 [02:16<04:07,  2.71it/s]Evaluating:  36%|███▌      | 371/1040 [02:16<04:06,  2.72it/s]Evaluating:  36%|███▌      | 372/1040 [02:17<04:05,  2.72it/s]Evaluating:  36%|███▌      | 373/1040 [02:17<04:06,  2.71it/s]Evaluating:  36%|███▌      | 374/1040 [02:17<04:05,  2.72it/s]Evaluating:  36%|███▌      | 375/1040 [02:18<04:04,  2.72it/s]Evaluating:  36%|███▌      | 376/1040 [02:18<04:04,  2.72it/s]Evaluating:  36%|███▋      | 377/1040 [02:19<04:03,  2.72it/s]Evaluating:  36%|███▋      | 378/1040 [02:19<04:02,  2.73it/s]Evaluating:  36%|███▋      | 379/1040 [02:19<04:03,  2.72it/s]Evaluating:  37%|███▋      | 380/1040 [02:20<04:02,  2.72it/s]Evaluating:  37%|███▋      | 381/1040 [02:20<04:02,  2.71it/s]Evaluating:  37%|███▋      | 382/1040 [02:20<04:02,  2.72it/s]Evaluating:  37%|███▋      | 383/1040 [02:21<04:02,  2.71it/s]Evaluating:  37%|███▋      | 384/1040 [02:21<04:00,  2.72it/s]Evaluating:  37%|███▋      | 385/1040 [02:21<04:00,  2.72it/s]Evaluating:  37%|███▋      | 386/1040 [02:22<04:00,  2.72it/s]Evaluating:  37%|███▋      | 387/1040 [02:22<04:00,  2.72it/s]Evaluating:  37%|███▋      | 388/1040 [02:23<04:00,  2.72it/s]Evaluating:  37%|███▋      | 389/1040 [02:23<03:59,  2.72it/s]Evaluating:  38%|███▊      | 390/1040 [02:23<03:59,  2.72it/s]Evaluating:  38%|███▊      | 391/1040 [02:24<03:58,  2.72it/s]Evaluating:  38%|███▊      | 392/1040 [02:24<03:58,  2.71it/s]Evaluating:  38%|███▊      | 393/1040 [02:24<03:58,  2.72it/s]Evaluating:  38%|███▊      | 394/1040 [02:25<03:58,  2.71it/s]Evaluating:  38%|███▊      | 395/1040 [02:25<03:58,  2.70it/s]Evaluating:  38%|███▊      | 396/1040 [02:26<03:58,  2.70it/s]Evaluating:  38%|███▊      | 397/1040 [02:26<03:57,  2.71it/s]Evaluating:  38%|███▊      | 398/1040 [02:26<03:57,  2.71it/s]Evaluating:  38%|███▊      | 399/1040 [02:27<03:57,  2.70it/s]Evaluating:  38%|███▊      | 400/1040 [02:27<03:56,  2.71it/s]Evaluating:  39%|███▊      | 401/1040 [02:27<03:55,  2.71it/s]Evaluating:  39%|███▊      | 402/1040 [02:28<03:54,  2.72it/s]Evaluating:  39%|███▉      | 403/1040 [02:28<03:54,  2.72it/s]Evaluating:  39%|███▉      | 404/1040 [02:28<03:54,  2.71it/s]Evaluating:  39%|███▉      | 405/1040 [02:29<03:53,  2.72it/s]Evaluating:  39%|███▉      | 406/1040 [02:29<03:53,  2.72it/s]Evaluating:  39%|███▉      | 407/1040 [02:30<03:53,  2.72it/s]Evaluating:  39%|███▉      | 408/1040 [02:30<03:52,  2.72it/s]Evaluating:  39%|███▉      | 409/1040 [02:30<03:52,  2.71it/s]Evaluating:  39%|███▉      | 410/1040 [02:31<03:52,  2.71it/s]Evaluating:  40%|███▉      | 411/1040 [02:31<03:51,  2.71it/s]Evaluating:  40%|███▉      | 412/1040 [02:31<03:52,  2.70it/s]Evaluating:  40%|███▉      | 413/1040 [02:32<03:52,  2.69it/s]Evaluating:  40%|███▉      | 414/1040 [02:32<03:52,  2.70it/s]Evaluating:  40%|███▉      | 415/1040 [02:33<03:51,  2.70it/s]Evaluating:  40%|████      | 416/1040 [02:33<03:51,  2.70it/s]Evaluating:  40%|████      | 417/1040 [02:33<03:59,  2.61it/s]Evaluating:  40%|████      | 418/1040 [02:34<03:56,  2.63it/s]Evaluating:  40%|████      | 419/1040 [02:34<03:54,  2.65it/s]Evaluating:  40%|████      | 420/1040 [02:34<03:51,  2.67it/s]Evaluating:  40%|████      | 421/1040 [02:35<03:50,  2.69it/s]Evaluating:  41%|████      | 422/1040 [02:35<03:49,  2.69it/s]Evaluating:  41%|████      | 423/1040 [02:36<03:48,  2.70it/s]Evaluating:  41%|████      | 424/1040 [02:36<03:48,  2.70it/s]Evaluating:  41%|████      | 425/1040 [02:36<03:46,  2.71it/s]Evaluating:  41%|████      | 426/1040 [02:37<03:46,  2.71it/s]Evaluating:  41%|████      | 427/1040 [02:37<03:45,  2.72it/s]Evaluating:  41%|████      | 428/1040 [02:37<03:45,  2.71it/s]Evaluating:  41%|████▏     | 429/1040 [02:38<03:45,  2.71it/s]Evaluating:  41%|████▏     | 430/1040 [02:38<03:44,  2.72it/s]Evaluating:  41%|████▏     | 431/1040 [02:39<03:43,  2.72it/s]Evaluating:  42%|████▏     | 432/1040 [02:39<03:43,  2.73it/s]Evaluating:  42%|████▏     | 433/1040 [02:39<03:42,  2.73it/s]Evaluating:  42%|████▏     | 434/1040 [02:40<03:41,  2.73it/s]Evaluating:  42%|████▏     | 435/1040 [02:40<03:40,  2.74it/s]Evaluating:  42%|████▏     | 436/1040 [02:40<03:40,  2.74it/s]Evaluating:  42%|████▏     | 437/1040 [02:41<03:39,  2.75it/s]Evaluating:  42%|████▏     | 438/1040 [02:41<03:39,  2.74it/s]Evaluating:  42%|████▏     | 439/1040 [02:41<03:40,  2.73it/s]Evaluating:  42%|████▏     | 440/1040 [02:42<03:40,  2.72it/s]Evaluating:  42%|████▏     | 441/1040 [02:42<03:39,  2.73it/s]Evaluating:  42%|████▎     | 442/1040 [02:43<03:39,  2.73it/s]Evaluating:  43%|████▎     | 443/1040 [02:43<03:39,  2.72it/s]Evaluating:  43%|████▎     | 444/1040 [02:43<03:39,  2.71it/s]Evaluating:  43%|████▎     | 445/1040 [02:44<03:38,  2.73it/s]Evaluating:  43%|████▎     | 446/1040 [02:44<03:37,  2.73it/s]Evaluating:  43%|████▎     | 447/1040 [02:44<03:37,  2.72it/s]Evaluating:  43%|████▎     | 448/1040 [02:45<03:37,  2.72it/s]Evaluating:  43%|████▎     | 449/1040 [02:45<03:37,  2.72it/s]Evaluating:  43%|████▎     | 450/1040 [02:45<03:37,  2.72it/s]Evaluating:  43%|████▎     | 451/1040 [02:46<03:36,  2.72it/s]Evaluating:  43%|████▎     | 452/1040 [02:46<03:35,  2.73it/s]Evaluating:  44%|████▎     | 453/1040 [02:47<03:35,  2.72it/s]Evaluating:  44%|████▎     | 454/1040 [02:47<03:35,  2.72it/s]Evaluating:  44%|████▍     | 455/1040 [02:47<03:35,  2.72it/s]Evaluating:  44%|████▍     | 456/1040 [02:48<03:34,  2.72it/s]Evaluating:  44%|████▍     | 457/1040 [02:48<03:34,  2.72it/s]Evaluating:  44%|████▍     | 458/1040 [02:48<03:33,  2.72it/s]Evaluating:  44%|████▍     | 459/1040 [02:49<03:33,  2.72it/s]Evaluating:  44%|████▍     | 460/1040 [02:49<03:33,  2.72it/s]Evaluating:  44%|████▍     | 461/1040 [02:50<03:32,  2.73it/s]Evaluating:  44%|████▍     | 462/1040 [02:50<03:31,  2.73it/s]Evaluating:  45%|████▍     | 463/1040 [02:50<03:31,  2.72it/s]Evaluating:  45%|████▍     | 464/1040 [02:51<03:30,  2.73it/s]Evaluating:  45%|████▍     | 465/1040 [02:51<03:29,  2.74it/s]Evaluating:  45%|████▍     | 466/1040 [02:51<03:30,  2.73it/s]Evaluating:  45%|████▍     | 467/1040 [02:52<03:30,  2.73it/s]Evaluating:  45%|████▌     | 468/1040 [02:52<03:30,  2.72it/s]Evaluating:  45%|████▌     | 469/1040 [02:52<03:30,  2.71it/s]Evaluating:  45%|████▌     | 470/1040 [02:53<03:30,  2.70it/s]Evaluating:  45%|████▌     | 471/1040 [02:53<03:30,  2.70it/s]Evaluating:  45%|████▌     | 472/1040 [02:54<03:29,  2.70it/s]Evaluating:  45%|████▌     | 473/1040 [02:54<03:28,  2.72it/s]Evaluating:  46%|████▌     | 474/1040 [02:54<03:27,  2.72it/s]Evaluating:  46%|████▌     | 475/1040 [02:55<03:28,  2.71it/s]Evaluating:  46%|████▌     | 476/1040 [02:55<03:28,  2.71it/s]Evaluating:  46%|████▌     | 477/1040 [02:55<03:26,  2.72it/s]Evaluating:  46%|████▌     | 478/1040 [02:56<03:26,  2.72it/s]Evaluating:  46%|████▌     | 479/1040 [02:56<03:26,  2.72it/s]Evaluating:  46%|████▌     | 480/1040 [02:57<03:26,  2.71it/s]Evaluating:  46%|████▋     | 481/1040 [02:57<03:26,  2.71it/s]Evaluating:  46%|████▋     | 482/1040 [02:57<03:26,  2.71it/s]Evaluating:  46%|████▋     | 483/1040 [02:58<03:25,  2.71it/s]Evaluating:  47%|████▋     | 484/1040 [02:58<03:24,  2.71it/s]Evaluating:  47%|████▋     | 485/1040 [02:58<03:23,  2.72it/s]Evaluating:  47%|████▋     | 486/1040 [02:59<03:24,  2.71it/s]Evaluating:  47%|████▋     | 487/1040 [02:59<03:23,  2.71it/s]Evaluating:  47%|████▋     | 488/1040 [02:59<03:23,  2.71it/s]Evaluating:  47%|████▋     | 489/1040 [03:00<03:23,  2.71it/s]Evaluating:  47%|████▋     | 490/1040 [03:00<03:22,  2.72it/s]Evaluating:  47%|████▋     | 491/1040 [03:01<03:21,  2.72it/s]Evaluating:  47%|████▋     | 492/1040 [03:01<03:22,  2.71it/s]Evaluating:  47%|████▋     | 493/1040 [03:01<03:21,  2.71it/s]Evaluating:  48%|████▊     | 494/1040 [03:02<03:22,  2.70it/s]Evaluating:  48%|████▊     | 495/1040 [03:02<03:21,  2.70it/s]Evaluating:  48%|████▊     | 496/1040 [03:02<03:21,  2.70it/s]Evaluating:  48%|████▊     | 497/1040 [03:03<03:21,  2.70it/s]Evaluating:  48%|████▊     | 498/1040 [03:03<03:20,  2.70it/s]Evaluating:  48%|████▊     | 499/1040 [03:04<03:20,  2.70it/s]Evaluating:  48%|████▊     | 500/1040 [03:04<03:20,  2.70it/s]Evaluating:  48%|████▊     | 501/1040 [03:04<03:19,  2.71it/s]Evaluating:  48%|████▊     | 502/1040 [03:05<03:18,  2.71it/s]Evaluating:  48%|████▊     | 503/1040 [03:05<03:18,  2.71it/s]Evaluating:  48%|████▊     | 504/1040 [03:05<03:17,  2.71it/s]Evaluating:  49%|████▊     | 505/1040 [03:06<03:17,  2.71it/s]Evaluating:  49%|████▊     | 506/1040 [03:06<03:16,  2.72it/s]Evaluating:  49%|████▉     | 507/1040 [03:06<03:15,  2.72it/s]Evaluating:  49%|████▉     | 508/1040 [03:07<03:15,  2.72it/s]Evaluating:  49%|████▉     | 509/1040 [03:07<03:15,  2.72it/s]Evaluating:  49%|████▉     | 510/1040 [03:08<03:15,  2.72it/s]Evaluating:  49%|████▉     | 511/1040 [03:08<03:14,  2.72it/s]Evaluating:  49%|████▉     | 512/1040 [03:08<03:14,  2.72it/s]Evaluating:  49%|████▉     | 513/1040 [03:09<03:14,  2.71it/s]Evaluating:  49%|████▉     | 514/1040 [03:09<03:13,  2.72it/s]Evaluating:  50%|████▉     | 515/1040 [03:09<03:13,  2.72it/s]Evaluating:  50%|████▉     | 516/1040 [03:10<03:13,  2.71it/s]Evaluating:  50%|████▉     | 517/1040 [03:10<03:12,  2.71it/s]Evaluating:  50%|████▉     | 518/1040 [03:11<03:12,  2.71it/s]Evaluating:  50%|████▉     | 519/1040 [03:11<03:12,  2.70it/s]Evaluating:  50%|█████     | 520/1040 [03:11<03:12,  2.70it/s]Evaluating:  50%|█████     | 521/1040 [03:12<03:12,  2.70it/s]Evaluating:  50%|█████     | 522/1040 [03:12<03:11,  2.70it/s]Evaluating:  50%|█████     | 523/1040 [03:12<03:10,  2.71it/s]Evaluating:  50%|█████     | 524/1040 [03:13<03:10,  2.71it/s]Evaluating:  50%|█████     | 525/1040 [03:13<03:10,  2.71it/s]Evaluating:  51%|█████     | 526/1040 [03:13<03:10,  2.70it/s]Evaluating:  51%|█████     | 527/1040 [03:14<03:10,  2.70it/s]Evaluating:  51%|█████     | 528/1040 [03:14<03:09,  2.71it/s]Evaluating:  51%|█████     | 529/1040 [03:15<03:07,  2.72it/s]Evaluating:  51%|█████     | 530/1040 [03:15<03:07,  2.72it/s]Evaluating:  51%|█████     | 531/1040 [03:15<03:07,  2.72it/s]Evaluating:  51%|█████     | 532/1040 [03:16<03:07,  2.71it/s]Evaluating:  51%|█████▏    | 533/1040 [03:16<03:07,  2.70it/s]Evaluating:  51%|█████▏    | 534/1040 [03:16<03:06,  2.71it/s]Evaluating:  51%|█████▏    | 535/1040 [03:17<03:06,  2.71it/s]Evaluating:  52%|█████▏    | 536/1040 [03:17<03:06,  2.70it/s]Evaluating:  52%|█████▏    | 537/1040 [03:18<03:05,  2.71it/s]Evaluating:  52%|█████▏    | 538/1040 [03:18<03:05,  2.70it/s]Evaluating:  52%|█████▏    | 539/1040 [03:18<03:05,  2.70it/s]Evaluating:  52%|█████▏    | 540/1040 [03:19<03:05,  2.70it/s]Evaluating:  52%|█████▏    | 541/1040 [03:19<03:04,  2.71it/s]Evaluating:  52%|█████▏    | 542/1040 [03:19<03:04,  2.70it/s]Evaluating:  52%|█████▏    | 543/1040 [03:20<03:04,  2.70it/s]Evaluating:  52%|█████▏    | 544/1040 [03:20<03:03,  2.70it/s]Evaluating:  52%|█████▏    | 545/1040 [03:21<03:03,  2.70it/s]Evaluating:  52%|█████▎    | 546/1040 [03:21<03:02,  2.71it/s]Evaluating:  53%|█████▎    | 547/1040 [03:21<03:01,  2.71it/s]Evaluating:  53%|█████▎    | 548/1040 [03:22<03:01,  2.71it/s]Evaluating:  53%|█████▎    | 549/1040 [03:22<03:02,  2.70it/s]Evaluating:  53%|█████▎    | 550/1040 [03:22<03:01,  2.70it/s]Evaluating:  53%|█████▎    | 551/1040 [03:23<03:00,  2.70it/s]Evaluating:  53%|█████▎    | 552/1040 [03:23<02:59,  2.72it/s]Evaluating:  53%|█████▎    | 553/1040 [03:23<02:59,  2.72it/s]Evaluating:  53%|█████▎    | 554/1040 [03:24<02:58,  2.72it/s]Evaluating:  53%|█████▎    | 555/1040 [03:24<02:57,  2.73it/s]Evaluating:  53%|█████▎    | 556/1040 [03:25<02:57,  2.73it/s]Evaluating:  54%|█████▎    | 557/1040 [03:25<02:57,  2.72it/s]Evaluating:  54%|█████▎    | 558/1040 [03:25<02:57,  2.72it/s]Evaluating:  54%|█████▍    | 559/1040 [03:26<02:56,  2.72it/s]Evaluating:  54%|█████▍    | 560/1040 [03:26<02:56,  2.71it/s]Evaluating:  54%|█████▍    | 561/1040 [03:26<02:58,  2.69it/s]Evaluating:  54%|█████▍    | 562/1040 [03:27<02:57,  2.69it/s]Evaluating:  54%|█████▍    | 563/1040 [03:27<02:57,  2.68it/s]Evaluating:  54%|█████▍    | 564/1040 [03:28<02:56,  2.70it/s]Evaluating:  54%|█████▍    | 565/1040 [03:28<02:55,  2.71it/s]Evaluating:  54%|█████▍    | 566/1040 [03:28<02:54,  2.71it/s]Evaluating:  55%|█████▍    | 567/1040 [03:29<02:54,  2.71it/s]Evaluating:  55%|█████▍    | 568/1040 [03:29<02:54,  2.71it/s]Evaluating:  55%|█████▍    | 569/1040 [03:29<02:53,  2.71it/s]Evaluating:  55%|█████▍    | 570/1040 [03:30<02:53,  2.71it/s]Evaluating:  55%|█████▍    | 571/1040 [03:30<02:52,  2.72it/s]Evaluating:  55%|█████▌    | 572/1040 [03:30<02:52,  2.72it/s]Evaluating:  55%|█████▌    | 573/1040 [03:31<02:52,  2.71it/s]Evaluating:  55%|█████▌    | 574/1040 [03:31<02:51,  2.72it/s]Evaluating:  55%|█████▌    | 575/1040 [03:32<02:51,  2.71it/s]Evaluating:  55%|█████▌    | 576/1040 [03:32<02:50,  2.72it/s]Evaluating:  55%|█████▌    | 577/1040 [03:32<02:49,  2.72it/s]Evaluating:  56%|█████▌    | 578/1040 [03:33<02:49,  2.73it/s]Evaluating:  56%|█████▌    | 579/1040 [03:33<02:49,  2.73it/s]Evaluating:  56%|█████▌    | 580/1040 [03:33<02:49,  2.71it/s]Evaluating:  56%|█████▌    | 581/1040 [03:34<02:49,  2.71it/s]Evaluating:  56%|█████▌    | 582/1040 [03:34<02:48,  2.71it/s]Evaluating:  56%|█████▌    | 583/1040 [03:35<02:47,  2.72it/s]Evaluating:  56%|█████▌    | 584/1040 [03:35<02:47,  2.72it/s]Evaluating:  56%|█████▋    | 585/1040 [03:35<02:47,  2.72it/s]Evaluating:  56%|█████▋    | 586/1040 [03:36<02:47,  2.71it/s]Evaluating:  56%|█████▋    | 587/1040 [03:36<02:47,  2.71it/s]Evaluating:  57%|█████▋    | 588/1040 [03:36<02:46,  2.71it/s]Evaluating:  57%|█████▋    | 589/1040 [03:37<02:46,  2.71it/s]Evaluating:  57%|█████▋    | 590/1040 [03:37<02:46,  2.71it/s]Evaluating:  57%|█████▋    | 591/1040 [03:37<02:46,  2.70it/s]Evaluating:  57%|█████▋    | 592/1040 [03:38<02:45,  2.70it/s]Evaluating:  57%|█████▋    | 593/1040 [03:38<02:45,  2.69it/s]Evaluating:  57%|█████▋    | 594/1040 [03:39<02:45,  2.69it/s]Evaluating:  57%|█████▋    | 595/1040 [03:39<02:45,  2.69it/s]Evaluating:  57%|█████▋    | 596/1040 [03:39<02:44,  2.70it/s]Evaluating:  57%|█████▋    | 597/1040 [03:40<02:43,  2.71it/s]Evaluating:  57%|█████▊    | 598/1040 [03:40<02:43,  2.70it/s]Evaluating:  58%|█████▊    | 599/1040 [03:40<02:43,  2.69it/s]Evaluating:  58%|█████▊    | 600/1040 [03:41<02:43,  2.69it/s]Evaluating:  58%|█████▊    | 601/1040 [03:41<02:43,  2.68it/s]Evaluating:  58%|█████▊    | 602/1040 [03:42<02:42,  2.70it/s]Evaluating:  58%|█████▊    | 603/1040 [03:42<02:41,  2.70it/s]Evaluating:  58%|█████▊    | 604/1040 [03:42<02:41,  2.70it/s]Evaluating:  58%|█████▊    | 605/1040 [03:43<02:41,  2.70it/s]Evaluating:  58%|█████▊    | 606/1040 [03:43<02:40,  2.70it/s]Evaluating:  58%|█████▊    | 607/1040 [03:43<02:40,  2.70it/s]Evaluating:  58%|█████▊    | 608/1040 [03:44<02:39,  2.70it/s]Evaluating:  59%|█████▊    | 609/1040 [03:44<02:40,  2.69it/s]Evaluating:  59%|█████▊    | 610/1040 [03:45<02:39,  2.70it/s]Evaluating:  59%|█████▉    | 611/1040 [03:45<02:38,  2.71it/s]Evaluating:  59%|█████▉    | 612/1040 [03:45<02:37,  2.72it/s]Evaluating:  59%|█████▉    | 613/1040 [03:46<02:36,  2.72it/s]Evaluating:  59%|█████▉    | 614/1040 [03:46<02:37,  2.70it/s]Evaluating:  59%|█████▉    | 615/1040 [03:46<02:37,  2.69it/s]Evaluating:  59%|█████▉    | 616/1040 [03:47<02:37,  2.69it/s]Evaluating:  59%|█████▉    | 617/1040 [03:47<02:36,  2.70it/s]Evaluating:  59%|█████▉    | 618/1040 [03:47<02:36,  2.69it/s]Evaluating:  60%|█████▉    | 619/1040 [03:48<02:36,  2.69it/s]Evaluating:  60%|█████▉    | 620/1040 [03:48<02:35,  2.70it/s]Evaluating:  60%|█████▉    | 621/1040 [03:49<02:34,  2.71it/s]Evaluating:  60%|█████▉    | 622/1040 [03:49<02:34,  2.71it/s]Evaluating:  60%|█████▉    | 623/1040 [03:49<02:33,  2.71it/s]Evaluating:  60%|██████    | 624/1040 [03:50<02:33,  2.71it/s]Evaluating:  60%|██████    | 625/1040 [03:50<02:33,  2.70it/s]Evaluating:  60%|██████    | 626/1040 [03:50<02:32,  2.71it/s]Evaluating:  60%|██████    | 627/1040 [03:51<02:32,  2.71it/s]Evaluating:  60%|██████    | 628/1040 [03:51<02:32,  2.71it/s]Evaluating:  60%|██████    | 629/1040 [03:52<02:31,  2.71it/s]Evaluating:  61%|██████    | 630/1040 [03:52<02:31,  2.71it/s]Evaluating:  61%|██████    | 631/1040 [03:52<02:31,  2.71it/s]Evaluating:  61%|██████    | 632/1040 [03:53<02:30,  2.71it/s]Evaluating:  61%|██████    | 633/1040 [03:53<02:32,  2.67it/s]Evaluating:  61%|██████    | 634/1040 [03:53<02:31,  2.68it/s]Evaluating:  61%|██████    | 635/1040 [03:54<02:30,  2.68it/s]Evaluating:  61%|██████    | 636/1040 [03:54<02:30,  2.69it/s]Evaluating:  61%|██████▏   | 637/1040 [03:55<02:29,  2.69it/s]Evaluating:  61%|██████▏   | 638/1040 [03:55<02:28,  2.70it/s]Evaluating:  61%|██████▏   | 639/1040 [03:55<02:28,  2.69it/s]Evaluating:  62%|██████▏   | 640/1040 [03:56<02:28,  2.70it/s]Evaluating:  62%|██████▏   | 641/1040 [03:56<02:27,  2.71it/s]Evaluating:  62%|██████▏   | 642/1040 [03:56<02:27,  2.71it/s]Evaluating:  62%|██████▏   | 643/1040 [03:57<02:26,  2.71it/s]Evaluating:  62%|██████▏   | 644/1040 [03:57<02:25,  2.72it/s]Evaluating:  62%|██████▏   | 645/1040 [03:57<02:25,  2.71it/s]Evaluating:  62%|██████▏   | 646/1040 [03:58<02:24,  2.72it/s]Evaluating:  62%|██████▏   | 647/1040 [03:58<02:24,  2.71it/s]Evaluating:  62%|██████▏   | 648/1040 [03:59<02:24,  2.71it/s]Evaluating:  62%|██████▏   | 649/1040 [03:59<02:24,  2.71it/s]Evaluating:  62%|██████▎   | 650/1040 [03:59<02:24,  2.70it/s]Evaluating:  63%|██████▎   | 651/1040 [04:00<03:53,  1.66it/s]Evaluating:  63%|██████▎   | 652/1040 [04:01<03:26,  1.88it/s]Evaluating:  63%|██████▎   | 653/1040 [04:01<03:06,  2.07it/s]Evaluating:  63%|██████▎   | 654/1040 [04:02<02:53,  2.23it/s]Evaluating:  63%|██████▎   | 655/1040 [04:02<02:43,  2.35it/s]Evaluating:  63%|██████▎   | 656/1040 [04:02<02:37,  2.45it/s]Evaluating:  63%|██████▎   | 657/1040 [04:03<02:32,  2.52it/s]Evaluating:  63%|██████▎   | 658/1040 [04:03<02:28,  2.57it/s]Evaluating:  63%|██████▎   | 659/1040 [04:03<02:26,  2.61it/s]Evaluating:  63%|██████▎   | 660/1040 [04:04<02:23,  2.64it/s]Evaluating:  64%|██████▎   | 661/1040 [04:04<02:22,  2.66it/s]Evaluating:  64%|██████▎   | 662/1040 [04:05<02:20,  2.68it/s]Evaluating:  64%|██████▍   | 663/1040 [04:05<02:20,  2.69it/s]Evaluating:  64%|██████▍   | 664/1040 [04:05<02:19,  2.69it/s]Evaluating:  64%|██████▍   | 665/1040 [04:06<02:19,  2.69it/s]Evaluating:  64%|██████▍   | 666/1040 [04:06<02:18,  2.70it/s]Evaluating:  64%|██████▍   | 667/1040 [04:06<02:18,  2.69it/s]Evaluating:  64%|██████▍   | 668/1040 [04:07<02:18,  2.69it/s]Evaluating:  64%|██████▍   | 669/1040 [04:07<02:17,  2.70it/s]Evaluating:  64%|██████▍   | 670/1040 [04:07<02:17,  2.70it/s]Evaluating:  65%|██████▍   | 671/1040 [04:08<02:16,  2.71it/s]Evaluating:  65%|██████▍   | 672/1040 [04:08<02:15,  2.71it/s]Evaluating:  65%|██████▍   | 673/1040 [04:09<02:15,  2.71it/s]Evaluating:  65%|██████▍   | 674/1040 [04:09<02:14,  2.72it/s]Evaluating:  65%|██████▍   | 675/1040 [04:09<02:14,  2.72it/s]Evaluating:  65%|██████▌   | 676/1040 [04:10<02:13,  2.73it/s]Evaluating:  65%|██████▌   | 677/1040 [04:10<02:13,  2.72it/s]Evaluating:  65%|██████▌   | 678/1040 [04:10<02:13,  2.71it/s]Evaluating:  65%|██████▌   | 679/1040 [04:11<02:13,  2.71it/s]Evaluating:  65%|██████▌   | 680/1040 [04:11<02:13,  2.70it/s]Evaluating:  65%|██████▌   | 681/1040 [04:12<02:13,  2.70it/s]Evaluating:  66%|██████▌   | 682/1040 [04:12<02:12,  2.70it/s]Evaluating:  66%|██████▌   | 683/1040 [04:12<02:12,  2.70it/s]Evaluating:  66%|██████▌   | 684/1040 [04:13<02:12,  2.69it/s]Evaluating:  66%|██████▌   | 685/1040 [04:13<02:12,  2.69it/s]Evaluating:  66%|██████▌   | 686/1040 [04:13<02:11,  2.69it/s]Evaluating:  66%|██████▌   | 687/1040 [04:14<02:10,  2.70it/s]Evaluating:  66%|██████▌   | 688/1040 [04:14<02:09,  2.71it/s]Evaluating:  66%|██████▋   | 689/1040 [04:14<02:09,  2.71it/s]Evaluating:  66%|██████▋   | 690/1040 [04:15<02:08,  2.72it/s]Evaluating:  66%|██████▋   | 691/1040 [04:15<02:08,  2.72it/s]Evaluating:  67%|██████▋   | 692/1040 [04:16<02:07,  2.73it/s]Evaluating:  67%|██████▋   | 693/1040 [04:16<02:06,  2.73it/s]Evaluating:  67%|██████▋   | 694/1040 [04:16<02:06,  2.73it/s]Evaluating:  67%|██████▋   | 695/1040 [04:17<02:06,  2.72it/s]Evaluating:  67%|██████▋   | 696/1040 [04:17<02:06,  2.72it/s]Evaluating:  67%|██████▋   | 697/1040 [04:17<02:06,  2.72it/s]Evaluating:  67%|██████▋   | 698/1040 [04:18<02:05,  2.73it/s]Evaluating:  67%|██████▋   | 699/1040 [04:18<02:05,  2.72it/s]Evaluating:  67%|██████▋   | 700/1040 [04:19<02:04,  2.73it/s]Evaluating:  67%|██████▋   | 701/1040 [04:19<02:04,  2.72it/s]Evaluating:  68%|██████▊   | 702/1040 [04:19<02:03,  2.73it/s]Evaluating:  68%|██████▊   | 703/1040 [04:20<02:03,  2.73it/s]Evaluating:  68%|██████▊   | 704/1040 [04:20<02:03,  2.72it/s]Evaluating:  68%|██████▊   | 705/1040 [04:20<02:03,  2.72it/s]Evaluating:  68%|██████▊   | 706/1040 [04:21<02:02,  2.72it/s]Evaluating:  68%|██████▊   | 707/1040 [04:21<02:02,  2.72it/s]Evaluating:  68%|██████▊   | 708/1040 [04:21<02:01,  2.72it/s]Evaluating:  68%|██████▊   | 709/1040 [04:22<02:01,  2.72it/s]Evaluating:  68%|██████▊   | 710/1040 [04:22<02:01,  2.71it/s]Evaluating:  68%|██████▊   | 711/1040 [04:23<02:01,  2.70it/s]Evaluating:  68%|██████▊   | 712/1040 [04:23<02:01,  2.70it/s]Evaluating:  69%|██████▊   | 713/1040 [04:23<02:01,  2.70it/s]Evaluating:  69%|██████▊   | 714/1040 [04:24<02:01,  2.69it/s]Evaluating:  69%|██████▉   | 715/1040 [04:24<02:00,  2.69it/s]Evaluating:  69%|██████▉   | 716/1040 [04:24<02:00,  2.69it/s]Evaluating:  69%|██████▉   | 717/1040 [04:25<02:00,  2.69it/s]Evaluating:  69%|██████▉   | 718/1040 [04:25<01:59,  2.70it/s]Evaluating:  69%|██████▉   | 719/1040 [04:26<01:58,  2.70it/s]Evaluating:  69%|██████▉   | 720/1040 [04:26<01:58,  2.70it/s]Evaluating:  69%|██████▉   | 721/1040 [04:26<01:58,  2.70it/s]Evaluating:  69%|██████▉   | 722/1040 [04:27<01:57,  2.70it/s]Evaluating:  70%|██████▉   | 723/1040 [04:27<01:57,  2.70it/s]Evaluating:  70%|██████▉   | 724/1040 [04:27<01:56,  2.70it/s]Evaluating:  70%|██████▉   | 725/1040 [04:28<01:57,  2.69it/s]Evaluating:  70%|██████▉   | 726/1040 [04:28<01:56,  2.69it/s]Evaluating:  70%|██████▉   | 727/1040 [04:29<01:56,  2.70it/s]Evaluating:  70%|███████   | 728/1040 [04:29<01:55,  2.69it/s]Evaluating:  70%|███████   | 729/1040 [04:29<01:55,  2.69it/s]Evaluating:  70%|███████   | 730/1040 [04:30<01:54,  2.70it/s]Evaluating:  70%|███████   | 731/1040 [04:30<01:54,  2.70it/s]Evaluating:  70%|███████   | 732/1040 [04:30<01:54,  2.70it/s]Evaluating:  70%|███████   | 733/1040 [04:31<01:53,  2.70it/s]Evaluating:  71%|███████   | 734/1040 [04:31<01:53,  2.70it/s]Evaluating:  71%|███████   | 735/1040 [04:31<01:52,  2.70it/s]Evaluating:  71%|███████   | 736/1040 [04:32<01:52,  2.70it/s]Evaluating:  71%|███████   | 737/1040 [04:32<01:51,  2.72it/s]Evaluating:  71%|███████   | 738/1040 [04:33<01:51,  2.72it/s]Evaluating:  71%|███████   | 739/1040 [04:33<01:50,  2.73it/s]Evaluating:  71%|███████   | 740/1040 [04:33<01:49,  2.73it/s]Evaluating:  71%|███████▏  | 741/1040 [04:34<01:49,  2.73it/s]Evaluating:  71%|███████▏  | 742/1040 [04:34<01:49,  2.72it/s]Evaluating:  71%|███████▏  | 743/1040 [04:34<01:49,  2.72it/s]Evaluating:  72%|███████▏  | 744/1040 [04:35<01:48,  2.72it/s]Evaluating:  72%|███████▏  | 745/1040 [04:35<01:48,  2.72it/s]Evaluating:  72%|███████▏  | 746/1040 [04:36<01:47,  2.72it/s]Evaluating:  72%|███████▏  | 747/1040 [04:36<01:47,  2.72it/s]Evaluating:  72%|███████▏  | 748/1040 [04:36<01:47,  2.71it/s]Evaluating:  72%|███████▏  | 749/1040 [04:37<01:47,  2.71it/s]Evaluating:  72%|███████▏  | 750/1040 [04:37<01:47,  2.71it/s]Evaluating:  72%|███████▏  | 751/1040 [04:37<01:46,  2.72it/s]Evaluating:  72%|███████▏  | 752/1040 [04:38<01:46,  2.71it/s]Evaluating:  72%|███████▏  | 753/1040 [04:38<01:45,  2.71it/s]Evaluating:  72%|███████▎  | 754/1040 [04:38<01:45,  2.71it/s]Evaluating:  73%|███████▎  | 755/1040 [04:39<01:45,  2.71it/s]Evaluating:  73%|███████▎  | 756/1040 [04:39<01:44,  2.71it/s]Evaluating:  73%|███████▎  | 757/1040 [04:40<01:44,  2.72it/s]Evaluating:  73%|███████▎  | 758/1040 [04:40<01:43,  2.73it/s]Evaluating:  73%|███████▎  | 759/1040 [04:40<01:43,  2.72it/s]Evaluating:  73%|███████▎  | 760/1040 [04:41<01:43,  2.72it/s]Evaluating:  73%|███████▎  | 761/1040 [04:41<01:43,  2.71it/s]Evaluating:  73%|███████▎  | 762/1040 [04:41<01:42,  2.71it/s]Evaluating:  73%|███████▎  | 763/1040 [04:42<01:42,  2.71it/s]Evaluating:  73%|███████▎  | 764/1040 [04:42<01:42,  2.70it/s]Evaluating:  74%|███████▎  | 765/1040 [04:43<01:41,  2.71it/s]Evaluating:  74%|███████▎  | 766/1040 [04:43<01:41,  2.71it/s]Evaluating:  74%|███████▍  | 767/1040 [04:43<01:40,  2.71it/s]Evaluating:  74%|███████▍  | 768/1040 [04:44<01:40,  2.71it/s]Evaluating:  74%|███████▍  | 769/1040 [04:44<01:40,  2.71it/s]Evaluating:  74%|███████▍  | 770/1040 [04:44<01:39,  2.71it/s]Evaluating:  74%|███████▍  | 771/1040 [04:45<01:39,  2.71it/s]Evaluating:  74%|███████▍  | 772/1040 [04:45<01:38,  2.71it/s]Evaluating:  74%|███████▍  | 773/1040 [04:45<01:37,  2.73it/s]Evaluating:  74%|███████▍  | 774/1040 [04:46<01:37,  2.72it/s]Evaluating:  75%|███████▍  | 775/1040 [04:46<01:37,  2.72it/s]Evaluating:  75%|███████▍  | 776/1040 [04:47<01:37,  2.71it/s]Evaluating:  75%|███████▍  | 777/1040 [04:47<01:36,  2.71it/s]Evaluating:  75%|███████▍  | 778/1040 [04:47<01:36,  2.71it/s]Evaluating:  75%|███████▍  | 779/1040 [04:48<01:36,  2.71it/s]Evaluating:  75%|███████▌  | 780/1040 [04:48<01:35,  2.71it/s]Evaluating:  75%|███████▌  | 781/1040 [04:48<01:35,  2.72it/s]Evaluating:  75%|███████▌  | 782/1040 [04:49<01:34,  2.72it/s]Evaluating:  75%|███████▌  | 783/1040 [04:49<01:34,  2.72it/s]Evaluating:  75%|███████▌  | 784/1040 [04:50<01:34,  2.72it/s]Evaluating:  75%|███████▌  | 785/1040 [04:50<01:33,  2.72it/s]Evaluating:  76%|███████▌  | 786/1040 [04:50<01:33,  2.72it/s]Evaluating:  76%|███████▌  | 787/1040 [04:51<01:33,  2.71it/s]Evaluating:  76%|███████▌  | 788/1040 [04:51<01:32,  2.72it/s]Evaluating:  76%|███████▌  | 789/1040 [04:51<01:31,  2.73it/s]Evaluating:  76%|███████▌  | 790/1040 [04:52<01:32,  2.71it/s]Evaluating:  76%|███████▌  | 791/1040 [04:52<01:31,  2.71it/s]Evaluating:  76%|███████▌  | 792/1040 [04:52<01:31,  2.71it/s]Evaluating:  76%|███████▋  | 793/1040 [04:53<01:31,  2.70it/s]Evaluating:  76%|███████▋  | 794/1040 [04:53<01:30,  2.71it/s]Evaluating:  76%|███████▋  | 795/1040 [04:54<01:30,  2.72it/s]Evaluating:  77%|███████▋  | 796/1040 [04:54<01:30,  2.71it/s]Evaluating:  77%|███████▋  | 797/1040 [04:54<01:29,  2.71it/s]Evaluating:  77%|███████▋  | 798/1040 [04:55<01:29,  2.71it/s]Evaluating:  77%|███████▋  | 799/1040 [04:55<01:29,  2.70it/s]Evaluating:  77%|███████▋  | 800/1040 [04:55<01:29,  2.69it/s]Evaluating:  77%|███████▋  | 801/1040 [04:56<01:28,  2.70it/s]Evaluating:  77%|███████▋  | 802/1040 [04:56<01:27,  2.72it/s]Evaluating:  77%|███████▋  | 803/1040 [04:57<01:26,  2.73it/s]Evaluating:  77%|███████▋  | 804/1040 [04:57<01:26,  2.72it/s]Evaluating:  77%|███████▋  | 805/1040 [04:57<01:26,  2.72it/s]Evaluating:  78%|███████▊  | 806/1040 [04:58<01:26,  2.71it/s]Evaluating:  78%|███████▊  | 807/1040 [04:58<01:26,  2.70it/s]Evaluating:  78%|███████▊  | 808/1040 [04:58<01:25,  2.71it/s]Evaluating:  78%|███████▊  | 809/1040 [04:59<01:24,  2.72it/s]Evaluating:  78%|███████▊  | 810/1040 [04:59<01:24,  2.71it/s]Evaluating:  78%|███████▊  | 811/1040 [04:59<01:24,  2.70it/s]Evaluating:  78%|███████▊  | 812/1040 [05:00<01:24,  2.70it/s]Evaluating:  78%|███████▊  | 813/1040 [05:00<01:23,  2.71it/s]Evaluating:  78%|███████▊  | 814/1040 [05:01<01:23,  2.70it/s]Evaluating:  78%|███████▊  | 815/1040 [05:01<01:23,  2.70it/s]Evaluating:  78%|███████▊  | 816/1040 [05:01<01:22,  2.70it/s]Evaluating:  79%|███████▊  | 817/1040 [05:02<01:22,  2.70it/s]Evaluating:  79%|███████▊  | 818/1040 [05:02<01:22,  2.71it/s]Evaluating:  79%|███████▉  | 819/1040 [05:02<01:21,  2.72it/s]Evaluating:  79%|███████▉  | 820/1040 [05:03<01:20,  2.72it/s]Evaluating:  79%|███████▉  | 821/1040 [05:03<01:20,  2.71it/s]Evaluating:  79%|███████▉  | 822/1040 [05:04<01:20,  2.71it/s]Evaluating:  79%|███████▉  | 823/1040 [05:04<01:19,  2.72it/s]Evaluating:  79%|███████▉  | 824/1040 [05:04<01:19,  2.73it/s]Evaluating:  79%|███████▉  | 825/1040 [05:05<01:19,  2.72it/s]Evaluating:  79%|███████▉  | 826/1040 [05:05<01:18,  2.71it/s]Evaluating:  80%|███████▉  | 827/1040 [05:05<01:18,  2.70it/s]Evaluating:  80%|███████▉  | 828/1040 [05:06<01:19,  2.66it/s]Evaluating:  80%|███████▉  | 829/1040 [05:06<01:18,  2.68it/s]Evaluating:  80%|███████▉  | 830/1040 [05:07<01:17,  2.69it/s]Evaluating:  80%|███████▉  | 831/1040 [05:07<01:17,  2.70it/s]Evaluating:  80%|████████  | 832/1040 [05:07<01:16,  2.70it/s]Evaluating:  80%|████████  | 833/1040 [05:08<01:16,  2.72it/s]Evaluating:  80%|████████  | 834/1040 [05:08<01:15,  2.73it/s]Evaluating:  80%|████████  | 835/1040 [05:08<01:14,  2.74it/s]Evaluating:  80%|████████  | 836/1040 [05:09<01:14,  2.74it/s]Evaluating:  80%|████████  | 837/1040 [05:09<01:14,  2.73it/s]Evaluating:  81%|████████  | 838/1040 [05:09<01:13,  2.73it/s]Evaluating:  81%|████████  | 839/1040 [05:10<01:13,  2.72it/s]Evaluating:  81%|████████  | 840/1040 [05:10<01:13,  2.72it/s]Evaluating:  81%|████████  | 841/1040 [05:11<01:12,  2.73it/s]Evaluating:  81%|████████  | 842/1040 [05:11<01:12,  2.73it/s]Evaluating:  81%|████████  | 843/1040 [05:11<01:12,  2.73it/s]Evaluating:  81%|████████  | 844/1040 [05:12<01:11,  2.73it/s]Evaluating:  81%|████████▏ | 845/1040 [05:12<01:11,  2.73it/s]Evaluating:  81%|████████▏ | 846/1040 [05:12<01:11,  2.73it/s]Evaluating:  81%|████████▏ | 847/1040 [05:13<01:10,  2.72it/s]Evaluating:  82%|████████▏ | 848/1040 [05:13<01:10,  2.72it/s]Evaluating:  82%|████████▏ | 849/1040 [05:13<01:10,  2.73it/s]Evaluating:  82%|████████▏ | 850/1040 [05:14<01:09,  2.72it/s]Evaluating:  82%|████████▏ | 851/1040 [05:14<01:09,  2.72it/s]Evaluating:  82%|████████▏ | 852/1040 [05:15<01:09,  2.71it/s]Evaluating:  82%|████████▏ | 853/1040 [05:15<01:09,  2.71it/s]Evaluating:  82%|████████▏ | 854/1040 [05:15<01:08,  2.71it/s]Evaluating:  82%|████████▏ | 855/1040 [05:16<01:08,  2.71it/s]Evaluating:  82%|████████▏ | 856/1040 [05:16<01:07,  2.72it/s]Evaluating:  82%|████████▏ | 857/1040 [05:16<01:07,  2.71it/s]Evaluating:  82%|████████▎ | 858/1040 [05:17<01:07,  2.71it/s]Evaluating:  83%|████████▎ | 859/1040 [05:17<01:06,  2.71it/s]Evaluating:  83%|████████▎ | 860/1040 [05:18<01:06,  2.71it/s]Evaluating:  83%|████████▎ | 861/1040 [05:18<01:06,  2.70it/s]Evaluating:  83%|████████▎ | 862/1040 [05:18<01:05,  2.70it/s]Evaluating:  83%|████████▎ | 863/1040 [05:19<01:05,  2.71it/s]Evaluating:  83%|████████▎ | 864/1040 [05:19<01:04,  2.71it/s]Evaluating:  83%|████████▎ | 865/1040 [05:19<01:04,  2.71it/s]Evaluating:  83%|████████▎ | 866/1040 [05:20<01:03,  2.72it/s]Evaluating:  83%|████████▎ | 867/1040 [05:20<01:03,  2.72it/s]Evaluating:  83%|████████▎ | 868/1040 [05:20<01:03,  2.70it/s]Evaluating:  84%|████████▎ | 869/1040 [05:21<01:03,  2.70it/s]Evaluating:  84%|████████▎ | 870/1040 [05:21<01:02,  2.70it/s]Evaluating:  84%|████████▍ | 871/1040 [05:22<01:02,  2.70it/s]Evaluating:  84%|████████▍ | 872/1040 [05:22<01:02,  2.70it/s]Evaluating:  84%|████████▍ | 873/1040 [05:22<01:01,  2.70it/s]Evaluating:  84%|████████▍ | 874/1040 [05:23<01:01,  2.70it/s]Evaluating:  84%|████████▍ | 875/1040 [05:23<01:01,  2.69it/s]Evaluating:  84%|████████▍ | 876/1040 [05:23<01:00,  2.69it/s]Evaluating:  84%|████████▍ | 877/1040 [05:24<01:00,  2.68it/s]Evaluating:  84%|████████▍ | 878/1040 [05:24<01:00,  2.69it/s]Evaluating:  85%|████████▍ | 879/1040 [05:25<00:59,  2.69it/s]Evaluating:  85%|████████▍ | 880/1040 [05:25<00:59,  2.70it/s]Evaluating:  85%|████████▍ | 881/1040 [05:25<00:58,  2.70it/s]Evaluating:  85%|████████▍ | 882/1040 [05:26<00:58,  2.70it/s]Evaluating:  85%|████████▍ | 883/1040 [05:26<00:58,  2.70it/s]Evaluating:  85%|████████▌ | 884/1040 [05:26<00:57,  2.70it/s]Evaluating:  85%|████████▌ | 885/1040 [05:27<00:57,  2.71it/s]Evaluating:  85%|████████▌ | 886/1040 [05:27<00:56,  2.70it/s]Evaluating:  85%|████████▌ | 887/1040 [05:28<00:56,  2.70it/s]Evaluating:  85%|████████▌ | 888/1040 [05:28<00:56,  2.70it/s]Evaluating:  85%|████████▌ | 889/1040 [05:28<00:55,  2.70it/s]Evaluating:  86%|████████▌ | 890/1040 [05:29<00:55,  2.71it/s]Evaluating:  86%|████████▌ | 891/1040 [05:29<00:54,  2.72it/s]Evaluating:  86%|████████▌ | 892/1040 [05:29<00:54,  2.73it/s]Evaluating:  86%|████████▌ | 893/1040 [05:30<00:54,  2.72it/s]Evaluating:  86%|████████▌ | 894/1040 [05:30<00:53,  2.71it/s]Evaluating:  86%|████████▌ | 895/1040 [05:30<00:53,  2.70it/s]Evaluating:  86%|████████▌ | 896/1040 [05:31<00:53,  2.71it/s]Evaluating:  86%|████████▋ | 897/1040 [05:31<00:52,  2.71it/s]Evaluating:  86%|████████▋ | 898/1040 [05:32<00:52,  2.71it/s]Evaluating:  86%|████████▋ | 899/1040 [05:32<00:51,  2.72it/s]Evaluating:  87%|████████▋ | 900/1040 [05:32<00:51,  2.72it/s]Evaluating:  87%|████████▋ | 901/1040 [05:33<00:51,  2.71it/s]Evaluating:  87%|████████▋ | 902/1040 [05:33<00:50,  2.71it/s]Evaluating:  87%|████████▋ | 903/1040 [05:33<00:50,  2.71it/s]Evaluating:  87%|████████▋ | 904/1040 [05:34<00:50,  2.71it/s]Evaluating:  87%|████████▋ | 905/1040 [05:34<00:49,  2.71it/s]Evaluating:  87%|████████▋ | 906/1040 [05:35<00:49,  2.71it/s]Evaluating:  87%|████████▋ | 907/1040 [05:35<00:48,  2.71it/s]Evaluating:  87%|████████▋ | 908/1040 [05:35<00:48,  2.71it/s]Evaluating:  87%|████████▋ | 909/1040 [05:36<00:48,  2.72it/s]Evaluating:  88%|████████▊ | 910/1040 [05:36<00:47,  2.73it/s]Evaluating:  88%|████████▊ | 911/1040 [05:36<00:47,  2.73it/s]Evaluating:  88%|████████▊ | 912/1040 [05:37<00:47,  2.71it/s]Evaluating:  88%|████████▊ | 913/1040 [05:37<00:46,  2.71it/s]Evaluating:  88%|████████▊ | 914/1040 [05:37<00:46,  2.72it/s]Evaluating:  88%|████████▊ | 915/1040 [05:38<00:46,  2.71it/s]Evaluating:  88%|████████▊ | 916/1040 [05:38<00:45,  2.71it/s]Evaluating:  88%|████████▊ | 917/1040 [05:39<00:45,  2.71it/s]Evaluating:  88%|████████▊ | 918/1040 [05:39<00:44,  2.72it/s]Evaluating:  88%|████████▊ | 919/1040 [05:39<00:44,  2.73it/s]Evaluating:  88%|████████▊ | 920/1040 [05:40<00:43,  2.73it/s]Evaluating:  89%|████████▊ | 921/1040 [05:40<00:43,  2.73it/s]Evaluating:  89%|████████▊ | 922/1040 [05:40<00:43,  2.73it/s]Evaluating:  89%|████████▉ | 923/1040 [05:41<00:42,  2.73it/s]Evaluating:  89%|████████▉ | 924/1040 [05:41<00:42,  2.72it/s]Evaluating:  89%|████████▉ | 925/1040 [05:42<00:42,  2.72it/s]Evaluating:  89%|████████▉ | 926/1040 [05:42<00:41,  2.72it/s]Evaluating:  89%|████████▉ | 927/1040 [05:42<00:41,  2.72it/s]Evaluating:  89%|████████▉ | 928/1040 [05:43<00:41,  2.72it/s]Evaluating:  89%|████████▉ | 929/1040 [05:43<00:40,  2.72it/s]Evaluating:  89%|████████▉ | 930/1040 [05:43<00:40,  2.72it/s]Evaluating:  90%|████████▉ | 931/1040 [05:44<00:40,  2.72it/s]Evaluating:  90%|████████▉ | 932/1040 [05:44<00:39,  2.72it/s]Evaluating:  90%|████████▉ | 933/1040 [05:44<00:39,  2.73it/s]Evaluating:  90%|████████▉ | 934/1040 [05:45<00:38,  2.73it/s]Evaluating:  90%|████████▉ | 935/1040 [05:45<00:38,  2.73it/s]Evaluating:  90%|█████████ | 936/1040 [05:46<00:38,  2.73it/s]Evaluating:  90%|█████████ | 937/1040 [05:46<00:37,  2.73it/s]Evaluating:  90%|█████████ | 938/1040 [05:46<00:37,  2.73it/s]Evaluating:  90%|█████████ | 939/1040 [05:47<00:36,  2.73it/s]Evaluating:  90%|█████████ | 940/1040 [05:47<00:36,  2.73it/s]Evaluating:  90%|█████████ | 941/1040 [05:47<00:36,  2.72it/s]Evaluating:  91%|█████████ | 942/1040 [05:48<00:35,  2.72it/s]Evaluating:  91%|█████████ | 943/1040 [05:48<00:35,  2.72it/s]Evaluating:  91%|█████████ | 944/1040 [05:48<00:35,  2.71it/s]Evaluating:  91%|█████████ | 945/1040 [05:49<00:35,  2.70it/s]Evaluating:  91%|█████████ | 946/1040 [05:49<00:34,  2.71it/s]Evaluating:  91%|█████████ | 947/1040 [05:50<00:34,  2.71it/s]Evaluating:  91%|█████████ | 948/1040 [05:50<00:33,  2.71it/s]Evaluating:  91%|█████████▏| 949/1040 [05:50<00:33,  2.71it/s]Evaluating:  91%|█████████▏| 950/1040 [05:51<00:33,  2.72it/s]Evaluating:  91%|█████████▏| 951/1040 [05:51<00:32,  2.73it/s]Evaluating:  92%|█████████▏| 952/1040 [05:51<00:32,  2.72it/s]Evaluating:  92%|█████████▏| 953/1040 [05:52<00:32,  2.71it/s]Evaluating:  92%|█████████▏| 954/1040 [05:52<00:31,  2.71it/s]Evaluating:  92%|█████████▏| 955/1040 [05:53<00:31,  2.71it/s]Evaluating:  92%|█████████▏| 956/1040 [05:53<00:31,  2.71it/s]Evaluating:  92%|█████████▏| 957/1040 [05:53<00:30,  2.70it/s]Evaluating:  92%|█████████▏| 958/1040 [05:54<00:30,  2.71it/s]Evaluating:  92%|█████████▏| 959/1040 [05:54<00:29,  2.70it/s]Evaluating:  92%|█████████▏| 960/1040 [05:54<00:29,  2.70it/s]Evaluating:  92%|█████████▏| 961/1040 [05:55<00:29,  2.70it/s]Evaluating:  92%|█████████▎| 962/1040 [05:55<00:28,  2.71it/s]Evaluating:  93%|█████████▎| 963/1040 [05:56<00:28,  2.71it/s]Evaluating:  93%|█████████▎| 964/1040 [05:56<00:28,  2.70it/s]Evaluating:  93%|█████████▎| 965/1040 [05:56<00:27,  2.71it/s]Evaluating:  93%|█████████▎| 966/1040 [05:57<00:27,  2.71it/s]Evaluating:  93%|█████████▎| 967/1040 [05:57<00:26,  2.72it/s]Evaluating:  93%|█████████▎| 968/1040 [05:57<00:26,  2.72it/s]Evaluating:  93%|█████████▎| 969/1040 [05:58<00:26,  2.70it/s]Evaluating:  93%|█████████▎| 970/1040 [05:58<00:25,  2.71it/s]Evaluating:  93%|█████████▎| 971/1040 [05:58<00:25,  2.71it/s]Evaluating:  93%|█████████▎| 972/1040 [05:59<00:25,  2.70it/s]Evaluating:  94%|█████████▎| 973/1040 [05:59<00:24,  2.71it/s]Evaluating:  94%|█████████▎| 974/1040 [06:00<00:24,  2.72it/s]Evaluating:  94%|█████████▍| 975/1040 [06:00<00:23,  2.73it/s]Evaluating:  94%|█████████▍| 976/1040 [06:00<00:23,  2.72it/s]Evaluating:  94%|█████████▍| 977/1040 [06:01<00:23,  2.72it/s]Evaluating:  94%|█████████▍| 978/1040 [06:01<00:22,  2.72it/s]Evaluating:  94%|█████████▍| 979/1040 [06:01<00:22,  2.73it/s]Evaluating:  94%|█████████▍| 980/1040 [06:02<00:21,  2.73it/s]Evaluating:  94%|█████████▍| 981/1040 [06:02<00:21,  2.73it/s]Evaluating:  94%|█████████▍| 982/1040 [06:02<00:21,  2.72it/s]Evaluating:  95%|█████████▍| 983/1040 [06:03<00:20,  2.73it/s]Evaluating:  95%|█████████▍| 984/1040 [06:03<00:20,  2.74it/s]Evaluating:  95%|█████████▍| 985/1040 [06:04<00:20,  2.74it/s]Evaluating:  95%|█████████▍| 986/1040 [06:04<00:19,  2.74it/s]Evaluating:  95%|█████████▍| 987/1040 [06:04<00:19,  2.75it/s]Evaluating:  95%|█████████▌| 988/1040 [06:05<00:18,  2.75it/s]Evaluating:  95%|█████████▌| 989/1040 [06:05<00:18,  2.75it/s]Evaluating:  95%|█████████▌| 990/1040 [06:05<00:18,  2.75it/s]Evaluating:  95%|█████████▌| 991/1040 [06:06<00:17,  2.75it/s]Evaluating:  95%|█████████▌| 992/1040 [06:06<00:17,  2.74it/s]Evaluating:  95%|█████████▌| 993/1040 [06:07<00:17,  2.73it/s]Evaluating:  96%|█████████▌| 994/1040 [06:07<00:16,  2.73it/s]Evaluating:  96%|█████████▌| 995/1040 [06:07<00:16,  2.72it/s]Evaluating:  96%|█████████▌| 996/1040 [06:08<00:16,  2.73it/s]Evaluating:  96%|█████████▌| 997/1040 [06:08<00:15,  2.74it/s]Evaluating:  96%|█████████▌| 998/1040 [06:08<00:15,  2.74it/s]Evaluating:  96%|█████████▌| 999/1040 [06:09<00:14,  2.73it/s]Evaluating:  96%|█████████▌| 1000/1040 [06:09<00:14,  2.74it/s]Evaluating:  96%|█████████▋| 1001/1040 [06:09<00:14,  2.74it/s]Evaluating:  96%|█████████▋| 1002/1040 [06:10<00:13,  2.73it/s]Evaluating:  96%|█████████▋| 1003/1040 [06:10<00:13,  2.73it/s]Evaluating:  97%|█████████▋| 1004/1040 [06:11<00:13,  2.72it/s]Evaluating:  97%|█████████▋| 1005/1040 [06:11<00:12,  2.73it/s]Evaluating:  97%|█████████▋| 1006/1040 [06:11<00:12,  2.73it/s]Evaluating:  97%|█████████▋| 1007/1040 [06:12<00:12,  2.73it/s]Evaluating:  97%|█████████▋| 1008/1040 [06:12<00:11,  2.73it/s]Evaluating:  97%|█████████▋| 1009/1040 [06:12<00:11,  2.73it/s]Evaluating:  97%|█████████▋| 1010/1040 [06:13<00:11,  2.71it/s]Evaluating:  97%|█████████▋| 1011/1040 [06:13<00:10,  2.71it/s]Evaluating:  97%|█████████▋| 1012/1040 [06:13<00:10,  2.69it/s]Evaluating:  97%|█████████▋| 1013/1040 [06:14<00:09,  2.70it/s]Evaluating:  98%|█████████▊| 1014/1040 [06:14<00:09,  2.70it/s]Evaluating:  98%|█████████▊| 1015/1040 [06:15<00:09,  2.70it/s]Evaluating:  98%|█████████▊| 1016/1040 [06:15<00:08,  2.71it/s]Evaluating:  98%|█████████▊| 1017/1040 [06:15<00:08,  2.70it/s]Evaluating:  98%|█████████▊| 1018/1040 [06:16<00:08,  2.71it/s]Evaluating:  98%|█████████▊| 1019/1040 [06:16<00:07,  2.72it/s]Evaluating:  98%|█████████▊| 1020/1040 [06:16<00:07,  2.71it/s]Evaluating:  98%|█████████▊| 1021/1040 [06:17<00:07,  2.71it/s]Evaluating:  98%|█████████▊| 1022/1040 [06:17<00:06,  2.72it/s]Evaluating:  98%|█████████▊| 1023/1040 [06:18<00:06,  2.72it/s]Evaluating:  98%|█████████▊| 1024/1040 [06:18<00:05,  2.72it/s]Evaluating:  99%|█████████▊| 1025/1040 [06:18<00:05,  2.73it/s]Evaluating:  99%|█████████▊| 1026/1040 [06:19<00:05,  2.73it/s]Evaluating:  99%|█████████▉| 1027/1040 [06:19<00:04,  2.74it/s]Evaluating:  99%|█████████▉| 1028/1040 [06:19<00:04,  2.73it/s]Evaluating:  99%|█████████▉| 1029/1040 [06:20<00:04,  2.73it/s]Evaluating:  99%|█████████▉| 1030/1040 [06:20<00:03,  2.74it/s]Evaluating:  99%|█████████▉| 1031/1040 [06:20<00:03,  2.73it/s]Evaluating:  99%|█████████▉| 1032/1040 [06:21<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 1033/1040 [06:21<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 1034/1040 [06:22<00:02,  2.74it/s]Evaluating: 100%|█████████▉| 1035/1040 [06:22<00:01,  2.69it/s]Evaluating: 100%|█████████▉| 1036/1040 [06:22<00:01,  2.70it/s]Evaluating: 100%|█████████▉| 1037/1040 [06:23<00:01,  2.71it/s]Evaluating: 100%|█████████▉| 1038/1040 [06:23<00:00,  2.72it/s]Evaluating: 100%|█████████▉| 1039/1040 [06:23<00:00,  2.73it/s]Evaluating: 100%|██████████| 1040/1040 [06:24<00:00,  3.35it/s]Evaluating: 100%|██████████| 1040/1040 [06:24<00:00,  2.71it/s]
05/10/2022 00:40:08 - INFO - __main__ -     Evaluation done in total 384.048589 secs (0.046187 sec per example)
05/10/2022 00:40:32 - INFO - __main__ -   Results: {'exact': 50.68243858052775, 'f1': 72.19201096017981, 'total': 5495, 'HasAns_exact': 50.68243858052775, 'HasAns_f1': 72.19201096017981, 'HasAns_total': 5495, 'best_exact': 50.68243858052775, 'best_exact_thresh': 0.0, 'best_f1': 72.19201096017981, 'best_f1_thresh': 0.0}
  zh 
2022-05-10 00:40:35.764839: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/10/2022 00:40:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'qa_outputs.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'qa_outputs.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:41:00 - INFO - __main__ -   lang2id = None
05/10/2022 00:41:05 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='zh', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/adapter/xlmr_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/mlqa/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/download//mlqa//MLQA_V1/test/test-context-zh-question-zh.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/10/2022 00:41:05 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/10/2022 00:41:05 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.bias', 'base_model.encoder.layer.14.attention.self.key.bias', 'base_model.encoder.layer.19.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.key.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.20.output.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.bias', 'base_model.encoder.layer.20.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.23.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.key.weight', 'base_model.encoder.layer.19.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.query.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.16.attention.self.key.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.13.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.12.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.17.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.attention.output.dense.bias', 'base_model.encoder.layer.21.attention.output.dense.weight', 'base_model.encoder.layer.23.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.output.dense.weight', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.13.attention.self.value.weight', 'base_model.encoder.layer.23.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.query.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.23.intermediate.dense.weight', 'base_model.encoder.layer.21.output.LayerNorm.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.self.value.bias', 'base_model.encoder.layer.21.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.15.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.19.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.key.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.15.output.dense.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.query.bias', 'base_model.encoder.layer.16.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.21.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.self.value.weight', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.13.attention.self.key.weight', 'base_model.encoder.layer.14.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.20.intermediate.dense.weight', 'base_model.encoder.layer.18.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.14.attention.self.value.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.dense.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.14.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.21.attention.output.dense.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.LayerNorm.weight', 'base_model.encoder.layer.17.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.23.attention.self.value.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.13.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.intermediate.dense.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.query.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.encoder.layer.12.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.15.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.22.attention.self.query.weight', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.16.attention.self.query.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.20.attention.self.key.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.20.output.LayerNorm.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.20.attention.self.query.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.16.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.15.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.query.bias', 'base_model.encoder.layer.17.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.12.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.value.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.20.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.12.attention.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.21.intermediate.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.15.intermediate.dense.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.dense.bias', 'base_model.encoder.layer.12.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.12.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.21.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.21.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.12.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.output.dense.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.21.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.22.attention.self.key.bias', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.attention.self.value.weight', 'base_model.encoder.layer.23.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.21.output.dense.weight', 'base_model.encoder.layer.15.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.16.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.22.attention.output.dense.weight', 'base_model.encoder.layer.19.intermediate.dense.weight', 'base_model.encoder.layer.15.attention.self.query.weight', 'base_model.encoder.layer.20.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.18.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.16.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.attention.self.query.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.13.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.18.attention.self.key.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.14.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.17.attention.output.dense.weight', 'base_model.encoder.layer.20.attention.self.query.bias', 'base_model.encoder.layer.20.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.16.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.23.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.17.output.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.22.output.dense.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.22.intermediate.dense.bias', 'base_model.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.20.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.12.attention.self.key.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.19.attention.output.dense.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.12.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.22.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.22.intermediate.dense.weight', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.20.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.16.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.19.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.20.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.14.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.23.intermediate.dense.bias', 'base_model.encoder.layer.14.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.22.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.14.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.15.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.16.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.13.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.20.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.23.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.12.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.23.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.value.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.encoder.layer.16.attention.self.value.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.21.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.18.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.self.value.weight', 'base_model.encoder.layer.22.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.19.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.14.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.18.attention.self.key.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.encoder.layer.17.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.16.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.12.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.14.output.LayerNorm.bias', 'base_model.encoder.layer.13.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.22.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.14.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.13.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.18.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.12.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.15.attention.output.dense.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.19.attention.self.value.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.output.LayerNorm.bias', 'base_model.encoder.layer.23.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.15.attention.self.value.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.attention.self.value.weight', 'base_model.encoder.layer.14.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'qa_outputs.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.13.attention.self.value.bias', 'base_model.encoder.layer.19.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.18.attention.output.dense.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.12.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.15.attention.self.key.weight', 'base_model.encoder.layer.14.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.17.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.20.output.LayerNorm.bias', 'base_model.encoder.layer.18.output.LayerNorm.weight', 'base_model.encoder.layer.17.attention.self.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'qa_outputs.bias', 'base_model.encoder.layer.16.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.18.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.17.attention.self.query.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.12.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.19.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.14.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.13.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.18.output.dense.weight', 'base_model.encoder.layer.21.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.15.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.22.attention.self.key.weight', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.12.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.23.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.16.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.19.output.dense.weight', 'base_model.encoder.layer.13.intermediate.dense.weight', 'base_model.encoder.layer.20.attention.self.value.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.18.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.22.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.12.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.13.output.dense.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.16.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.12.output.dense.weight', 'base_model.encoder.layer.13.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.17.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.21.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.18.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.22.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.23.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.output.dense.bias', 'base_model.encoder.layer.18.attention.self.query.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.19.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.22.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.16.attention.self.value.weight', 'base_model.encoder.layer.19.attention.output.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.17.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.18.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.14.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.16.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.19.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.13.attention.output.dense.bias', 'base_model.encoder.layer.20.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.17.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.18.intermediate.dense.weight', 'base_model.encoder.layer.13.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.17.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.16.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.21.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.22.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.16.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.21.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.17.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.15.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.14.attention.self.key.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.19.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.encoder.layer.23.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.14.intermediate.dense.weight', 'base_model.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.encoder.layer.18.output.LayerNorm.bias', 'base_model.encoder.layer.22.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/10/2022 00:41:32 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlmr/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 1024,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 16,
  "n_langs": 1,
  "n_layers": 24,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/2429 [00:00<?, ?it/s] 10%|█         | 245/2429 [00:00<00:00, 2448.23it/s] 20%|██        | 490/2429 [00:00<00:01, 1906.30it/s] 28%|██▊       | 688/2429 [00:00<00:00, 1785.64it/s] 40%|████      | 983/2429 [00:00<00:00, 2184.17it/s] 50%|████▉     | 1210/2429 [00:00<00:00, 2141.64it/s] 62%|██████▏   | 1512/2429 [00:00<00:00, 2411.98it/s] 72%|███████▏  | 1759/2429 [00:00<00:00, 2418.04it/s] 84%|████████▍ | 2043/2429 [00:00<00:00, 2544.26it/s]100%|██████████| 2429/2429 [00:00<00:00, 2571.02it/s]
convert squad examples to features:   0%|          | 0/5137 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   1%|          | 33/5137 [00:00<00:23, 219.61it/s]convert squad examples to features:   2%|▏         | 97/5137 [00:00<00:13, 364.79it/s]convert squad examples to features:   3%|▎         | 161/5137 [00:00<00:11, 428.27it/s]convert squad examples to features:   4%|▍         | 225/5137 [00:00<00:11, 445.90it/s]convert squad examples to features:   5%|▌         | 271/5137 [00:00<00:12, 392.45it/s]convert squad examples to features:   6%|▌         | 321/5137 [00:00<00:16, 294.89it/s]convert squad examples to features:   7%|▋         | 354/5137 [00:01<00:21, 226.61it/s]convert squad examples to features:   8%|▊         | 417/5137 [00:01<00:16, 293.61it/s]convert squad examples to features:   9%|▉         | 481/5137 [00:01<00:13, 354.49it/s]convert squad examples to features:  11%|█         | 545/5137 [00:01<00:13, 346.87it/s]convert squad examples to features:  11%|█▏        | 585/5137 [00:01<00:13, 338.15it/s]convert squad examples to features:  12%|█▏        | 622/5137 [00:01<00:14, 309.77it/s]convert squad examples to features:  13%|█▎        | 673/5137 [00:02<00:14, 299.05it/s]convert squad examples to features:  14%|█▎        | 705/5137 [00:02<00:15, 279.31it/s]convert squad examples to features:  14%|█▍        | 737/5137 [00:02<00:15, 276.70it/s]convert squad examples to features:  16%|█▌        | 801/5137 [00:02<00:12, 333.57it/s]convert squad examples to features:  17%|█▋        | 865/5137 [00:02<00:11, 360.56it/s]convert squad examples to features:  18%|█▊        | 902/5137 [00:02<00:13, 317.19it/s]convert squad examples to features:  18%|█▊        | 935/5137 [00:02<00:13, 310.10it/s]convert squad examples to features:  19%|█▉        | 993/5137 [00:03<00:12, 336.28it/s]convert squad examples to features:  21%|██        | 1057/5137 [00:03<00:15, 268.63it/s]convert squad examples to features:  21%|██        | 1089/5137 [00:03<00:15, 258.87it/s]convert squad examples to features:  22%|██▏       | 1153/5137 [00:03<00:12, 313.54it/s]convert squad examples to features:  24%|██▎       | 1217/5137 [00:03<00:12, 319.51it/s]convert squad examples to features:  24%|██▍       | 1251/5137 [00:03<00:12, 303.96it/s]convert squad examples to features:  25%|██▍       | 1283/5137 [00:04<00:13, 294.56it/s]convert squad examples to features:  26%|██▌       | 1313/5137 [00:04<00:13, 278.09it/s]convert squad examples to features:  26%|██▌       | 1345/5137 [00:04<00:13, 287.30it/s]convert squad examples to features:  27%|██▋       | 1377/5137 [00:04<00:13, 288.76it/s]convert squad examples to features:  28%|██▊       | 1441/5137 [00:04<00:12, 305.52it/s]convert squad examples to features:  29%|██▊       | 1473/5137 [00:04<00:13, 262.34it/s]convert squad examples to features:  30%|██▉       | 1537/5137 [00:04<00:12, 297.88it/s]convert squad examples to features:  31%|███       | 1601/5137 [00:05<00:10, 334.15it/s]convert squad examples to features:  32%|███▏      | 1635/5137 [00:05<00:12, 287.96it/s]convert squad examples to features:  33%|███▎      | 1697/5137 [00:05<00:10, 313.24it/s]convert squad examples to features:  34%|███▍      | 1761/5137 [00:05<00:10, 329.96it/s]convert squad examples to features:  36%|███▌      | 1825/5137 [00:05<00:09, 349.65it/s]convert squad examples to features:  37%|███▋      | 1889/5137 [00:05<00:09, 353.77it/s]convert squad examples to features:  37%|███▋      | 1925/5137 [00:06<00:13, 246.22it/s]convert squad examples to features:  40%|███▉      | 2049/5137 [00:06<00:08, 373.48it/s]convert squad examples to features:  41%|████      | 2113/5137 [00:06<00:08, 357.05it/s]convert squad examples to features:  42%|████▏     | 2177/5137 [00:06<00:07, 386.58it/s]convert squad examples to features:  44%|████▎     | 2241/5137 [00:06<00:06, 431.02it/s]convert squad examples to features:  45%|████▍     | 2289/5137 [00:07<00:06, 419.03it/s]convert squad examples to features:  45%|████▌     | 2337/5137 [00:07<00:06, 416.96it/s]convert squad examples to features:  47%|████▋     | 2401/5137 [00:07<00:06, 444.10it/s]convert squad examples to features:  48%|████▊     | 2465/5137 [00:07<00:05, 452.21it/s]convert squad examples to features:  49%|████▉     | 2529/5137 [00:07<00:05, 475.22it/s]convert squad examples to features:  50%|█████     | 2593/5137 [00:07<00:05, 474.94it/s]convert squad examples to features:  52%|█████▏    | 2657/5137 [00:08<00:08, 287.75it/s]convert squad examples to features:  52%|█████▏    | 2696/5137 [00:08<00:10, 239.75it/s]convert squad examples to features:  54%|█████▎    | 2753/5137 [00:08<00:08, 265.72it/s]convert squad examples to features:  55%|█████▍    | 2817/5137 [00:08<00:07, 316.56it/s]convert squad examples to features:  56%|█████▌    | 2881/5137 [00:08<00:06, 347.04it/s]convert squad examples to features:  57%|█████▋    | 2921/5137 [00:08<00:07, 307.59it/s]convert squad examples to features:  58%|█████▊    | 2956/5137 [00:09<00:08, 259.38it/s]convert squad examples to features:  59%|█████▉    | 3041/5137 [00:09<00:06, 327.15it/s]convert squad examples to features:  60%|██████    | 3105/5137 [00:09<00:05, 378.06it/s]convert squad examples to features:  62%|██████▏   | 3169/5137 [00:09<00:05, 382.52it/s]convert squad examples to features:  63%|██████▎   | 3211/5137 [00:09<00:05, 347.08it/s]convert squad examples to features:  64%|██████▎   | 3265/5137 [00:09<00:05, 329.06it/s]convert squad examples to features:  65%|██████▍   | 3329/5137 [00:10<00:05, 358.28it/s]convert squad examples to features:  66%|██████▌   | 3393/5137 [00:10<00:04, 410.34it/s]convert squad examples to features:  67%|██████▋   | 3457/5137 [00:10<00:04, 410.76it/s]convert squad examples to features:  68%|██████▊   | 3501/5137 [00:10<00:04, 369.80it/s]convert squad examples to features:  69%|██████▉   | 3553/5137 [00:10<00:04, 373.93it/s]convert squad examples to features:  70%|██████▉   | 3592/5137 [00:10<00:04, 334.44it/s]convert squad examples to features:  71%|███████   | 3649/5137 [00:11<00:04, 329.62it/s]convert squad examples to features:  72%|███████▏  | 3683/5137 [00:11<00:04, 319.04it/s]convert squad examples to features:  73%|███████▎  | 3745/5137 [00:11<00:04, 320.56it/s]convert squad examples to features:  74%|███████▍  | 3809/5137 [00:11<00:03, 366.48it/s]convert squad examples to features:  75%|███████▌  | 3873/5137 [00:11<00:03, 346.12it/s]convert squad examples to features:  77%|███████▋  | 3937/5137 [00:11<00:03, 359.74it/s]convert squad examples to features:  78%|███████▊  | 4001/5137 [00:12<00:03, 318.05it/s]convert squad examples to features:  79%|███████▊  | 4035/5137 [00:12<00:04, 250.61it/s]convert squad examples to features:  80%|███████▉  | 4097/5137 [00:12<00:03, 280.25it/s]convert squad examples to features:  81%|████████  | 4161/5137 [00:12<00:03, 312.36it/s]convert squad examples to features:  82%|████████▏ | 4225/5137 [00:12<00:03, 293.50it/s]convert squad examples to features:  83%|████████▎ | 4289/5137 [00:13<00:02, 313.48it/s]convert squad examples to features:  85%|████████▍ | 4353/5137 [00:13<00:02, 337.93it/s]convert squad examples to features:  86%|████████▌ | 4417/5137 [00:13<00:01, 386.54it/s]convert squad examples to features:  87%|████████▋ | 4481/5137 [00:13<00:01, 391.99it/s]convert squad examples to features:  88%|████████▊ | 4523/5137 [00:13<00:01, 396.97it/s]convert squad examples to features:  89%|████████▉ | 4565/5137 [00:13<00:01, 402.21it/s]convert squad examples to features:  90%|████████▉ | 4609/5137 [00:13<00:01, 364.09it/s]convert squad examples to features:  91%|█████████ | 4673/5137 [00:14<00:01, 362.66it/s]convert squad examples to features:  92%|█████████▏| 4711/5137 [00:14<00:01, 363.70it/s]convert squad examples to features:  93%|█████████▎| 4769/5137 [00:14<00:00, 396.17it/s]convert squad examples to features:  94%|█████████▍| 4833/5137 [00:14<00:00, 399.81it/s]convert squad examples to features:  95%|█████████▌| 4897/5137 [00:14<00:00, 445.33it/s]convert squad examples to features:  97%|█████████▋| 4961/5137 [00:14<00:00, 440.78it/s]convert squad examples to features:  98%|█████████▊| 5025/5137 [00:14<00:00, 457.40it/s]convert squad examples to features:  99%|█████████▊| 5072/5137 [00:14<00:00, 437.44it/s]convert squad examples to features: 100%|██████████| 5137/5137 [00:15<00:00, 342.33it/s]
add example index and unique id:   0%|          | 0/5137 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 5137/5137 [00:00<00:00, 503744.03it/s]
05/10/2022 00:41:49 - INFO - __main__ -   Saving features into cached file ./cached_test-context-zh-question-zh.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_zh
05/10/2022 00:41:56 - INFO - __main__ -   ***** Running evaluation  *****
05/10/2022 00:41:56 - INFO - __main__ -     Num examples = 5981
05/10/2022 00:41:56 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/748 [00:00<?, ?it/s]Evaluating:   0%|          | 1/748 [00:01<12:54,  1.04s/it]Evaluating:   0%|          | 2/748 [00:01<07:57,  1.56it/s]Evaluating:   0%|          | 3/748 [00:01<06:23,  1.94it/s]Evaluating:   1%|          | 4/748 [00:02<05:38,  2.20it/s]Evaluating:   1%|          | 5/748 [00:02<05:14,  2.36it/s]Evaluating:   1%|          | 6/748 [00:02<04:59,  2.48it/s]Evaluating:   1%|          | 7/748 [00:03<04:48,  2.57it/s]Evaluating:   1%|          | 8/748 [00:03<04:42,  2.62it/s]Evaluating:   1%|          | 9/748 [00:03<04:36,  2.68it/s]Evaluating:   1%|▏         | 10/748 [00:04<04:34,  2.69it/s]Evaluating:   1%|▏         | 11/748 [00:04<04:32,  2.71it/s]Evaluating:   2%|▏         | 12/748 [00:05<04:30,  2.72it/s]Evaluating:   2%|▏         | 13/748 [00:05<04:28,  2.74it/s]Evaluating:   2%|▏         | 14/748 [00:05<04:27,  2.74it/s]Evaluating:   2%|▏         | 15/748 [00:06<04:27,  2.74it/s]Evaluating:   2%|▏         | 16/748 [00:06<04:27,  2.74it/s]Evaluating:   2%|▏         | 17/748 [00:06<04:26,  2.74it/s]Evaluating:   2%|▏         | 18/748 [00:07<04:26,  2.74it/s]Evaluating:   3%|▎         | 19/748 [00:07<04:26,  2.73it/s]Evaluating:   3%|▎         | 20/748 [00:07<04:25,  2.74it/s]Evaluating:   3%|▎         | 21/748 [00:08<04:23,  2.76it/s]Evaluating:   3%|▎         | 22/748 [00:08<04:23,  2.76it/s]Evaluating:   3%|▎         | 23/748 [00:09<04:22,  2.76it/s]Evaluating:   3%|▎         | 24/748 [00:09<04:23,  2.75it/s]Evaluating:   3%|▎         | 25/748 [00:09<04:24,  2.74it/s]Evaluating:   3%|▎         | 26/748 [00:10<04:24,  2.73it/s]Evaluating:   4%|▎         | 27/748 [00:10<04:24,  2.73it/s]Evaluating:   4%|▎         | 28/748 [00:10<04:22,  2.74it/s]Evaluating:   4%|▍         | 29/748 [00:11<04:22,  2.74it/s]Evaluating:   4%|▍         | 30/748 [00:11<04:21,  2.75it/s]Evaluating:   4%|▍         | 31/748 [00:11<04:20,  2.75it/s]Evaluating:   4%|▍         | 32/748 [00:12<04:19,  2.76it/s]Evaluating:   4%|▍         | 33/748 [00:12<04:19,  2.76it/s]Evaluating:   5%|▍         | 34/748 [00:13<04:20,  2.74it/s]Evaluating:   5%|▍         | 35/748 [00:13<04:19,  2.75it/s]Evaluating:   5%|▍         | 36/748 [00:13<04:20,  2.73it/s]Evaluating:   5%|▍         | 37/748 [00:14<04:20,  2.73it/s]Evaluating:   5%|▌         | 38/748 [00:14<04:20,  2.72it/s]Evaluating:   5%|▌         | 39/748 [00:14<04:20,  2.72it/s]Evaluating:   5%|▌         | 40/748 [00:15<04:26,  2.66it/s]Evaluating:   5%|▌         | 41/748 [00:15<04:23,  2.69it/s]Evaluating:   6%|▌         | 42/748 [00:16<04:20,  2.71it/s]Evaluating:   6%|▌         | 43/748 [00:16<04:18,  2.72it/s]Evaluating:   6%|▌         | 44/748 [00:16<04:19,  2.72it/s]Evaluating:   6%|▌         | 45/748 [00:17<04:19,  2.71it/s]Evaluating:   6%|▌         | 46/748 [00:17<04:19,  2.71it/s]Evaluating:   6%|▋         | 47/748 [00:17<04:18,  2.71it/s]Evaluating:   6%|▋         | 48/748 [00:18<04:17,  2.72it/s]Evaluating:   7%|▋         | 49/748 [00:18<04:17,  2.71it/s]Evaluating:   7%|▋         | 50/748 [00:18<04:16,  2.72it/s]Evaluating:   7%|▋         | 51/748 [00:19<04:15,  2.72it/s]Evaluating:   7%|▋         | 52/748 [00:19<04:15,  2.73it/s]Evaluating:   7%|▋         | 53/748 [00:20<04:17,  2.70it/s]Evaluating:   7%|▋         | 54/748 [00:20<04:17,  2.69it/s]Evaluating:   7%|▋         | 55/748 [00:20<04:17,  2.69it/s]Evaluating:   7%|▋         | 56/748 [00:21<04:15,  2.71it/s]Evaluating:   8%|▊         | 57/748 [00:21<04:14,  2.71it/s]Evaluating:   8%|▊         | 58/748 [00:21<04:13,  2.72it/s]Evaluating:   8%|▊         | 59/748 [00:22<04:13,  2.72it/s]Evaluating:   8%|▊         | 60/748 [00:22<04:12,  2.73it/s]Evaluating:   8%|▊         | 61/748 [00:22<04:11,  2.73it/s]Evaluating:   8%|▊         | 62/748 [00:23<04:10,  2.73it/s]Evaluating:   8%|▊         | 63/748 [00:23<04:09,  2.74it/s]Evaluating:   9%|▊         | 64/748 [00:24<04:09,  2.75it/s]Evaluating:   9%|▊         | 65/748 [00:24<04:09,  2.74it/s]Evaluating:   9%|▉         | 66/748 [00:24<04:09,  2.74it/s]Evaluating:   9%|▉         | 67/748 [00:25<04:07,  2.75it/s]Evaluating:   9%|▉         | 68/748 [00:25<04:07,  2.75it/s]Evaluating:   9%|▉         | 69/748 [00:25<04:07,  2.74it/s]Evaluating:   9%|▉         | 70/748 [00:26<04:08,  2.73it/s]Evaluating:   9%|▉         | 71/748 [00:26<04:07,  2.74it/s]Evaluating:  10%|▉         | 72/748 [00:27<04:08,  2.73it/s]Evaluating:  10%|▉         | 73/748 [00:27<04:07,  2.73it/s]Evaluating:  10%|▉         | 74/748 [00:27<04:06,  2.74it/s]Evaluating:  10%|█         | 75/748 [00:28<04:05,  2.74it/s]Evaluating:  10%|█         | 76/748 [00:28<04:05,  2.74it/s]Evaluating:  10%|█         | 77/748 [00:28<04:05,  2.73it/s]Evaluating:  10%|█         | 78/748 [00:29<04:06,  2.72it/s]Evaluating:  11%|█         | 79/748 [00:29<04:05,  2.73it/s]Evaluating:  11%|█         | 80/748 [00:29<04:04,  2.73it/s]Evaluating:  11%|█         | 81/748 [00:30<04:03,  2.74it/s]Evaluating:  11%|█         | 82/748 [00:30<04:03,  2.73it/s]Evaluating:  11%|█         | 83/748 [00:31<04:04,  2.72it/s]Evaluating:  11%|█         | 84/748 [00:31<04:03,  2.72it/s]Evaluating:  11%|█▏        | 85/748 [00:31<04:04,  2.72it/s]Evaluating:  11%|█▏        | 86/748 [00:32<04:02,  2.72it/s]Evaluating:  12%|█▏        | 87/748 [00:32<04:02,  2.72it/s]Evaluating:  12%|█▏        | 88/748 [00:32<04:02,  2.72it/s]Evaluating:  12%|█▏        | 89/748 [00:33<04:02,  2.71it/s]Evaluating:  12%|█▏        | 90/748 [00:33<04:02,  2.71it/s]Evaluating:  12%|█▏        | 91/748 [00:33<04:02,  2.71it/s]Evaluating:  12%|█▏        | 92/748 [00:34<04:01,  2.71it/s]Evaluating:  12%|█▏        | 93/748 [00:34<04:01,  2.71it/s]Evaluating:  13%|█▎        | 94/748 [00:35<04:00,  2.72it/s]Evaluating:  13%|█▎        | 95/748 [00:35<04:00,  2.72it/s]Evaluating:  13%|█▎        | 96/748 [00:35<03:59,  2.72it/s]Evaluating:  13%|█▎        | 97/748 [00:36<03:59,  2.72it/s]Evaluating:  13%|█▎        | 98/748 [00:36<03:59,  2.72it/s]Evaluating:  13%|█▎        | 99/748 [00:36<03:58,  2.72it/s]Evaluating:  13%|█▎        | 100/748 [00:37<03:58,  2.72it/s]Evaluating:  14%|█▎        | 101/748 [00:37<03:58,  2.71it/s]Evaluating:  14%|█▎        | 102/748 [00:38<03:57,  2.72it/s]Evaluating:  14%|█▍        | 103/748 [00:38<03:56,  2.73it/s]Evaluating:  14%|█▍        | 104/748 [00:38<03:55,  2.73it/s]Evaluating:  14%|█▍        | 105/748 [00:39<03:55,  2.72it/s]Evaluating:  14%|█▍        | 106/748 [00:39<03:55,  2.72it/s]Evaluating:  14%|█▍        | 107/748 [00:39<03:54,  2.73it/s]Evaluating:  14%|█▍        | 108/748 [00:40<03:54,  2.73it/s]Evaluating:  15%|█▍        | 109/748 [00:40<03:53,  2.73it/s]Evaluating:  15%|█▍        | 110/748 [00:40<03:53,  2.74it/s]Evaluating:  15%|█▍        | 111/748 [00:41<03:53,  2.73it/s]Evaluating:  15%|█▍        | 112/748 [00:41<03:52,  2.74it/s]Evaluating:  15%|█▌        | 113/748 [00:42<03:52,  2.73it/s]Evaluating:  15%|█▌        | 114/748 [00:42<03:52,  2.73it/s]Evaluating:  15%|█▌        | 115/748 [00:42<03:51,  2.73it/s]Evaluating:  16%|█▌        | 116/748 [00:43<03:51,  2.72it/s]Evaluating:  16%|█▌        | 117/748 [00:43<03:51,  2.73it/s]Evaluating:  16%|█▌        | 118/748 [00:43<03:50,  2.74it/s]Evaluating:  16%|█▌        | 119/748 [00:44<03:50,  2.73it/s]Evaluating:  16%|█▌        | 120/748 [00:44<03:50,  2.72it/s]Evaluating:  16%|█▌        | 121/748 [00:44<03:50,  2.72it/s]Evaluating:  16%|█▋        | 122/748 [00:45<03:50,  2.72it/s]Evaluating:  16%|█▋        | 123/748 [00:45<03:49,  2.73it/s]Evaluating:  17%|█▋        | 124/748 [00:46<03:48,  2.73it/s]Evaluating:  17%|█▋        | 125/748 [00:46<03:48,  2.73it/s]Evaluating:  17%|█▋        | 126/748 [00:46<03:47,  2.73it/s]Evaluating:  17%|█▋        | 127/748 [00:47<03:48,  2.72it/s]Evaluating:  17%|█▋        | 128/748 [00:47<03:47,  2.73it/s]Evaluating:  17%|█▋        | 129/748 [00:47<03:46,  2.74it/s]Evaluating:  17%|█▋        | 130/748 [00:48<03:47,  2.71it/s]Evaluating:  18%|█▊        | 131/748 [00:48<03:47,  2.71it/s]Evaluating:  18%|█▊        | 132/748 [00:49<03:47,  2.71it/s]Evaluating:  18%|█▊        | 133/748 [00:49<03:46,  2.72it/s]Evaluating:  18%|█▊        | 134/748 [00:49<03:46,  2.71it/s]Evaluating:  18%|█▊        | 135/748 [00:50<03:46,  2.71it/s]Evaluating:  18%|█▊        | 136/748 [00:50<03:45,  2.72it/s]Evaluating:  18%|█▊        | 137/748 [00:50<03:44,  2.72it/s]Evaluating:  18%|█▊        | 138/748 [00:51<03:45,  2.71it/s]Evaluating:  19%|█▊        | 139/748 [00:51<03:43,  2.72it/s]Evaluating:  19%|█▊        | 140/748 [00:51<03:42,  2.73it/s]Evaluating:  19%|█▉        | 141/748 [00:52<03:42,  2.73it/s]Evaluating:  19%|█▉        | 142/748 [00:52<03:42,  2.73it/s]Evaluating:  19%|█▉        | 143/748 [00:53<03:41,  2.73it/s]Evaluating:  19%|█▉        | 144/748 [00:53<03:40,  2.73it/s]Evaluating:  19%|█▉        | 145/748 [00:53<03:40,  2.73it/s]Evaluating:  20%|█▉        | 146/748 [00:54<03:40,  2.74it/s]Evaluating:  20%|█▉        | 147/748 [00:54<03:39,  2.73it/s]Evaluating:  20%|█▉        | 148/748 [00:54<03:39,  2.74it/s]Evaluating:  20%|█▉        | 149/748 [00:55<03:38,  2.74it/s]Evaluating:  20%|██        | 150/748 [00:55<03:37,  2.75it/s]Evaluating:  20%|██        | 151/748 [00:55<03:36,  2.75it/s]Evaluating:  20%|██        | 152/748 [00:56<03:37,  2.74it/s]Evaluating:  20%|██        | 153/748 [00:56<03:38,  2.72it/s]Evaluating:  21%|██        | 154/748 [00:57<03:38,  2.72it/s]Evaluating:  21%|██        | 155/748 [00:57<03:38,  2.71it/s]Evaluating:  21%|██        | 156/748 [00:57<03:38,  2.71it/s]Evaluating:  21%|██        | 157/748 [00:58<03:37,  2.71it/s]Evaluating:  21%|██        | 158/748 [00:58<03:37,  2.71it/s]Evaluating:  21%|██▏       | 159/748 [00:58<03:38,  2.70it/s]Evaluating:  21%|██▏       | 160/748 [00:59<03:39,  2.68it/s]Evaluating:  22%|██▏       | 161/748 [00:59<03:38,  2.69it/s]Evaluating:  22%|██▏       | 162/748 [01:00<03:37,  2.69it/s]Evaluating:  22%|██▏       | 163/748 [01:00<03:37,  2.70it/s]Evaluating:  22%|██▏       | 164/748 [01:00<03:35,  2.71it/s]Evaluating:  22%|██▏       | 165/748 [01:01<03:34,  2.71it/s]Evaluating:  22%|██▏       | 166/748 [01:01<03:34,  2.71it/s]Evaluating:  22%|██▏       | 167/748 [01:01<03:34,  2.71it/s]Evaluating:  22%|██▏       | 168/748 [01:02<03:34,  2.70it/s]Evaluating:  23%|██▎       | 169/748 [01:02<03:33,  2.71it/s]Evaluating:  23%|██▎       | 170/748 [01:03<03:32,  2.72it/s]Evaluating:  23%|██▎       | 171/748 [01:03<03:31,  2.73it/s]Evaluating:  23%|██▎       | 172/748 [01:03<03:30,  2.74it/s]Evaluating:  23%|██▎       | 173/748 [01:04<03:30,  2.74it/s]Evaluating:  23%|██▎       | 174/748 [01:04<03:30,  2.73it/s]Evaluating:  23%|██▎       | 175/748 [01:04<03:30,  2.72it/s]Evaluating:  24%|██▎       | 176/748 [01:05<03:29,  2.73it/s]Evaluating:  24%|██▎       | 177/748 [01:05<03:29,  2.72it/s]Evaluating:  24%|██▍       | 178/748 [01:05<03:30,  2.71it/s]Evaluating:  24%|██▍       | 179/748 [01:06<03:30,  2.71it/s]Evaluating:  24%|██▍       | 180/748 [01:06<03:29,  2.70it/s]Evaluating:  24%|██▍       | 181/748 [01:07<03:29,  2.71it/s]Evaluating:  24%|██▍       | 182/748 [01:07<03:29,  2.70it/s]Evaluating:  24%|██▍       | 183/748 [01:07<03:29,  2.69it/s]Evaluating:  25%|██▍       | 184/748 [01:08<03:29,  2.70it/s]Evaluating:  25%|██▍       | 185/748 [01:08<03:28,  2.70it/s]Evaluating:  25%|██▍       | 186/748 [01:08<03:27,  2.70it/s]Evaluating:  25%|██▌       | 187/748 [01:09<03:26,  2.71it/s]Evaluating:  25%|██▌       | 188/748 [01:09<03:26,  2.71it/s]Evaluating:  25%|██▌       | 189/748 [01:10<03:26,  2.71it/s]Evaluating:  25%|██▌       | 190/748 [01:10<03:25,  2.71it/s]Evaluating:  26%|██▌       | 191/748 [01:10<03:25,  2.71it/s]Evaluating:  26%|██▌       | 192/748 [01:11<03:25,  2.71it/s]Evaluating:  26%|██▌       | 193/748 [01:11<03:24,  2.71it/s]Evaluating:  26%|██▌       | 194/748 [01:11<03:23,  2.72it/s]Evaluating:  26%|██▌       | 195/748 [01:12<03:22,  2.72it/s]Evaluating:  26%|██▌       | 196/748 [01:12<03:22,  2.72it/s]Evaluating:  26%|██▋       | 197/748 [01:12<03:23,  2.71it/s]Evaluating:  26%|██▋       | 198/748 [01:13<03:22,  2.71it/s]Evaluating:  27%|██▋       | 199/748 [01:13<03:22,  2.72it/s]Evaluating:  27%|██▋       | 200/748 [01:14<03:21,  2.72it/s]Evaluating:  27%|██▋       | 201/748 [01:14<03:21,  2.72it/s]Evaluating:  27%|██▋       | 202/748 [01:14<03:20,  2.72it/s]Evaluating:  27%|██▋       | 203/748 [01:15<03:20,  2.72it/s]Evaluating:  27%|██▋       | 204/748 [01:15<03:19,  2.73it/s]Evaluating:  27%|██▋       | 205/748 [01:15<03:19,  2.72it/s]Evaluating:  28%|██▊       | 206/748 [01:16<03:19,  2.72it/s]Evaluating:  28%|██▊       | 207/748 [01:16<03:18,  2.73it/s]Evaluating:  28%|██▊       | 208/748 [01:17<03:18,  2.73it/s]Evaluating:  28%|██▊       | 209/748 [01:17<03:18,  2.71it/s]Evaluating:  28%|██▊       | 210/748 [01:17<03:18,  2.71it/s]Evaluating:  28%|██▊       | 211/748 [01:18<03:19,  2.69it/s]Evaluating:  28%|██▊       | 212/748 [01:18<03:18,  2.70it/s]Evaluating:  28%|██▊       | 213/748 [01:18<03:16,  2.72it/s]Evaluating:  29%|██▊       | 214/748 [01:19<03:16,  2.72it/s]Evaluating:  29%|██▊       | 215/748 [01:19<03:15,  2.72it/s]Evaluating:  29%|██▉       | 216/748 [01:19<03:15,  2.71it/s]Evaluating:  29%|██▉       | 217/748 [01:20<03:16,  2.71it/s]Evaluating:  29%|██▉       | 218/748 [01:20<03:15,  2.71it/s]Evaluating:  29%|██▉       | 219/748 [01:21<03:15,  2.70it/s]Evaluating:  29%|██▉       | 220/748 [01:21<03:19,  2.65it/s]Evaluating:  30%|██▉       | 221/748 [01:21<03:16,  2.68it/s]Evaluating:  30%|██▉       | 222/748 [01:22<03:14,  2.70it/s]Evaluating:  30%|██▉       | 223/748 [01:22<03:13,  2.71it/s]Evaluating:  30%|██▉       | 224/748 [01:22<03:13,  2.71it/s]Evaluating:  30%|███       | 225/748 [01:23<03:12,  2.71it/s]Evaluating:  30%|███       | 226/748 [01:23<03:12,  2.72it/s]Evaluating:  30%|███       | 227/748 [01:24<03:11,  2.72it/s]Evaluating:  30%|███       | 228/748 [01:24<03:11,  2.72it/s]Evaluating:  31%|███       | 229/748 [01:24<03:10,  2.72it/s]Evaluating:  31%|███       | 230/748 [01:25<03:10,  2.72it/s]Evaluating:  31%|███       | 231/748 [01:25<03:10,  2.71it/s]Evaluating:  31%|███       | 232/748 [01:25<03:10,  2.71it/s]Evaluating:  31%|███       | 233/748 [01:26<03:09,  2.72it/s]Evaluating:  31%|███▏      | 234/748 [01:26<03:08,  2.72it/s]Evaluating:  31%|███▏      | 235/748 [01:26<03:08,  2.72it/s]Evaluating:  32%|███▏      | 236/748 [01:27<03:08,  2.72it/s]Evaluating:  32%|███▏      | 237/748 [01:27<03:08,  2.71it/s]Evaluating:  32%|███▏      | 238/748 [01:28<03:07,  2.72it/s]Evaluating:  32%|███▏      | 239/748 [01:28<03:06,  2.72it/s]Evaluating:  32%|███▏      | 240/748 [01:28<03:06,  2.72it/s]Evaluating:  32%|███▏      | 241/748 [01:29<03:06,  2.73it/s]Evaluating:  32%|███▏      | 242/748 [01:29<03:06,  2.72it/s]Evaluating:  32%|███▏      | 243/748 [01:29<03:04,  2.73it/s]Evaluating:  33%|███▎      | 244/748 [01:30<03:04,  2.74it/s]Evaluating:  33%|███▎      | 245/748 [01:30<03:04,  2.73it/s]Evaluating:  33%|███▎      | 246/748 [01:31<03:04,  2.72it/s]Evaluating:  33%|███▎      | 247/748 [01:31<03:04,  2.72it/s]Evaluating:  33%|███▎      | 248/748 [01:31<03:03,  2.73it/s]Evaluating:  33%|███▎      | 249/748 [01:32<03:02,  2.73it/s]Evaluating:  33%|███▎      | 250/748 [01:32<03:02,  2.73it/s]Evaluating:  34%|███▎      | 251/748 [01:32<03:02,  2.72it/s]Evaluating:  34%|███▎      | 252/748 [01:33<03:02,  2.71it/s]Evaluating:  34%|███▍      | 253/748 [01:33<03:02,  2.71it/s]Evaluating:  34%|███▍      | 254/748 [01:33<03:01,  2.72it/s]Evaluating:  34%|███▍      | 255/748 [01:34<03:01,  2.72it/s]Evaluating:  34%|███▍      | 256/748 [01:34<03:00,  2.72it/s]Evaluating:  34%|███▍      | 257/748 [01:35<03:00,  2.72it/s]Evaluating:  34%|███▍      | 258/748 [01:35<02:59,  2.73it/s]Evaluating:  35%|███▍      | 259/748 [01:35<02:59,  2.73it/s]Evaluating:  35%|███▍      | 260/748 [01:36<02:58,  2.74it/s]Evaluating:  35%|███▍      | 261/748 [01:36<02:57,  2.74it/s]Evaluating:  35%|███▌      | 262/748 [01:36<02:57,  2.74it/s]Evaluating:  35%|███▌      | 263/748 [01:37<02:57,  2.73it/s]Evaluating:  35%|███▌      | 264/748 [01:37<02:58,  2.72it/s]Evaluating:  35%|███▌      | 265/748 [01:37<02:57,  2.72it/s]Evaluating:  36%|███▌      | 266/748 [01:38<02:57,  2.71it/s]Evaluating:  36%|███▌      | 267/748 [01:38<02:57,  2.71it/s]Evaluating:  36%|███▌      | 268/748 [01:39<02:56,  2.72it/s]Evaluating:  36%|███▌      | 269/748 [01:39<02:56,  2.72it/s]Evaluating:  36%|███▌      | 270/748 [01:39<02:57,  2.69it/s]Evaluating:  36%|███▌      | 271/748 [01:40<02:56,  2.70it/s]Evaluating:  36%|███▋      | 272/748 [01:40<02:56,  2.70it/s]Evaluating:  36%|███▋      | 273/748 [01:40<02:56,  2.69it/s]Evaluating:  37%|███▋      | 274/748 [01:41<02:56,  2.69it/s]Evaluating:  37%|███▋      | 275/748 [01:41<02:56,  2.69it/s]Evaluating:  37%|███▋      | 276/748 [01:42<02:54,  2.70it/s]Evaluating:  37%|███▋      | 277/748 [01:42<02:54,  2.70it/s]Evaluating:  37%|███▋      | 278/748 [01:42<02:53,  2.71it/s]Evaluating:  37%|███▋      | 279/748 [01:43<02:52,  2.71it/s]Evaluating:  37%|███▋      | 280/748 [01:43<02:51,  2.73it/s]Evaluating:  38%|███▊      | 281/748 [01:43<02:51,  2.73it/s]Evaluating:  38%|███▊      | 282/748 [01:44<02:50,  2.73it/s]Evaluating:  38%|███▊      | 283/748 [01:44<02:50,  2.72it/s]Evaluating:  38%|███▊      | 284/748 [01:45<02:50,  2.72it/s]Evaluating:  38%|███▊      | 285/748 [01:45<02:49,  2.73it/s]Evaluating:  38%|███▊      | 286/748 [01:45<02:49,  2.73it/s]Evaluating:  38%|███▊      | 287/748 [01:46<02:49,  2.73it/s]Evaluating:  39%|███▊      | 288/748 [01:46<02:49,  2.72it/s]Evaluating:  39%|███▊      | 289/748 [01:46<02:48,  2.73it/s]Evaluating:  39%|███▉      | 290/748 [01:47<02:47,  2.73it/s]Evaluating:  39%|███▉      | 291/748 [01:47<02:46,  2.74it/s]Evaluating:  39%|███▉      | 292/748 [01:47<02:46,  2.73it/s]Evaluating:  39%|███▉      | 293/748 [01:48<02:46,  2.73it/s]Evaluating:  39%|███▉      | 294/748 [01:48<02:46,  2.72it/s]Evaluating:  39%|███▉      | 295/748 [01:49<02:45,  2.73it/s]Evaluating:  40%|███▉      | 296/748 [01:49<02:46,  2.71it/s]Evaluating:  40%|███▉      | 297/748 [01:49<02:45,  2.72it/s]Evaluating:  40%|███▉      | 298/748 [01:50<02:45,  2.72it/s]Evaluating:  40%|███▉      | 299/748 [01:50<02:44,  2.73it/s]Evaluating:  40%|████      | 300/748 [01:50<02:43,  2.74it/s]Evaluating:  40%|████      | 301/748 [01:51<02:43,  2.73it/s]Evaluating:  40%|████      | 302/748 [01:51<02:42,  2.74it/s]Evaluating:  41%|████      | 303/748 [01:51<02:42,  2.73it/s]Evaluating:  41%|████      | 304/748 [01:52<02:42,  2.73it/s]Evaluating:  41%|████      | 305/748 [01:52<02:42,  2.73it/s]Evaluating:  41%|████      | 306/748 [01:53<02:42,  2.73it/s]Evaluating:  41%|████      | 307/748 [01:53<02:41,  2.73it/s]Evaluating:  41%|████      | 308/748 [01:53<02:40,  2.73it/s]Evaluating:  41%|████▏     | 309/748 [01:54<02:40,  2.74it/s]Evaluating:  41%|████▏     | 310/748 [01:54<02:39,  2.74it/s]Evaluating:  42%|████▏     | 311/748 [01:54<02:40,  2.73it/s]Evaluating:  42%|████▏     | 312/748 [01:55<02:40,  2.72it/s]Evaluating:  42%|████▏     | 313/748 [01:55<02:40,  2.72it/s]Evaluating:  42%|████▏     | 314/748 [01:55<02:39,  2.72it/s]Evaluating:  42%|████▏     | 315/748 [01:56<02:38,  2.73it/s]Evaluating:  42%|████▏     | 316/748 [01:56<02:38,  2.73it/s]Evaluating:  42%|████▏     | 317/748 [01:57<02:38,  2.72it/s]Evaluating:  43%|████▎     | 318/748 [01:57<02:37,  2.73it/s]Evaluating:  43%|████▎     | 319/748 [01:57<02:37,  2.73it/s]Evaluating:  43%|████▎     | 320/748 [01:58<02:36,  2.74it/s]Evaluating:  43%|████▎     | 321/748 [01:58<02:35,  2.74it/s]Evaluating:  43%|████▎     | 322/748 [01:58<02:35,  2.75it/s]Evaluating:  43%|████▎     | 323/748 [01:59<02:35,  2.74it/s]Evaluating:  43%|████▎     | 324/748 [01:59<02:35,  2.73it/s]Evaluating:  43%|████▎     | 325/748 [02:00<02:35,  2.72it/s]Evaluating:  44%|████▎     | 326/748 [02:00<02:35,  2.72it/s]Evaluating:  44%|████▎     | 327/748 [02:00<02:34,  2.72it/s]Evaluating:  44%|████▍     | 328/748 [02:01<02:34,  2.72it/s]Evaluating:  44%|████▍     | 329/748 [02:01<02:34,  2.72it/s]Evaluating:  44%|████▍     | 330/748 [02:01<02:34,  2.71it/s]Evaluating:  44%|████▍     | 331/748 [02:02<02:33,  2.71it/s]Evaluating:  44%|████▍     | 332/748 [02:02<02:33,  2.71it/s]Evaluating:  45%|████▍     | 333/748 [02:02<02:33,  2.71it/s]Evaluating:  45%|████▍     | 334/748 [02:03<02:32,  2.72it/s]Evaluating:  45%|████▍     | 335/748 [02:03<02:31,  2.72it/s]Evaluating:  45%|████▍     | 336/748 [02:04<02:31,  2.72it/s]Evaluating:  45%|████▌     | 337/748 [02:04<02:31,  2.71it/s]Evaluating:  45%|████▌     | 338/748 [02:04<02:31,  2.70it/s]Evaluating:  45%|████▌     | 339/748 [02:05<02:31,  2.71it/s]Evaluating:  45%|████▌     | 340/748 [02:05<02:30,  2.70it/s]Evaluating:  46%|████▌     | 341/748 [02:05<02:30,  2.71it/s]Evaluating:  46%|████▌     | 342/748 [02:06<02:29,  2.72it/s]Evaluating:  46%|████▌     | 343/748 [02:06<02:28,  2.73it/s]Evaluating:  46%|████▌     | 344/748 [02:07<02:28,  2.73it/s]Evaluating:  46%|████▌     | 345/748 [02:07<02:27,  2.73it/s]Evaluating:  46%|████▋     | 346/748 [02:07<02:26,  2.74it/s]Evaluating:  46%|████▋     | 347/748 [02:08<02:26,  2.74it/s]Evaluating:  47%|████▋     | 348/748 [02:08<02:26,  2.74it/s]Evaluating:  47%|████▋     | 349/748 [02:08<02:25,  2.74it/s]Evaluating:  47%|████▋     | 350/748 [02:09<02:25,  2.74it/s]Evaluating:  47%|████▋     | 351/748 [02:09<02:25,  2.74it/s]Evaluating:  47%|████▋     | 352/748 [02:09<02:25,  2.72it/s]Evaluating:  47%|████▋     | 353/748 [02:10<02:25,  2.72it/s]Evaluating:  47%|████▋     | 354/748 [02:10<02:24,  2.72it/s]Evaluating:  47%|████▋     | 355/748 [02:11<02:24,  2.73it/s]Evaluating:  48%|████▊     | 356/748 [02:11<02:23,  2.74it/s]Evaluating:  48%|████▊     | 357/748 [02:11<02:23,  2.73it/s]Evaluating:  48%|████▊     | 358/748 [02:12<02:22,  2.73it/s]Evaluating:  48%|████▊     | 359/748 [02:12<02:22,  2.73it/s]Evaluating:  48%|████▊     | 360/748 [02:12<02:22,  2.73it/s]Evaluating:  48%|████▊     | 361/748 [02:13<02:21,  2.73it/s]Evaluating:  48%|████▊     | 362/748 [02:13<02:21,  2.73it/s]Evaluating:  49%|████▊     | 363/748 [02:13<02:21,  2.73it/s]Evaluating:  49%|████▊     | 364/748 [02:14<02:20,  2.73it/s]Evaluating:  49%|████▉     | 365/748 [02:14<02:20,  2.74it/s]Evaluating:  49%|████▉     | 366/748 [02:15<02:20,  2.72it/s]Evaluating:  49%|████▉     | 367/748 [02:15<02:19,  2.73it/s]Evaluating:  49%|████▉     | 368/748 [02:15<02:18,  2.74it/s]Evaluating:  49%|████▉     | 369/748 [02:16<02:18,  2.74it/s]Evaluating:  49%|████▉     | 370/748 [02:16<02:17,  2.74it/s]Evaluating:  50%|████▉     | 371/748 [02:16<02:17,  2.75it/s]Evaluating:  50%|████▉     | 372/748 [02:17<02:17,  2.74it/s]Evaluating:  50%|████▉     | 373/748 [02:17<02:16,  2.74it/s]Evaluating:  50%|█████     | 374/748 [02:17<02:16,  2.74it/s]Evaluating:  50%|█████     | 375/748 [02:18<02:16,  2.74it/s]Evaluating:  50%|█████     | 376/748 [02:18<02:15,  2.74it/s]Evaluating:  50%|█████     | 377/748 [02:19<02:15,  2.73it/s]Evaluating:  51%|█████     | 378/748 [02:19<02:15,  2.73it/s]Evaluating:  51%|█████     | 379/748 [02:19<02:15,  2.73it/s]Evaluating:  51%|█████     | 380/748 [02:20<02:14,  2.74it/s]Evaluating:  51%|█████     | 381/748 [02:20<02:14,  2.73it/s]Evaluating:  51%|█████     | 382/748 [02:20<02:14,  2.72it/s]Evaluating:  51%|█████     | 383/748 [02:21<02:14,  2.72it/s]Evaluating:  51%|█████▏    | 384/748 [02:21<02:14,  2.70it/s]Evaluating:  51%|█████▏    | 385/748 [02:22<02:14,  2.70it/s]Evaluating:  52%|█████▏    | 386/748 [02:22<02:14,  2.70it/s]Evaluating:  52%|█████▏    | 387/748 [02:22<02:13,  2.70it/s]Evaluating:  52%|█████▏    | 388/748 [02:23<02:13,  2.69it/s]Evaluating:  52%|█████▏    | 389/748 [02:23<02:13,  2.70it/s]Evaluating:  52%|█████▏    | 390/748 [02:23<02:12,  2.70it/s]Evaluating:  52%|█████▏    | 391/748 [02:24<02:12,  2.69it/s]Evaluating:  52%|█████▏    | 392/748 [02:24<02:12,  2.69it/s]Evaluating:  53%|█████▎    | 393/748 [02:25<02:11,  2.69it/s]Evaluating:  53%|█████▎    | 394/748 [02:25<02:11,  2.70it/s]Evaluating:  53%|█████▎    | 395/748 [02:25<02:11,  2.69it/s]Evaluating:  53%|█████▎    | 396/748 [02:26<02:10,  2.69it/s]Evaluating:  53%|█████▎    | 397/748 [02:26<02:10,  2.69it/s]Evaluating:  53%|█████▎    | 398/748 [02:26<02:09,  2.70it/s]Evaluating:  53%|█████▎    | 399/748 [02:27<02:09,  2.70it/s]Evaluating:  53%|█████▎    | 400/748 [02:27<02:08,  2.71it/s]Evaluating:  54%|█████▎    | 401/748 [02:27<02:08,  2.70it/s]Evaluating:  54%|█████▎    | 402/748 [02:28<02:07,  2.71it/s]Evaluating:  54%|█████▍    | 403/748 [02:28<02:07,  2.72it/s]Evaluating:  54%|█████▍    | 404/748 [02:29<02:06,  2.73it/s]Evaluating:  54%|█████▍    | 405/748 [02:29<02:06,  2.72it/s]Evaluating:  54%|█████▍    | 406/748 [02:29<02:06,  2.71it/s]Evaluating:  54%|█████▍    | 407/748 [02:30<02:05,  2.72it/s]Evaluating:  55%|█████▍    | 408/748 [02:30<02:04,  2.72it/s]Evaluating:  55%|█████▍    | 409/748 [02:30<02:04,  2.72it/s]Evaluating:  55%|█████▍    | 410/748 [02:31<02:03,  2.73it/s]Evaluating:  55%|█████▍    | 411/748 [02:31<02:03,  2.73it/s]Evaluating:  55%|█████▌    | 412/748 [02:31<02:03,  2.73it/s]Evaluating:  55%|█████▌    | 413/748 [02:32<02:02,  2.73it/s]Evaluating:  55%|█████▌    | 414/748 [02:32<02:02,  2.72it/s]Evaluating:  55%|█████▌    | 415/748 [02:33<02:01,  2.73it/s]Evaluating:  56%|█████▌    | 416/748 [02:33<02:01,  2.74it/s]Evaluating:  56%|█████▌    | 417/748 [02:33<02:00,  2.74it/s]Evaluating:  56%|█████▌    | 418/748 [02:34<02:00,  2.73it/s]Evaluating:  56%|█████▌    | 419/748 [02:34<02:00,  2.73it/s]Evaluating:  56%|█████▌    | 420/748 [02:34<02:00,  2.72it/s]Evaluating:  56%|█████▋    | 421/748 [02:35<01:59,  2.74it/s]Evaluating:  56%|█████▋    | 422/748 [02:35<01:59,  2.73it/s]Evaluating:  57%|█████▋    | 423/748 [02:36<01:58,  2.73it/s]Evaluating:  57%|█████▋    | 424/748 [02:36<01:58,  2.73it/s]Evaluating:  57%|█████▋    | 425/748 [02:36<01:58,  2.72it/s]Evaluating:  57%|█████▋    | 426/748 [02:37<01:58,  2.72it/s]Evaluating:  57%|█████▋    | 427/748 [02:37<01:57,  2.72it/s]Evaluating:  57%|█████▋    | 428/748 [02:37<01:57,  2.72it/s]Evaluating:  57%|█████▋    | 429/748 [02:38<01:57,  2.72it/s]Evaluating:  57%|█████▋    | 430/748 [02:38<01:56,  2.72it/s]Evaluating:  58%|█████▊    | 431/748 [02:38<01:56,  2.72it/s]Evaluating:  58%|█████▊    | 432/748 [02:39<01:56,  2.72it/s]Evaluating:  58%|█████▊    | 433/748 [02:39<01:55,  2.73it/s]Evaluating:  58%|█████▊    | 434/748 [02:40<01:55,  2.73it/s]Evaluating:  58%|█████▊    | 435/748 [02:40<01:58,  2.65it/s]Evaluating:  58%|█████▊    | 436/748 [02:40<01:56,  2.67it/s]Evaluating:  58%|█████▊    | 437/748 [02:41<01:55,  2.68it/s]Evaluating:  59%|█████▊    | 438/748 [02:41<01:55,  2.69it/s]Evaluating:  59%|█████▊    | 439/748 [02:41<01:54,  2.69it/s]Evaluating:  59%|█████▉    | 440/748 [02:42<01:54,  2.70it/s]Evaluating:  59%|█████▉    | 441/748 [02:42<01:53,  2.72it/s]Evaluating:  59%|█████▉    | 442/748 [02:43<01:52,  2.71it/s]Evaluating:  59%|█████▉    | 443/748 [02:43<01:52,  2.72it/s]Evaluating:  59%|█████▉    | 444/748 [02:43<01:51,  2.73it/s]Evaluating:  59%|█████▉    | 445/748 [02:44<01:50,  2.73it/s]Evaluating:  60%|█████▉    | 446/748 [02:44<01:50,  2.74it/s]Evaluating:  60%|█████▉    | 447/748 [02:44<01:50,  2.73it/s]Evaluating:  60%|█████▉    | 448/748 [02:45<01:49,  2.73it/s]Evaluating:  60%|██████    | 449/748 [02:45<01:49,  2.73it/s]Evaluating:  60%|██████    | 450/748 [02:45<01:48,  2.74it/s]Evaluating:  60%|██████    | 451/748 [02:46<01:48,  2.75it/s]Evaluating:  60%|██████    | 452/748 [02:46<01:47,  2.75it/s]Evaluating:  61%|██████    | 453/748 [02:47<01:47,  2.75it/s]Evaluating:  61%|██████    | 454/748 [02:47<01:46,  2.76it/s]Evaluating:  61%|██████    | 455/748 [02:47<01:46,  2.75it/s]Evaluating:  61%|██████    | 456/748 [02:48<01:46,  2.74it/s]Evaluating:  61%|██████    | 457/748 [02:48<01:46,  2.75it/s]Evaluating:  61%|██████    | 458/748 [02:48<01:46,  2.73it/s]Evaluating:  61%|██████▏   | 459/748 [02:49<01:46,  2.73it/s]Evaluating:  61%|██████▏   | 460/748 [02:49<01:45,  2.72it/s]Evaluating:  62%|██████▏   | 461/748 [02:49<01:45,  2.73it/s]Evaluating:  62%|██████▏   | 462/748 [02:50<01:44,  2.73it/s]Evaluating:  62%|██████▏   | 463/748 [02:50<01:44,  2.73it/s]Evaluating:  62%|██████▏   | 464/748 [02:51<01:44,  2.73it/s]Evaluating:  62%|██████▏   | 465/748 [02:51<01:43,  2.73it/s]Evaluating:  62%|██████▏   | 466/748 [02:51<01:43,  2.73it/s]Evaluating:  62%|██████▏   | 467/748 [02:52<01:42,  2.73it/s]Evaluating:  63%|██████▎   | 468/748 [02:52<01:42,  2.74it/s]Evaluating:  63%|██████▎   | 469/748 [02:52<01:41,  2.74it/s]Evaluating:  63%|██████▎   | 470/748 [02:53<01:41,  2.74it/s]Evaluating:  63%|██████▎   | 471/748 [02:53<01:41,  2.74it/s]Evaluating:  63%|██████▎   | 472/748 [02:54<01:41,  2.72it/s]Evaluating:  63%|██████▎   | 473/748 [02:54<01:41,  2.71it/s]Evaluating:  63%|██████▎   | 474/748 [02:54<01:41,  2.70it/s]Evaluating:  64%|██████▎   | 475/748 [02:55<01:40,  2.70it/s]Evaluating:  64%|██████▎   | 476/748 [02:55<01:40,  2.71it/s]Evaluating:  64%|██████▍   | 477/748 [02:55<01:39,  2.73it/s]Evaluating:  64%|██████▍   | 478/748 [02:56<01:38,  2.73it/s]Evaluating:  64%|██████▍   | 479/748 [02:56<01:38,  2.73it/s]Evaluating:  64%|██████▍   | 480/748 [02:56<01:38,  2.73it/s]Evaluating:  64%|██████▍   | 481/748 [02:57<01:37,  2.73it/s]Evaluating:  64%|██████▍   | 482/748 [02:57<01:37,  2.72it/s]Evaluating:  65%|██████▍   | 483/748 [02:58<01:37,  2.72it/s]Evaluating:  65%|██████▍   | 484/748 [02:58<01:37,  2.72it/s]Evaluating:  65%|██████▍   | 485/748 [02:58<01:36,  2.71it/s]Evaluating:  65%|██████▍   | 486/748 [02:59<01:36,  2.72it/s]Evaluating:  65%|██████▌   | 487/748 [02:59<01:35,  2.73it/s]Evaluating:  65%|██████▌   | 488/748 [02:59<01:35,  2.73it/s]Evaluating:  65%|██████▌   | 489/748 [03:00<01:34,  2.73it/s]Evaluating:  66%|██████▌   | 490/748 [03:00<01:34,  2.73it/s]Evaluating:  66%|██████▌   | 491/748 [03:00<01:34,  2.73it/s]Evaluating:  66%|██████▌   | 492/748 [03:01<01:33,  2.73it/s]Evaluating:  66%|██████▌   | 493/748 [03:01<01:33,  2.73it/s]Evaluating:  66%|██████▌   | 494/748 [03:02<01:33,  2.73it/s]Evaluating:  66%|██████▌   | 495/748 [03:02<01:33,  2.71it/s]Evaluating:  66%|██████▋   | 496/748 [03:02<01:32,  2.72it/s]Evaluating:  66%|██████▋   | 497/748 [03:03<01:32,  2.73it/s]Evaluating:  67%|██████▋   | 498/748 [03:03<01:31,  2.73it/s]Evaluating:  67%|██████▋   | 499/748 [03:03<01:31,  2.72it/s]Evaluating:  67%|██████▋   | 500/748 [03:04<01:31,  2.72it/s]Evaluating:  67%|██████▋   | 501/748 [03:04<01:31,  2.71it/s]Evaluating:  67%|██████▋   | 502/748 [03:05<01:30,  2.72it/s]Evaluating:  67%|██████▋   | 503/748 [03:05<01:29,  2.73it/s]Evaluating:  67%|██████▋   | 504/748 [03:05<01:29,  2.73it/s]Evaluating:  68%|██████▊   | 505/748 [03:06<01:29,  2.72it/s]Evaluating:  68%|██████▊   | 506/748 [03:06<01:28,  2.73it/s]Evaluating:  68%|██████▊   | 507/748 [03:06<01:28,  2.73it/s]Evaluating:  68%|██████▊   | 508/748 [03:07<01:27,  2.73it/s]Evaluating:  68%|██████▊   | 509/748 [03:07<01:27,  2.72it/s]Evaluating:  68%|██████▊   | 510/748 [03:07<01:27,  2.72it/s]Evaluating:  68%|██████▊   | 511/748 [03:08<01:26,  2.73it/s]Evaluating:  68%|██████▊   | 512/748 [03:08<01:26,  2.72it/s]Evaluating:  69%|██████▊   | 513/748 [03:09<01:26,  2.72it/s]Evaluating:  69%|██████▊   | 514/748 [03:09<01:25,  2.73it/s]Evaluating:  69%|██████▉   | 515/748 [03:09<01:25,  2.73it/s]Evaluating:  69%|██████▉   | 516/748 [03:10<01:24,  2.74it/s]Evaluating:  69%|██████▉   | 517/748 [03:10<01:24,  2.74it/s]Evaluating:  69%|██████▉   | 518/748 [03:10<01:23,  2.74it/s]Evaluating:  69%|██████▉   | 519/748 [03:11<01:23,  2.74it/s]Evaluating:  70%|██████▉   | 520/748 [03:11<01:23,  2.74it/s]Evaluating:  70%|██████▉   | 521/748 [03:11<01:23,  2.73it/s]Evaluating:  70%|██████▉   | 522/748 [03:12<01:22,  2.74it/s]Evaluating:  70%|██████▉   | 523/748 [03:12<01:22,  2.74it/s]Evaluating:  70%|███████   | 524/748 [03:13<01:21,  2.73it/s]Evaluating:  70%|███████   | 525/748 [03:13<01:21,  2.74it/s]Evaluating:  70%|███████   | 526/748 [03:13<01:21,  2.74it/s]Evaluating:  70%|███████   | 527/748 [03:14<01:21,  2.73it/s]Evaluating:  71%|███████   | 528/748 [03:14<01:21,  2.71it/s]Evaluating:  71%|███████   | 529/748 [03:14<01:20,  2.71it/s]Evaluating:  71%|███████   | 530/748 [03:15<01:20,  2.71it/s]Evaluating:  71%|███████   | 531/748 [03:15<01:19,  2.72it/s]Evaluating:  71%|███████   | 532/748 [03:16<01:19,  2.72it/s]Evaluating:  71%|███████▏  | 533/748 [03:16<01:19,  2.72it/s]Evaluating:  71%|███████▏  | 534/748 [03:16<01:18,  2.72it/s]Evaluating:  72%|███████▏  | 535/748 [03:17<01:18,  2.71it/s]Evaluating:  72%|███████▏  | 536/748 [03:17<01:18,  2.71it/s]Evaluating:  72%|███████▏  | 537/748 [03:17<01:17,  2.72it/s]Evaluating:  72%|███████▏  | 538/748 [03:18<01:17,  2.71it/s]Evaluating:  72%|███████▏  | 539/748 [03:18<01:17,  2.71it/s]Evaluating:  72%|███████▏  | 540/748 [03:18<01:16,  2.72it/s]Evaluating:  72%|███████▏  | 541/748 [03:19<01:16,  2.72it/s]Evaluating:  72%|███████▏  | 542/748 [03:19<01:15,  2.71it/s]Evaluating:  73%|███████▎  | 543/748 [03:20<01:15,  2.71it/s]Evaluating:  73%|███████▎  | 544/748 [03:20<01:15,  2.72it/s]Evaluating:  73%|███████▎  | 545/748 [03:20<01:14,  2.71it/s]Evaluating:  73%|███████▎  | 546/748 [03:21<01:14,  2.71it/s]Evaluating:  73%|███████▎  | 547/748 [03:21<01:14,  2.71it/s]Evaluating:  73%|███████▎  | 548/748 [03:21<01:13,  2.73it/s]Evaluating:  73%|███████▎  | 549/748 [03:22<01:12,  2.73it/s]Evaluating:  74%|███████▎  | 550/748 [03:22<01:12,  2.73it/s]Evaluating:  74%|███████▎  | 551/748 [03:23<01:12,  2.72it/s]Evaluating:  74%|███████▍  | 552/748 [03:23<01:12,  2.71it/s]Evaluating:  74%|███████▍  | 553/748 [03:23<01:11,  2.72it/s]Evaluating:  74%|███████▍  | 554/748 [03:24<01:11,  2.72it/s]Evaluating:  74%|███████▍  | 555/748 [03:24<01:10,  2.72it/s]Evaluating:  74%|███████▍  | 556/748 [03:24<01:10,  2.73it/s]Evaluating:  74%|███████▍  | 557/748 [03:25<01:09,  2.73it/s]Evaluating:  75%|███████▍  | 558/748 [03:25<01:09,  2.73it/s]Evaluating:  75%|███████▍  | 559/748 [03:25<01:09,  2.73it/s]Evaluating:  75%|███████▍  | 560/748 [03:26<01:09,  2.72it/s]Evaluating:  75%|███████▌  | 561/748 [03:26<01:08,  2.72it/s]Evaluating:  75%|███████▌  | 562/748 [03:27<01:08,  2.73it/s]Evaluating:  75%|███████▌  | 563/748 [03:27<01:08,  2.72it/s]Evaluating:  75%|███████▌  | 564/748 [03:27<01:07,  2.72it/s]Evaluating:  76%|███████▌  | 565/748 [03:28<01:07,  2.72it/s]Evaluating:  76%|███████▌  | 566/748 [03:28<01:06,  2.72it/s]Evaluating:  76%|███████▌  | 567/748 [03:28<01:06,  2.72it/s]Evaluating:  76%|███████▌  | 568/748 [03:29<01:06,  2.73it/s]Evaluating:  76%|███████▌  | 569/748 [03:29<01:05,  2.73it/s]Evaluating:  76%|███████▌  | 570/748 [03:30<01:05,  2.72it/s]Evaluating:  76%|███████▋  | 571/748 [03:30<01:04,  2.73it/s]Evaluating:  76%|███████▋  | 572/748 [03:30<01:04,  2.71it/s]Evaluating:  77%|███████▋  | 573/748 [03:31<01:04,  2.71it/s]Evaluating:  77%|███████▋  | 574/748 [03:31<01:04,  2.72it/s]Evaluating:  77%|███████▋  | 575/748 [03:31<01:03,  2.71it/s]Evaluating:  77%|███████▋  | 576/748 [03:32<01:03,  2.72it/s]Evaluating:  77%|███████▋  | 577/748 [03:32<01:02,  2.73it/s]Evaluating:  77%|███████▋  | 578/748 [03:32<01:02,  2.73it/s]Evaluating:  77%|███████▋  | 579/748 [03:33<01:02,  2.73it/s]Evaluating:  78%|███████▊  | 580/748 [03:33<01:01,  2.74it/s]Evaluating:  78%|███████▊  | 581/748 [03:34<01:01,  2.73it/s]Evaluating:  78%|███████▊  | 582/748 [03:34<01:00,  2.73it/s]Evaluating:  78%|███████▊  | 583/748 [03:34<01:00,  2.73it/s]Evaluating:  78%|███████▊  | 584/748 [03:35<00:59,  2.74it/s]Evaluating:  78%|███████▊  | 585/748 [03:35<00:59,  2.74it/s]Evaluating:  78%|███████▊  | 586/748 [03:35<00:59,  2.73it/s]Evaluating:  78%|███████▊  | 587/748 [03:36<00:59,  2.71it/s]Evaluating:  79%|███████▊  | 588/748 [03:36<00:58,  2.71it/s]Evaluating:  79%|███████▊  | 589/748 [03:36<00:58,  2.72it/s]Evaluating:  79%|███████▉  | 590/748 [03:37<00:58,  2.72it/s]Evaluating:  79%|███████▉  | 591/748 [03:37<00:58,  2.71it/s]Evaluating:  79%|███████▉  | 592/748 [03:38<00:57,  2.70it/s]Evaluating:  79%|███████▉  | 593/748 [03:38<00:57,  2.69it/s]Evaluating:  79%|███████▉  | 594/748 [03:38<00:57,  2.69it/s]Evaluating:  80%|███████▉  | 595/748 [03:39<00:57,  2.68it/s]Evaluating:  80%|███████▉  | 596/748 [03:39<00:56,  2.68it/s]Evaluating:  80%|███████▉  | 597/748 [03:39<00:56,  2.69it/s]Evaluating:  80%|███████▉  | 598/748 [03:40<00:55,  2.70it/s]Evaluating:  80%|████████  | 599/748 [03:40<00:54,  2.71it/s]Evaluating:  80%|████████  | 600/748 [03:41<00:54,  2.71it/s]Evaluating:  80%|████████  | 601/748 [03:41<00:54,  2.72it/s]Evaluating:  80%|████████  | 602/748 [03:41<00:53,  2.72it/s]Evaluating:  81%|████████  | 603/748 [03:42<00:53,  2.72it/s]Evaluating:  81%|████████  | 604/748 [03:42<00:52,  2.73it/s]Evaluating:  81%|████████  | 605/748 [03:42<00:52,  2.71it/s]Evaluating:  81%|████████  | 606/748 [03:43<00:52,  2.72it/s]Evaluating:  81%|████████  | 607/748 [03:43<00:52,  2.71it/s]Evaluating:  81%|████████▏ | 608/748 [03:44<00:51,  2.71it/s]Evaluating:  81%|████████▏ | 609/748 [03:44<00:51,  2.72it/s]Evaluating:  82%|████████▏ | 610/748 [03:44<00:50,  2.72it/s]Evaluating:  82%|████████▏ | 611/748 [03:45<00:50,  2.72it/s]Evaluating:  82%|████████▏ | 612/748 [03:45<00:49,  2.73it/s]Evaluating:  82%|████████▏ | 613/748 [03:45<00:49,  2.71it/s]Evaluating:  82%|████████▏ | 614/748 [03:46<00:49,  2.70it/s]Evaluating:  82%|████████▏ | 615/748 [03:46<00:49,  2.71it/s]Evaluating:  82%|████████▏ | 616/748 [03:46<00:48,  2.71it/s]Evaluating:  82%|████████▏ | 617/748 [03:47<00:48,  2.72it/s]Evaluating:  83%|████████▎ | 618/748 [03:47<00:47,  2.73it/s]Evaluating:  83%|████████▎ | 619/748 [03:48<00:47,  2.72it/s]Evaluating:  83%|████████▎ | 620/748 [03:48<00:47,  2.72it/s]Evaluating:  83%|████████▎ | 621/748 [03:48<00:46,  2.72it/s]Evaluating:  83%|████████▎ | 622/748 [03:49<00:46,  2.72it/s]Evaluating:  83%|████████▎ | 623/748 [03:49<00:46,  2.72it/s]Evaluating:  83%|████████▎ | 624/748 [03:49<00:45,  2.73it/s]Evaluating:  84%|████████▎ | 625/748 [03:50<00:45,  2.72it/s]Evaluating:  84%|████████▎ | 626/748 [03:50<00:44,  2.71it/s]Evaluating:  84%|████████▍ | 627/748 [03:51<00:44,  2.70it/s]Evaluating:  84%|████████▍ | 628/748 [03:51<00:44,  2.70it/s]Evaluating:  84%|████████▍ | 629/748 [03:51<00:44,  2.70it/s]Evaluating:  84%|████████▍ | 630/748 [03:52<00:43,  2.70it/s]Evaluating:  84%|████████▍ | 631/748 [03:52<00:43,  2.71it/s]Evaluating:  84%|████████▍ | 632/748 [03:52<00:42,  2.71it/s]Evaluating:  85%|████████▍ | 633/748 [03:53<00:42,  2.70it/s]Evaluating:  85%|████████▍ | 634/748 [03:53<00:42,  2.71it/s]Evaluating:  85%|████████▍ | 635/748 [03:53<00:41,  2.70it/s]Evaluating:  85%|████████▌ | 636/748 [03:54<00:41,  2.71it/s]Evaluating:  85%|████████▌ | 637/748 [03:54<00:40,  2.72it/s]Evaluating:  85%|████████▌ | 638/748 [03:55<00:40,  2.70it/s]Evaluating:  85%|████████▌ | 639/748 [03:55<00:40,  2.71it/s]Evaluating:  86%|████████▌ | 640/748 [03:55<00:39,  2.71it/s]Evaluating:  86%|████████▌ | 641/748 [03:56<00:39,  2.70it/s]Evaluating:  86%|████████▌ | 642/748 [03:56<00:39,  2.70it/s]Evaluating:  86%|████████▌ | 643/748 [03:56<00:38,  2.70it/s]Evaluating:  86%|████████▌ | 644/748 [03:57<00:38,  2.70it/s]Evaluating:  86%|████████▌ | 645/748 [03:57<00:37,  2.71it/s]Evaluating:  86%|████████▋ | 646/748 [03:58<00:37,  2.72it/s]Evaluating:  86%|████████▋ | 647/748 [03:58<00:37,  2.72it/s]Evaluating:  87%|████████▋ | 648/748 [03:58<00:36,  2.72it/s]Evaluating:  87%|████████▋ | 649/748 [03:59<00:36,  2.72it/s]Evaluating:  87%|████████▋ | 650/748 [03:59<00:36,  2.71it/s]Evaluating:  87%|████████▋ | 651/748 [03:59<00:37,  2.61it/s]Evaluating:  87%|████████▋ | 652/748 [04:00<00:36,  2.65it/s]Evaluating:  87%|████████▋ | 653/748 [04:00<00:35,  2.68it/s]Evaluating:  87%|████████▋ | 654/748 [04:00<00:34,  2.70it/s]Evaluating:  88%|████████▊ | 655/748 [04:01<00:34,  2.71it/s]Evaluating:  88%|████████▊ | 656/748 [04:01<00:33,  2.71it/s]Evaluating:  88%|████████▊ | 657/748 [04:02<00:33,  2.72it/s]Evaluating:  88%|████████▊ | 658/748 [04:02<00:33,  2.71it/s]Evaluating:  88%|████████▊ | 659/748 [04:02<00:32,  2.72it/s]Evaluating:  88%|████████▊ | 660/748 [04:03<00:32,  2.73it/s]Evaluating:  88%|████████▊ | 661/748 [04:03<00:31,  2.73it/s]Evaluating:  89%|████████▊ | 662/748 [04:03<00:31,  2.74it/s]Evaluating:  89%|████████▊ | 663/748 [04:04<00:31,  2.73it/s]Evaluating:  89%|████████▉ | 664/748 [04:04<00:30,  2.71it/s]Evaluating:  89%|████████▉ | 665/748 [04:05<00:30,  2.71it/s]Evaluating:  89%|████████▉ | 666/748 [04:05<00:30,  2.71it/s]Evaluating:  89%|████████▉ | 667/748 [04:05<00:29,  2.72it/s]Evaluating:  89%|████████▉ | 668/748 [04:06<00:29,  2.70it/s]Evaluating:  89%|████████▉ | 669/748 [04:06<00:29,  2.69it/s]Evaluating:  90%|████████▉ | 670/748 [04:06<00:28,  2.69it/s]Evaluating:  90%|████████▉ | 671/748 [04:07<00:28,  2.70it/s]Evaluating:  90%|████████▉ | 672/748 [04:07<00:28,  2.70it/s]Evaluating:  90%|████████▉ | 673/748 [04:07<00:27,  2.70it/s]Evaluating:  90%|█████████ | 674/748 [04:08<00:27,  2.70it/s]Evaluating:  90%|█████████ | 675/748 [04:08<00:26,  2.71it/s]Evaluating:  90%|█████████ | 676/748 [04:09<00:26,  2.71it/s]Evaluating:  91%|█████████ | 677/748 [04:09<00:26,  2.70it/s]Evaluating:  91%|█████████ | 678/748 [04:09<00:25,  2.70it/s]Evaluating:  91%|█████████ | 679/748 [04:10<00:25,  2.70it/s]Evaluating:  91%|█████████ | 680/748 [04:10<00:25,  2.71it/s]Evaluating:  91%|█████████ | 681/748 [04:10<00:24,  2.71it/s]Evaluating:  91%|█████████ | 682/748 [04:11<00:24,  2.71it/s]Evaluating:  91%|█████████▏| 683/748 [04:11<00:24,  2.69it/s]Evaluating:  91%|█████████▏| 684/748 [04:12<00:23,  2.70it/s]Evaluating:  92%|█████████▏| 685/748 [04:12<00:23,  2.71it/s]Evaluating:  92%|█████████▏| 686/748 [04:12<00:22,  2.72it/s]Evaluating:  92%|█████████▏| 687/748 [04:13<00:22,  2.71it/s]Evaluating:  92%|█████████▏| 688/748 [04:13<00:22,  2.72it/s]Evaluating:  92%|█████████▏| 689/748 [04:13<00:21,  2.72it/s]Evaluating:  92%|█████████▏| 690/748 [04:14<00:21,  2.71it/s]Evaluating:  92%|█████████▏| 691/748 [04:14<00:21,  2.71it/s]Evaluating:  93%|█████████▎| 692/748 [04:15<00:20,  2.72it/s]Evaluating:  93%|█████████▎| 693/748 [04:15<00:20,  2.72it/s]Evaluating:  93%|█████████▎| 694/748 [04:15<00:19,  2.72it/s]Evaluating:  93%|█████████▎| 695/748 [04:16<00:19,  2.72it/s]Evaluating:  93%|█████████▎| 696/748 [04:16<00:19,  2.72it/s]Evaluating:  93%|█████████▎| 697/748 [04:16<00:18,  2.72it/s]Evaluating:  93%|█████████▎| 698/748 [04:17<00:18,  2.73it/s]Evaluating:  93%|█████████▎| 699/748 [04:17<00:18,  2.72it/s]Evaluating:  94%|█████████▎| 700/748 [04:17<00:17,  2.72it/s]Evaluating:  94%|█████████▎| 701/748 [04:18<00:17,  2.73it/s]Evaluating:  94%|█████████▍| 702/748 [04:18<00:16,  2.74it/s]Evaluating:  94%|█████████▍| 703/748 [04:19<00:16,  2.73it/s]Evaluating:  94%|█████████▍| 704/748 [04:19<00:16,  2.73it/s]Evaluating:  94%|█████████▍| 705/748 [04:19<00:15,  2.72it/s]Evaluating:  94%|█████████▍| 706/748 [04:20<00:15,  2.72it/s]Evaluating:  95%|█████████▍| 707/748 [04:20<00:15,  2.72it/s]Evaluating:  95%|█████████▍| 708/748 [04:20<00:14,  2.71it/s]Evaluating:  95%|█████████▍| 709/748 [04:21<00:14,  2.71it/s]Evaluating:  95%|█████████▍| 710/748 [04:21<00:14,  2.71it/s]Evaluating:  95%|█████████▌| 711/748 [04:21<00:13,  2.72it/s]Evaluating:  95%|█████████▌| 712/748 [04:22<00:13,  2.72it/s]Evaluating:  95%|█████████▌| 713/748 [04:22<00:12,  2.73it/s]Evaluating:  95%|█████████▌| 714/748 [04:23<00:12,  2.71it/s]Evaluating:  96%|█████████▌| 715/748 [04:23<00:12,  2.70it/s]Evaluating:  96%|█████████▌| 716/748 [04:23<00:11,  2.72it/s]Evaluating:  96%|█████████▌| 717/748 [04:24<00:11,  2.73it/s]Evaluating:  96%|█████████▌| 718/748 [04:24<00:10,  2.74it/s]Evaluating:  96%|█████████▌| 719/748 [04:24<00:10,  2.73it/s]Evaluating:  96%|█████████▋| 720/748 [04:25<00:10,  2.74it/s]Evaluating:  96%|█████████▋| 721/748 [04:25<00:09,  2.74it/s]Evaluating:  97%|█████████▋| 722/748 [04:26<00:09,  2.75it/s]Evaluating:  97%|█████████▋| 723/748 [04:26<00:09,  2.73it/s]Evaluating:  97%|█████████▋| 724/748 [04:26<00:08,  2.72it/s]Evaluating:  97%|█████████▋| 725/748 [04:27<00:08,  2.71it/s]Evaluating:  97%|█████████▋| 726/748 [04:27<00:08,  2.72it/s]Evaluating:  97%|█████████▋| 727/748 [04:27<00:07,  2.72it/s]Evaluating:  97%|█████████▋| 728/748 [04:28<00:07,  2.73it/s]Evaluating:  97%|█████████▋| 729/748 [04:28<00:06,  2.72it/s]Evaluating:  98%|█████████▊| 730/748 [04:28<00:06,  2.71it/s]Evaluating:  98%|█████████▊| 731/748 [04:29<00:06,  2.71it/s]Evaluating:  98%|█████████▊| 732/748 [04:29<00:05,  2.72it/s]Evaluating:  98%|█████████▊| 733/748 [04:30<00:05,  2.72it/s]Evaluating:  98%|█████████▊| 734/748 [04:30<00:05,  2.72it/s]Evaluating:  98%|█████████▊| 735/748 [04:30<00:04,  2.72it/s]Evaluating:  98%|█████████▊| 736/748 [04:31<00:04,  2.72it/s]Evaluating:  99%|█████████▊| 737/748 [04:31<00:04,  2.73it/s]Evaluating:  99%|█████████▊| 738/748 [04:31<00:03,  2.73it/s]Evaluating:  99%|█████████▉| 739/748 [04:32<00:03,  2.74it/s]Evaluating:  99%|█████████▉| 740/748 [04:32<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 741/748 [04:33<00:02,  2.73it/s]Evaluating:  99%|█████████▉| 742/748 [04:33<00:02,  2.72it/s]Evaluating:  99%|█████████▉| 743/748 [04:33<00:01,  2.72it/s]Evaluating:  99%|█████████▉| 744/748 [04:34<00:01,  2.71it/s]Evaluating: 100%|█████████▉| 745/748 [04:34<00:01,  2.72it/s]Evaluating: 100%|█████████▉| 746/748 [04:34<00:00,  2.72it/s]Evaluating: 100%|█████████▉| 747/748 [04:35<00:00,  2.72it/s]Evaluating: 100%|██████████| 748/748 [04:35<00:00,  3.02it/s]Evaluating: 100%|██████████| 748/748 [04:35<00:00,  2.72it/s]
05/10/2022 00:46:31 - INFO - __main__ -     Evaluation done in total 275.457181 secs (0.046055 sec per example)
05/10/2022 00:54:15 - INFO - __main__ -   Results: {'exact': 5.3338524430601515, 'f1': 6.346757572667639, 'total': 5137, 'HasAns_exact': 5.3338524430601515, 'HasAns_f1': 6.346757572667639, 'HasAns_total': 5137, 'best_exact': 5.3338524430601515, 'best_exact_thresh': 0.0, 'best_f1': 6.346757572667639, 'best_f1_thresh': 0.0}
