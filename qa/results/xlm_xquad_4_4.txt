Fine-tuning /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/xlm-roberta-base on xquad using GPU 2
Load data from /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download/, and save models to /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter/
************************
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/xlm-roberta-base
************************

Predictions on xquad
  en 
2022-05-08 20:31:18.999793: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:31:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'qa_outputs.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'qa_outputs.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:31:31 - INFO - __main__ -   lang2id = None
05/08/2022 20:31:34 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='en', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.en.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:31:34 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:31:34 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'qa_outputs.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'qa_outputs.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:31:48 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 31%|      | 15/48 [00:00<00:00, 143.26it/s] 62%|   | 30/48 [00:00<00:00, 113.52it/s] 94%|| 45/48 [00:00<00:00, 123.55it/s]100%|| 48/48 [00:00<00:00, 124.69it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<02:46,  7.16it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:04, 228.10it/s]convert squad examples to features:  11%|         | 129/1190 [00:00<00:03, 305.37it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:03, 295.62it/s]convert squad examples to features:  19%|        | 225/1190 [00:00<00:02, 334.05it/s]convert squad examples to features:  22%|       | 258/1190 [00:00<00:03, 302.13it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:02, 303.72it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:03, 274.32it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:02, 284.15it/s]convert squad examples to features:  32%|      | 385/1190 [00:01<00:04, 170.50it/s]convert squad examples to features:  35%|      | 417/1190 [00:01<00:04, 183.93it/s]convert squad examples to features:  38%|      | 449/1190 [00:01<00:03, 202.70it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:03, 199.52it/s]convert squad examples to features:  46%|     | 545/1190 [00:02<00:03, 183.46it/s]convert squad examples to features:  51%|     | 609/1190 [00:02<00:02, 236.78it/s]convert squad examples to features:  54%|    | 641/1190 [00:02<00:02, 237.00it/s]convert squad examples to features:  57%|    | 673/1190 [00:02<00:02, 236.14it/s]convert squad examples to features:  59%|    | 705/1190 [00:02<00:02, 242.12it/s]convert squad examples to features:  62%|   | 737/1190 [00:03<00:01, 249.24it/s]convert squad examples to features:  65%|   | 769/1190 [00:03<00:01, 252.54it/s]convert squad examples to features:  67%|   | 801/1190 [00:03<00:01, 267.13it/s]convert squad examples to features:  70%|   | 833/1190 [00:03<00:01, 246.52it/s]convert squad examples to features:  73%|  | 865/1190 [00:03<00:01, 247.15it/s]convert squad examples to features:  75%|  | 897/1190 [00:03<00:01, 241.92it/s]convert squad examples to features:  78%|  | 929/1190 [00:03<00:01, 245.58it/s]convert squad examples to features:  81%|  | 961/1190 [00:03<00:00, 253.15it/s]convert squad examples to features:  86%| | 1025/1190 [00:04<00:00, 268.67it/s]convert squad examples to features:  92%|| 1089/1190 [00:04<00:00, 279.72it/s]convert squad examples to features:  94%|| 1121/1190 [00:04<00:00, 274.11it/s]convert squad examples to features:  97%|| 1153/1190 [00:04<00:00, 245.55it/s]convert squad examples to features: 100%|| 1190/1190 [00:04<00:00, 251.59it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 588934.72it/s]
05/08/2022 20:31:53 - INFO - __main__ -   Saving features into cached file ./cached_xquad.en.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_en
05/08/2022 20:31:54 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:31:54 - INFO - __main__ -     Num examples = 1270
05/08/2022 20:31:54 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]Evaluating:   1%|          | 1/159 [00:00<01:41,  1.55it/s]Evaluating:   1%|         | 2/159 [00:00<00:53,  2.96it/s]Evaluating:   2%|         | 3/159 [00:00<00:37,  4.17it/s]Evaluating:   3%|         | 4/159 [00:01<00:29,  5.24it/s]Evaluating:   3%|         | 5/159 [00:01<00:25,  6.09it/s]Evaluating:   4%|         | 6/159 [00:01<00:22,  6.75it/s]Evaluating:   4%|         | 7/159 [00:01<00:20,  7.26it/s]Evaluating:   5%|         | 8/159 [00:01<00:19,  7.63it/s]Evaluating:   6%|         | 9/159 [00:01<00:19,  7.83it/s]Evaluating:   6%|         | 10/159 [00:01<00:18,  8.02it/s]Evaluating:   7%|         | 11/159 [00:01<00:18,  8.16it/s]Evaluating:   8%|         | 12/159 [00:01<00:17,  8.23it/s]Evaluating:   8%|         | 13/159 [00:02<00:17,  8.27it/s]Evaluating:   9%|         | 14/159 [00:02<00:17,  8.32it/s]Evaluating:   9%|         | 15/159 [00:02<00:17,  8.32it/s]Evaluating:  10%|         | 16/159 [00:02<00:17,  8.32it/s]Evaluating:  11%|         | 17/159 [00:02<00:17,  8.31it/s]Evaluating:  11%|        | 18/159 [00:02<00:16,  8.40it/s]Evaluating:  12%|        | 19/159 [00:02<00:16,  8.43it/s]Evaluating:  13%|        | 20/159 [00:02<00:16,  8.43it/s]Evaluating:  13%|        | 21/159 [00:03<00:16,  8.47it/s]Evaluating:  14%|        | 22/159 [00:03<00:16,  8.50it/s]Evaluating:  14%|        | 23/159 [00:03<00:16,  8.45it/s]Evaluating:  15%|        | 24/159 [00:03<00:15,  8.50it/s]Evaluating:  16%|        | 25/159 [00:03<00:16,  8.18it/s]Evaluating:  16%|        | 26/159 [00:03<00:16,  8.29it/s]Evaluating:  17%|        | 27/159 [00:03<00:15,  8.35it/s]Evaluating:  18%|        | 28/159 [00:03<00:15,  8.29it/s]Evaluating:  18%|        | 29/159 [00:03<00:15,  8.34it/s]Evaluating:  19%|        | 30/159 [00:04<00:15,  8.41it/s]Evaluating:  19%|        | 31/159 [00:04<00:15,  8.44it/s]Evaluating:  20%|        | 32/159 [00:04<00:15,  8.43it/s]Evaluating:  21%|        | 33/159 [00:04<00:14,  8.48it/s]Evaluating:  21%|       | 34/159 [00:04<00:14,  8.48it/s]Evaluating:  22%|       | 35/159 [00:04<00:14,  8.43it/s]Evaluating:  23%|       | 36/159 [00:04<00:14,  8.39it/s]Evaluating:  23%|       | 37/159 [00:04<00:14,  8.37it/s]Evaluating:  24%|       | 38/159 [00:05<00:14,  8.38it/s]Evaluating:  25%|       | 39/159 [00:05<00:14,  8.34it/s]Evaluating:  25%|       | 40/159 [00:05<00:14,  8.37it/s]Evaluating:  26%|       | 41/159 [00:05<00:14,  8.40it/s]Evaluating:  26%|       | 42/159 [00:05<00:13,  8.44it/s]Evaluating:  27%|       | 43/159 [00:05<00:13,  8.46it/s]Evaluating:  28%|       | 44/159 [00:05<00:13,  8.43it/s]Evaluating:  28%|       | 45/159 [00:05<00:13,  8.45it/s]Evaluating:  29%|       | 46/159 [00:05<00:13,  8.45it/s]Evaluating:  30%|       | 47/159 [00:06<00:13,  8.38it/s]Evaluating:  30%|       | 48/159 [00:06<00:13,  8.40it/s]Evaluating:  31%|       | 49/159 [00:06<00:13,  8.35it/s]Evaluating:  31%|      | 50/159 [00:06<00:12,  8.40it/s]Evaluating:  32%|      | 51/159 [00:06<00:12,  8.41it/s]Evaluating:  33%|      | 52/159 [00:06<00:12,  8.43it/s]Evaluating:  33%|      | 53/159 [00:06<00:12,  8.41it/s]Evaluating:  34%|      | 54/159 [00:06<00:12,  8.34it/s]Evaluating:  35%|      | 55/159 [00:07<00:12,  8.37it/s]Evaluating:  35%|      | 56/159 [00:07<00:12,  8.38it/s]Evaluating:  36%|      | 57/159 [00:07<00:12,  8.33it/s]Evaluating:  36%|      | 58/159 [00:07<00:12,  8.32it/s]Evaluating:  37%|      | 59/159 [00:07<00:11,  8.36it/s]Evaluating:  38%|      | 60/159 [00:07<00:11,  8.35it/s]Evaluating:  38%|      | 61/159 [00:07<00:11,  8.39it/s]Evaluating:  39%|      | 62/159 [00:07<00:11,  8.43it/s]Evaluating:  40%|      | 63/159 [00:08<00:11,  8.42it/s]Evaluating:  40%|      | 64/159 [00:08<00:11,  8.46it/s]Evaluating:  41%|      | 65/159 [00:08<00:11,  8.48it/s]Evaluating:  42%|     | 66/159 [00:08<00:11,  8.44it/s]Evaluating:  42%|     | 67/159 [00:08<00:10,  8.40it/s]Evaluating:  43%|     | 68/159 [00:08<00:10,  8.43it/s]Evaluating:  43%|     | 69/159 [00:08<00:10,  8.45it/s]Evaluating:  44%|     | 70/159 [00:08<00:10,  8.47it/s]Evaluating:  45%|     | 71/159 [00:08<00:10,  8.48it/s]Evaluating:  45%|     | 72/159 [00:09<00:10,  8.46it/s]Evaluating:  46%|     | 73/159 [00:09<00:10,  8.48it/s]Evaluating:  47%|     | 74/159 [00:09<00:10,  8.48it/s]Evaluating:  47%|     | 75/159 [00:09<00:09,  8.49it/s]Evaluating:  48%|     | 76/159 [00:09<00:09,  8.49it/s]Evaluating:  48%|     | 77/159 [00:09<00:09,  8.50it/s]Evaluating:  49%|     | 78/159 [00:09<00:09,  8.52it/s]Evaluating:  50%|     | 79/159 [00:09<00:09,  8.50it/s]Evaluating:  50%|     | 80/159 [00:10<00:09,  8.51it/s]Evaluating:  51%|     | 81/159 [00:10<00:09,  8.52it/s]Evaluating:  52%|    | 82/159 [00:10<00:09,  8.52it/s]Evaluating:  52%|    | 83/159 [00:10<00:08,  8.53it/s]Evaluating:  53%|    | 84/159 [00:10<00:08,  8.51it/s]Evaluating:  53%|    | 85/159 [00:10<00:08,  8.52it/s]Evaluating:  54%|    | 86/159 [00:10<00:08,  8.52it/s]Evaluating:  55%|    | 87/159 [00:10<00:08,  8.52it/s]Evaluating:  55%|    | 88/159 [00:10<00:08,  8.52it/s]Evaluating:  56%|    | 89/159 [00:11<00:08,  8.46it/s]Evaluating:  57%|    | 90/159 [00:11<00:08,  8.42it/s]Evaluating:  57%|    | 91/159 [00:11<00:08,  8.41it/s]Evaluating:  58%|    | 92/159 [00:11<00:07,  8.44it/s]Evaluating:  58%|    | 93/159 [00:11<00:07,  8.44it/s]Evaluating:  59%|    | 94/159 [00:11<00:07,  8.45it/s]Evaluating:  60%|    | 95/159 [00:11<00:07,  8.48it/s]Evaluating:  60%|    | 96/159 [00:11<00:07,  8.50it/s]Evaluating:  61%|    | 97/159 [00:12<00:07,  8.51it/s]Evaluating:  62%|   | 98/159 [00:12<00:07,  8.51it/s]Evaluating:  62%|   | 99/159 [00:12<00:07,  8.50it/s]Evaluating:  63%|   | 100/159 [00:12<00:06,  8.51it/s]Evaluating:  64%|   | 101/159 [00:12<00:06,  8.49it/s]Evaluating:  64%|   | 102/159 [00:12<00:06,  8.50it/s]Evaluating:  65%|   | 103/159 [00:12<00:06,  8.51it/s]Evaluating:  65%|   | 104/159 [00:12<00:06,  8.53it/s]Evaluating:  66%|   | 105/159 [00:12<00:06,  8.54it/s]Evaluating:  67%|   | 106/159 [00:13<00:06,  8.54it/s]Evaluating:  67%|   | 107/159 [00:13<00:06,  8.54it/s]Evaluating:  68%|   | 108/159 [00:13<00:05,  8.55it/s]Evaluating:  69%|   | 109/159 [00:13<00:05,  8.53it/s]Evaluating:  69%|   | 110/159 [00:13<00:05,  8.54it/s]Evaluating:  70%|   | 111/159 [00:13<00:05,  8.54it/s]Evaluating:  70%|   | 112/159 [00:13<00:05,  8.55it/s]Evaluating:  71%|   | 113/159 [00:13<00:05,  8.54it/s]Evaluating:  72%|  | 114/159 [00:14<00:05,  8.51it/s]Evaluating:  72%|  | 115/159 [00:14<00:05,  8.50it/s]Evaluating:  73%|  | 116/159 [00:14<00:05,  8.51it/s]Evaluating:  74%|  | 117/159 [00:14<00:04,  8.51it/s]Evaluating:  74%|  | 118/159 [00:14<00:04,  8.51it/s]Evaluating:  75%|  | 119/159 [00:14<00:04,  8.51it/s]Evaluating:  75%|  | 120/159 [00:14<00:04,  8.49it/s]Evaluating:  76%|  | 121/159 [00:14<00:04,  8.44it/s]Evaluating:  77%|  | 122/159 [00:14<00:04,  8.47it/s]Evaluating:  77%|  | 123/159 [00:15<00:04,  8.47it/s]Evaluating:  78%|  | 124/159 [00:15<00:04,  8.47it/s]Evaluating:  79%|  | 125/159 [00:15<00:04,  8.44it/s]Evaluating:  79%|  | 126/159 [00:15<00:03,  8.32it/s]Evaluating:  80%|  | 127/159 [00:15<00:03,  8.31it/s]Evaluating:  81%|  | 128/159 [00:15<00:03,  8.28it/s]Evaluating:  81%|  | 129/159 [00:15<00:03,  8.32it/s]Evaluating:  82%| | 130/159 [00:15<00:03,  8.39it/s]Evaluating:  82%| | 131/159 [00:16<00:03,  8.44it/s]Evaluating:  83%| | 132/159 [00:16<00:03,  8.46it/s]Evaluating:  84%| | 133/159 [00:16<00:03,  8.49it/s]Evaluating:  84%| | 134/159 [00:16<00:02,  8.44it/s]Evaluating:  85%| | 135/159 [00:16<00:02,  8.44it/s]Evaluating:  86%| | 136/159 [00:16<00:02,  8.46it/s]Evaluating:  86%| | 137/159 [00:16<00:02,  8.46it/s]Evaluating:  87%| | 138/159 [00:16<00:02,  8.48it/s]Evaluating:  87%| | 139/159 [00:16<00:02,  8.46it/s]Evaluating:  88%| | 140/159 [00:17<00:02,  8.43it/s]Evaluating:  89%| | 141/159 [00:17<00:02,  8.41it/s]Evaluating:  89%| | 142/159 [00:17<00:02,  8.43it/s]Evaluating:  90%| | 143/159 [00:17<00:01,  8.46it/s]Evaluating:  91%| | 144/159 [00:17<00:01,  8.44it/s]Evaluating:  91%| | 145/159 [00:17<00:01,  8.46it/s]Evaluating:  92%|| 146/159 [00:17<00:01,  8.46it/s]Evaluating:  92%|| 147/159 [00:17<00:01,  8.48it/s]Evaluating:  93%|| 148/159 [00:18<00:01,  8.48it/s]Evaluating:  94%|| 149/159 [00:18<00:01,  8.48it/s]Evaluating:  94%|| 150/159 [00:18<00:01,  8.49it/s]Evaluating:  95%|| 151/159 [00:18<00:00,  8.49it/s]Evaluating:  96%|| 152/159 [00:18<00:00,  8.49it/s]Evaluating:  96%|| 153/159 [00:18<00:00,  8.49it/s]Evaluating:  97%|| 154/159 [00:18<00:00,  8.49it/s]Evaluating:  97%|| 155/159 [00:18<00:00,  8.50it/s]Evaluating:  98%|| 156/159 [00:18<00:00,  8.49it/s]Evaluating:  99%|| 157/159 [00:19<00:00,  8.48it/s]Evaluating:  99%|| 158/159 [00:19<00:00,  8.43it/s]Evaluating: 100%|| 159/159 [00:19<00:00,  8.23it/s]
05/08/2022 20:32:14 - INFO - __main__ -     Evaluation done in total 19.324418 secs (0.015216 sec per example)
05/08/2022 20:32:17 - INFO - __main__ -   Results: {'exact': 72.60504201680672, 'f1': 83.68146177179473, 'total': 1190, 'HasAns_exact': 72.60504201680672, 'HasAns_f1': 83.68146177179473, 'HasAns_total': 1190, 'best_exact': 72.60504201680672, 'best_exact_thresh': 0.0, 'best_f1': 83.68146177179473, 'best_f1_thresh': 0.0}
  es 
2022-05-08 20:32:20.603664: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:32:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'qa_outputs.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'qa_outputs.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:32:32 - INFO - __main__ -   lang2id = None
05/08/2022 20:32:36 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='es', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.es.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:32:36 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:32:36 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'qa_outputs.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'qa_outputs.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:32:48 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 27%|       | 13/48 [00:00<00:00, 127.40it/s] 54%|    | 26/48 [00:00<00:00, 100.75it/s] 77%|  | 37/48 [00:00<00:00, 89.86it/s] 100%|| 48/48 [00:00<00:00, 103.55it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<04:10,  4.75it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:06, 169.43it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:05, 206.74it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:03, 273.46it/s]convert squad examples to features:  19%|        | 225/1190 [00:00<00:03, 296.93it/s]convert squad examples to features:  22%|       | 257/1190 [00:01<00:03, 271.36it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:03, 246.54it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:03, 229.27it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:03, 215.79it/s]convert squad examples to features:  32%|      | 385/1190 [00:02<00:06, 131.54it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:05, 140.40it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:04, 157.50it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:04, 157.09it/s]convert squad examples to features:  43%|     | 513/1190 [00:02<00:03, 170.93it/s]convert squad examples to features:  46%|     | 545/1190 [00:03<00:04, 134.18it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:03, 182.77it/s]convert squad examples to features:  54%|    | 641/1190 [00:03<00:03, 182.21it/s]convert squad examples to features:  57%|    | 673/1190 [00:03<00:02, 181.16it/s]convert squad examples to features:  59%|    | 705/1190 [00:03<00:02, 198.38it/s]convert squad examples to features:  62%|   | 737/1190 [00:03<00:02, 215.44it/s]convert squad examples to features:  65%|   | 769/1190 [00:04<00:01, 220.34it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:01, 214.23it/s]convert squad examples to features:  70%|   | 833/1190 [00:04<00:01, 208.17it/s]convert squad examples to features:  73%|  | 865/1190 [00:04<00:01, 211.69it/s]convert squad examples to features:  75%|  | 897/1190 [00:04<00:01, 215.64it/s]convert squad examples to features:  78%|  | 929/1190 [00:04<00:01, 202.43it/s]convert squad examples to features:  81%|  | 961/1190 [00:04<00:01, 201.68it/s]convert squad examples to features:  83%| | 993/1190 [00:05<00:00, 219.24it/s]convert squad examples to features:  86%| | 1025/1190 [00:05<00:00, 236.00it/s]convert squad examples to features:  89%| | 1057/1190 [00:05<00:00, 247.45it/s]convert squad examples to features:  92%|| 1089/1190 [00:05<00:00, 250.71it/s]convert squad examples to features:  94%|| 1121/1190 [00:05<00:00, 241.01it/s]convert squad examples to features:  97%|| 1153/1190 [00:05<00:00, 229.31it/s]convert squad examples to features: 100%|| 1190/1190 [00:05<00:00, 206.03it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 667097.27it/s]
05/08/2022 20:32:54 - INFO - __main__ -   Saving features into cached file ./cached_xquad.es.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_es
05/08/2022 20:32:55 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:32:55 - INFO - __main__ -     Num examples = 1304
05/08/2022 20:32:55 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]Evaluating:   1%|          | 1/163 [00:00<01:29,  1.81it/s]Evaluating:   1%|          | 2/163 [00:00<00:49,  3.28it/s]Evaluating:   2%|         | 3/163 [00:00<00:35,  4.55it/s]Evaluating:   2%|         | 4/163 [00:00<00:28,  5.57it/s]Evaluating:   3%|         | 5/163 [00:01<00:24,  6.37it/s]Evaluating:   4%|         | 6/163 [00:01<00:22,  6.99it/s]Evaluating:   4%|         | 7/163 [00:01<00:20,  7.44it/s]Evaluating:   5%|         | 8/163 [00:01<00:19,  7.79it/s]Evaluating:   6%|         | 9/163 [00:01<00:19,  7.98it/s]Evaluating:   6%|         | 10/163 [00:01<00:19,  8.03it/s]Evaluating:   7%|         | 11/163 [00:01<00:18,  8.12it/s]Evaluating:   7%|         | 12/163 [00:01<00:18,  8.26it/s]Evaluating:   8%|         | 13/163 [00:01<00:17,  8.34it/s]Evaluating:   9%|         | 14/163 [00:02<00:17,  8.37it/s]Evaluating:   9%|         | 15/163 [00:02<00:17,  8.43it/s]Evaluating:  10%|         | 16/163 [00:02<00:17,  8.44it/s]Evaluating:  10%|         | 17/163 [00:02<00:17,  8.41it/s]Evaluating:  11%|         | 18/163 [00:02<00:17,  8.38it/s]Evaluating:  12%|        | 19/163 [00:02<00:17,  8.38it/s]Evaluating:  12%|        | 20/163 [00:02<00:16,  8.42it/s]Evaluating:  13%|        | 21/163 [00:02<00:16,  8.45it/s]Evaluating:  13%|        | 22/163 [00:03<00:16,  8.49it/s]Evaluating:  14%|        | 23/163 [00:03<00:16,  8.51it/s]Evaluating:  15%|        | 24/163 [00:03<00:17,  8.14it/s]Evaluating:  15%|        | 25/163 [00:03<00:16,  8.25it/s]Evaluating:  16%|        | 26/163 [00:03<00:16,  8.30it/s]Evaluating:  17%|        | 27/163 [00:03<00:16,  8.37it/s]Evaluating:  17%|        | 28/163 [00:03<00:16,  8.27it/s]Evaluating:  18%|        | 29/163 [00:03<00:16,  8.28it/s]Evaluating:  18%|        | 30/163 [00:04<00:15,  8.37it/s]Evaluating:  19%|        | 31/163 [00:04<00:15,  8.40it/s]Evaluating:  20%|        | 32/163 [00:04<00:15,  8.40it/s]Evaluating:  20%|        | 33/163 [00:04<00:15,  8.35it/s]Evaluating:  21%|        | 34/163 [00:04<00:15,  8.38it/s]Evaluating:  21%|       | 35/163 [00:04<00:15,  8.41it/s]Evaluating:  22%|       | 36/163 [00:04<00:15,  8.43it/s]Evaluating:  23%|       | 37/163 [00:04<00:14,  8.42it/s]Evaluating:  23%|       | 38/163 [00:04<00:14,  8.41it/s]Evaluating:  24%|       | 39/163 [00:05<00:14,  8.42it/s]Evaluating:  25%|       | 40/163 [00:05<00:14,  8.44it/s]Evaluating:  25%|       | 41/163 [00:05<00:14,  8.47it/s]Evaluating:  26%|       | 42/163 [00:05<00:14,  8.47it/s]Evaluating:  26%|       | 43/163 [00:05<00:14,  8.45it/s]Evaluating:  27%|       | 44/163 [00:05<00:14,  8.42it/s]Evaluating:  28%|       | 45/163 [00:05<00:14,  8.41it/s]Evaluating:  28%|       | 46/163 [00:05<00:13,  8.44it/s]Evaluating:  29%|       | 47/163 [00:06<00:13,  8.44it/s]Evaluating:  29%|       | 48/163 [00:06<00:13,  8.40it/s]Evaluating:  30%|       | 49/163 [00:06<00:13,  8.18it/s]Evaluating:  31%|       | 50/163 [00:06<00:13,  8.27it/s]Evaluating:  31%|      | 51/163 [00:06<00:13,  8.29it/s]Evaluating:  32%|      | 52/163 [00:06<00:13,  8.27it/s]Evaluating:  33%|      | 53/163 [00:06<00:13,  8.25it/s]Evaluating:  33%|      | 54/163 [00:06<00:13,  8.28it/s]Evaluating:  34%|      | 55/163 [00:06<00:12,  8.33it/s]Evaluating:  34%|      | 56/163 [00:07<00:12,  8.35it/s]Evaluating:  35%|      | 57/163 [00:07<00:12,  8.29it/s]Evaluating:  36%|      | 58/163 [00:07<00:12,  8.25it/s]Evaluating:  36%|      | 59/163 [00:07<00:12,  8.20it/s]Evaluating:  37%|      | 60/163 [00:07<00:12,  8.19it/s]Evaluating:  37%|      | 61/163 [00:07<00:12,  8.20it/s]Evaluating:  38%|      | 62/163 [00:07<00:12,  8.16it/s]Evaluating:  39%|      | 63/163 [00:07<00:12,  8.25it/s]Evaluating:  39%|      | 64/163 [00:08<00:11,  8.30it/s]Evaluating:  40%|      | 65/163 [00:08<00:11,  8.30it/s]Evaluating:  40%|      | 66/163 [00:08<00:11,  8.33it/s]Evaluating:  41%|      | 67/163 [00:08<00:11,  8.36it/s]Evaluating:  42%|     | 68/163 [00:08<00:11,  8.35it/s]Evaluating:  42%|     | 69/163 [00:08<00:11,  8.34it/s]Evaluating:  43%|     | 70/163 [00:08<00:11,  8.37it/s]Evaluating:  44%|     | 71/163 [00:08<00:11,  8.34it/s]Evaluating:  44%|     | 72/163 [00:09<00:10,  8.38it/s]Evaluating:  45%|     | 73/163 [00:09<00:10,  8.40it/s]Evaluating:  45%|     | 74/163 [00:09<00:10,  8.39it/s]Evaluating:  46%|     | 75/163 [00:09<00:10,  8.34it/s]Evaluating:  47%|     | 76/163 [00:09<00:10,  8.37it/s]Evaluating:  47%|     | 77/163 [00:09<00:10,  8.37it/s]Evaluating:  48%|     | 78/163 [00:09<00:10,  8.31it/s]Evaluating:  48%|     | 79/163 [00:09<00:10,  8.29it/s]Evaluating:  49%|     | 80/163 [00:10<00:09,  8.32it/s]Evaluating:  50%|     | 81/163 [00:10<00:09,  8.36it/s]Evaluating:  50%|     | 82/163 [00:10<00:09,  8.40it/s]Evaluating:  51%|     | 83/163 [00:10<00:09,  8.41it/s]Evaluating:  52%|    | 84/163 [00:10<00:09,  8.38it/s]Evaluating:  52%|    | 85/163 [00:10<00:09,  8.40it/s]Evaluating:  53%|    | 86/163 [00:10<00:09,  8.41it/s]Evaluating:  53%|    | 87/163 [00:10<00:09,  8.42it/s]Evaluating:  54%|    | 88/163 [00:10<00:08,  8.33it/s]Evaluating:  55%|    | 89/163 [00:11<00:08,  8.34it/s]Evaluating:  55%|    | 90/163 [00:11<00:08,  8.39it/s]Evaluating:  56%|    | 91/163 [00:11<00:08,  8.39it/s]Evaluating:  56%|    | 92/163 [00:11<00:08,  8.41it/s]Evaluating:  57%|    | 93/163 [00:11<00:08,  8.40it/s]Evaluating:  58%|    | 94/163 [00:11<00:08,  8.39it/s]Evaluating:  58%|    | 95/163 [00:11<00:08,  8.37it/s]Evaluating:  59%|    | 96/163 [00:11<00:08,  8.33it/s]Evaluating:  60%|    | 97/163 [00:12<00:07,  8.35it/s]Evaluating:  60%|    | 98/163 [00:12<00:07,  8.39it/s]Evaluating:  61%|    | 99/163 [00:12<00:07,  8.39it/s]Evaluating:  61%|   | 100/163 [00:12<00:07,  8.42it/s]Evaluating:  62%|   | 101/163 [00:12<00:07,  8.32it/s]Evaluating:  63%|   | 102/163 [00:12<00:07,  8.36it/s]Evaluating:  63%|   | 103/163 [00:12<00:07,  8.39it/s]Evaluating:  64%|   | 104/163 [00:12<00:07,  8.43it/s]Evaluating:  64%|   | 105/163 [00:12<00:06,  8.43it/s]Evaluating:  65%|   | 106/163 [00:13<00:06,  8.43it/s]Evaluating:  66%|   | 107/163 [00:13<00:06,  8.40it/s]Evaluating:  66%|   | 108/163 [00:13<00:06,  8.44it/s]Evaluating:  67%|   | 109/163 [00:13<00:06,  8.45it/s]Evaluating:  67%|   | 110/163 [00:13<00:06,  8.47it/s]Evaluating:  68%|   | 111/163 [00:13<00:06,  8.48it/s]Evaluating:  69%|   | 112/163 [00:13<00:06,  8.48it/s]Evaluating:  69%|   | 113/163 [00:13<00:05,  8.44it/s]Evaluating:  70%|   | 114/163 [00:14<00:05,  8.38it/s]Evaluating:  71%|   | 115/163 [00:14<00:05,  8.34it/s]Evaluating:  71%|   | 116/163 [00:14<00:05,  8.28it/s]Evaluating:  72%|  | 117/163 [00:14<00:05,  8.32it/s]Evaluating:  72%|  | 118/163 [00:14<00:05,  8.35it/s]Evaluating:  73%|  | 119/163 [00:14<00:05,  8.38it/s]Evaluating:  74%|  | 120/163 [00:14<00:05,  8.39it/s]Evaluating:  74%|  | 121/163 [00:14<00:05,  8.36it/s]Evaluating:  75%|  | 122/163 [00:15<00:04,  8.35it/s]Evaluating:  75%|  | 123/163 [00:15<00:04,  8.37it/s]Evaluating:  76%|  | 124/163 [00:15<00:04,  8.29it/s]Evaluating:  77%|  | 125/163 [00:15<00:04,  8.32it/s]Evaluating:  77%|  | 126/163 [00:15<00:04,  8.33it/s]Evaluating:  78%|  | 127/163 [00:15<00:04,  8.36it/s]Evaluating:  79%|  | 128/163 [00:15<00:04,  8.32it/s]Evaluating:  79%|  | 129/163 [00:15<00:04,  8.33it/s]Evaluating:  80%|  | 130/163 [00:15<00:03,  8.37it/s]Evaluating:  80%|  | 131/163 [00:16<00:03,  8.38it/s]Evaluating:  81%|  | 132/163 [00:16<00:03,  8.41it/s]Evaluating:  82%| | 133/163 [00:16<00:03,  8.40it/s]Evaluating:  82%| | 134/163 [00:16<00:03,  8.43it/s]Evaluating:  83%| | 135/163 [00:16<00:03,  8.45it/s]Evaluating:  83%| | 136/163 [00:16<00:03,  8.46it/s]Evaluating:  84%| | 137/163 [00:16<00:03,  8.44it/s]Evaluating:  85%| | 138/163 [00:16<00:02,  8.46it/s]Evaluating:  85%| | 139/163 [00:17<00:02,  8.47it/s]Evaluating:  86%| | 140/163 [00:17<00:02,  8.48it/s]Evaluating:  87%| | 141/163 [00:17<00:02,  8.46it/s]Evaluating:  87%| | 142/163 [00:17<00:02,  8.46it/s]Evaluating:  88%| | 143/163 [00:17<00:02,  8.46it/s]Evaluating:  88%| | 144/163 [00:17<00:02,  8.44it/s]Evaluating:  89%| | 145/163 [00:17<00:02,  8.42it/s]Evaluating:  90%| | 146/163 [00:17<00:02,  8.44it/s]Evaluating:  90%| | 147/163 [00:17<00:01,  8.45it/s]Evaluating:  91%| | 148/163 [00:18<00:01,  8.45it/s]Evaluating:  91%|| 149/163 [00:18<00:01,  8.45it/s]Evaluating:  92%|| 150/163 [00:18<00:01,  8.45it/s]Evaluating:  93%|| 151/163 [00:18<00:01,  8.45it/s]Evaluating:  93%|| 152/163 [00:18<00:01,  8.45it/s]Evaluating:  94%|| 153/163 [00:18<00:01,  8.43it/s]Evaluating:  94%|| 154/163 [00:18<00:01,  8.41it/s]Evaluating:  95%|| 155/163 [00:18<00:00,  8.41it/s]Evaluating:  96%|| 156/163 [00:19<00:00,  8.39it/s]Evaluating:  96%|| 157/163 [00:19<00:00,  8.33it/s]Evaluating:  97%|| 158/163 [00:19<00:00,  8.36it/s]Evaluating:  98%|| 159/163 [00:19<00:00,  8.38it/s]Evaluating:  98%|| 160/163 [00:19<00:00,  8.37it/s]Evaluating:  99%|| 161/163 [00:19<00:00,  8.29it/s]Evaluating:  99%|| 162/163 [00:19<00:00,  8.22it/s]Evaluating: 100%|| 163/163 [00:19<00:00,  8.26it/s]Evaluating: 100%|| 163/163 [00:19<00:00,  8.19it/s]
05/08/2022 20:33:15 - INFO - __main__ -     Evaluation done in total 19.898521 secs (0.015260 sec per example)
05/08/2022 20:33:20 - INFO - __main__ -   Results: {'exact': 58.0672268907563, 'f1': 76.45639278523313, 'total': 1190, 'HasAns_exact': 58.0672268907563, 'HasAns_f1': 76.45639278523313, 'HasAns_total': 1190, 'best_exact': 58.0672268907563, 'best_exact_thresh': 0.0, 'best_f1': 76.45639278523313, 'best_f1_thresh': 0.0}
  de 
2022-05-08 20:33:22.631774: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:33:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.pooler.dense.weight', 'qa_outputs.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'qa_outputs.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:33:35 - INFO - __main__ -   lang2id = None
05/08/2022 20:33:38 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='de', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.de.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:33:38 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:33:38 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.pooler.dense.weight', 'qa_outputs.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'qa_outputs.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:33:51 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 27%|       | 13/48 [00:00<00:00, 118.19it/s] 52%|    | 25/48 [00:00<00:00, 92.82it/s]  73%|  | 35/48 [00:00<00:00, 82.63it/s] 96%|| 46/48 [00:00<00:00, 90.29it/s]100%|| 48/48 [00:00<00:00, 92.53it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<02:45,  7.20it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:05, 192.21it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:05, 217.62it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:03, 286.27it/s]convert squad examples to features:  19%|        | 225/1190 [00:00<00:03, 301.04it/s]convert squad examples to features:  22%|       | 257/1190 [00:00<00:03, 282.88it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:03, 268.38it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:04, 207.23it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:04, 201.70it/s]convert squad examples to features:  32%|      | 385/1190 [00:02<00:06, 128.29it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:05, 143.06it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:04, 150.84it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:04, 156.09it/s]convert squad examples to features:  43%|     | 513/1190 [00:02<00:03, 175.43it/s]convert squad examples to features:  46%|     | 545/1190 [00:03<00:04, 137.76it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:03, 188.15it/s]convert squad examples to features:  54%|    | 641/1190 [00:03<00:03, 182.46it/s]convert squad examples to features:  57%|    | 673/1190 [00:03<00:02, 174.27it/s]convert squad examples to features:  59%|    | 705/1190 [00:03<00:02, 188.35it/s]convert squad examples to features:  62%|   | 737/1190 [00:03<00:02, 198.72it/s]convert squad examples to features:  65%|   | 769/1190 [00:04<00:01, 212.94it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:01, 224.58it/s]convert squad examples to features:  70%|   | 833/1190 [00:04<00:01, 213.12it/s]convert squad examples to features:  73%|  | 865/1190 [00:04<00:01, 216.62it/s]convert squad examples to features:  75%|  | 897/1190 [00:04<00:01, 213.70it/s]convert squad examples to features:  78%|  | 929/1190 [00:04<00:01, 205.12it/s]convert squad examples to features:  81%|  | 961/1190 [00:04<00:01, 202.53it/s]convert squad examples to features:  86%| | 1025/1190 [00:05<00:00, 243.29it/s]convert squad examples to features:  89%| | 1057/1190 [00:05<00:00, 248.45it/s]convert squad examples to features:  92%|| 1089/1190 [00:05<00:00, 245.48it/s]convert squad examples to features:  94%|| 1121/1190 [00:05<00:00, 236.34it/s]convert squad examples to features:  97%|| 1153/1190 [00:05<00:00, 212.05it/s]convert squad examples to features: 100%|| 1190/1190 [00:05<00:00, 205.98it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 614077.48it/s]
05/08/2022 20:33:57 - INFO - __main__ -   Saving features into cached file ./cached_xquad.de.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_de
05/08/2022 20:33:59 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:33:59 - INFO - __main__ -     Num examples = 1303
05/08/2022 20:33:59 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]Evaluating:   1%|          | 1/163 [00:00<02:38,  1.02it/s]Evaluating:   1%|          | 2/163 [00:01<01:16,  2.10it/s]Evaluating:   2%|         | 3/163 [00:01<00:50,  3.19it/s]Evaluating:   2%|         | 4/163 [00:01<00:37,  4.24it/s]Evaluating:   3%|         | 5/163 [00:01<00:30,  5.19it/s]Evaluating:   4%|         | 6/163 [00:01<00:26,  5.99it/s]Evaluating:   4%|         | 7/163 [00:01<00:23,  6.58it/s]Evaluating:   5%|         | 8/163 [00:01<00:21,  7.05it/s]Evaluating:   6%|         | 9/163 [00:01<00:20,  7.43it/s]Evaluating:   6%|         | 10/163 [00:02<00:19,  7.71it/s]Evaluating:   7%|         | 11/163 [00:02<00:19,  7.93it/s]Evaluating:   7%|         | 12/163 [00:02<00:18,  8.07it/s]Evaluating:   8%|         | 13/163 [00:02<00:18,  8.17it/s]Evaluating:   9%|         | 14/163 [00:02<00:18,  8.26it/s]Evaluating:   9%|         | 15/163 [00:02<00:17,  8.33it/s]Evaluating:  10%|         | 16/163 [00:02<00:17,  8.38it/s]Evaluating:  10%|         | 17/163 [00:02<00:17,  8.39it/s]Evaluating:  11%|         | 18/163 [00:02<00:17,  8.43it/s]Evaluating:  12%|        | 19/163 [00:03<00:17,  8.46it/s]Evaluating:  12%|        | 20/163 [00:03<00:16,  8.47it/s]Evaluating:  13%|        | 21/163 [00:03<00:16,  8.48it/s]Evaluating:  13%|        | 22/163 [00:03<00:16,  8.49it/s]Evaluating:  14%|        | 23/163 [00:03<00:16,  8.48it/s]Evaluating:  15%|        | 24/163 [00:03<00:18,  7.69it/s]Evaluating:  15%|        | 25/163 [00:03<00:17,  7.91it/s]Evaluating:  16%|        | 26/163 [00:03<00:16,  8.08it/s]Evaluating:  17%|        | 27/163 [00:04<00:16,  8.17it/s]Evaluating:  17%|        | 28/163 [00:04<00:16,  8.24it/s]Evaluating:  18%|        | 29/163 [00:04<00:16,  8.29it/s]Evaluating:  18%|        | 30/163 [00:04<00:16,  8.26it/s]Evaluating:  19%|        | 31/163 [00:04<00:15,  8.33it/s]Evaluating:  20%|        | 32/163 [00:04<00:15,  8.37it/s]Evaluating:  20%|        | 33/163 [00:04<00:15,  8.43it/s]Evaluating:  21%|        | 34/163 [00:04<00:15,  8.44it/s]Evaluating:  21%|       | 35/163 [00:05<00:15,  8.46it/s]Evaluating:  22%|       | 36/163 [00:05<00:14,  8.47it/s]Evaluating:  23%|       | 37/163 [00:05<00:14,  8.45it/s]Evaluating:  23%|       | 38/163 [00:05<00:14,  8.47it/s]Evaluating:  24%|       | 39/163 [00:05<00:14,  8.45it/s]Evaluating:  25%|       | 40/163 [00:05<00:14,  8.42it/s]Evaluating:  25%|       | 41/163 [00:05<00:14,  8.32it/s]Evaluating:  26%|       | 42/163 [00:05<00:14,  8.28it/s]Evaluating:  26%|       | 43/163 [00:06<00:14,  8.36it/s]Evaluating:  27%|       | 44/163 [00:06<00:14,  8.40it/s]Evaluating:  28%|       | 45/163 [00:06<00:13,  8.44it/s]Evaluating:  28%|       | 46/163 [00:06<00:13,  8.47it/s]Evaluating:  29%|       | 47/163 [00:06<00:13,  8.46it/s]Evaluating:  29%|       | 48/163 [00:06<00:13,  8.45it/s]Evaluating:  30%|       | 49/163 [00:06<00:13,  8.46it/s]Evaluating:  31%|       | 50/163 [00:06<00:13,  8.29it/s]Evaluating:  31%|      | 51/163 [00:06<00:13,  8.30it/s]Evaluating:  32%|      | 52/163 [00:07<00:13,  8.34it/s]Evaluating:  33%|      | 53/163 [00:07<00:13,  8.32it/s]Evaluating:  33%|      | 54/163 [00:07<00:13,  8.30it/s]Evaluating:  34%|      | 55/163 [00:07<00:13,  8.30it/s]Evaluating:  34%|      | 56/163 [00:07<00:12,  8.31it/s]Evaluating:  35%|      | 57/163 [00:07<00:12,  8.34it/s]Evaluating:  36%|      | 58/163 [00:07<00:12,  8.32it/s]Evaluating:  36%|      | 59/163 [00:07<00:12,  8.33it/s]Evaluating:  37%|      | 60/163 [00:08<00:12,  8.35it/s]Evaluating:  37%|      | 61/163 [00:08<00:12,  8.39it/s]Evaluating:  38%|      | 62/163 [00:08<00:12,  8.32it/s]Evaluating:  39%|      | 63/163 [00:08<00:11,  8.37it/s]Evaluating:  39%|      | 64/163 [00:08<00:11,  8.40it/s]Evaluating:  40%|      | 65/163 [00:08<00:11,  8.36it/s]Evaluating:  40%|      | 66/163 [00:08<00:11,  8.41it/s]Evaluating:  41%|      | 67/163 [00:08<00:11,  8.37it/s]Evaluating:  42%|     | 68/163 [00:08<00:11,  8.35it/s]Evaluating:  42%|     | 69/163 [00:09<00:11,  8.33it/s]Evaluating:  43%|     | 70/163 [00:09<00:11,  8.31it/s]Evaluating:  44%|     | 71/163 [00:09<00:11,  8.30it/s]Evaluating:  44%|     | 72/163 [00:09<00:11,  8.26it/s]Evaluating:  45%|     | 73/163 [00:09<00:10,  8.25it/s]Evaluating:  45%|     | 74/163 [00:09<00:10,  8.26it/s]Evaluating:  46%|     | 75/163 [00:09<00:10,  8.31it/s]Evaluating:  47%|     | 76/163 [00:09<00:10,  8.32it/s]Evaluating:  47%|     | 77/163 [00:10<00:10,  8.34it/s]Evaluating:  48%|     | 78/163 [00:10<00:10,  8.37it/s]Evaluating:  48%|     | 79/163 [00:10<00:09,  8.41it/s]Evaluating:  49%|     | 80/163 [00:10<00:09,  8.39it/s]Evaluating:  50%|     | 81/163 [00:10<00:09,  8.40it/s]Evaluating:  50%|     | 82/163 [00:10<00:09,  8.42it/s]Evaluating:  51%|     | 83/163 [00:10<00:09,  8.20it/s]Evaluating:  52%|    | 84/163 [00:10<00:09,  8.16it/s]Evaluating:  52%|    | 85/163 [00:11<00:09,  8.19it/s]Evaluating:  53%|    | 86/163 [00:11<00:09,  8.25it/s]Evaluating:  53%|    | 87/163 [00:11<00:09,  8.29it/s]Evaluating:  54%|    | 88/163 [00:11<00:09,  8.27it/s]Evaluating:  55%|    | 89/163 [00:11<00:08,  8.30it/s]Evaluating:  55%|    | 90/163 [00:11<00:08,  8.30it/s]Evaluating:  56%|    | 91/163 [00:11<00:08,  8.29it/s]Evaluating:  56%|    | 92/163 [00:11<00:08,  8.21it/s]Evaluating:  57%|    | 93/163 [00:12<00:08,  8.24it/s]Evaluating:  58%|    | 94/163 [00:12<00:08,  8.24it/s]Evaluating:  58%|    | 95/163 [00:12<00:08,  8.23it/s]Evaluating:  59%|    | 96/163 [00:12<00:08,  8.25it/s]Evaluating:  60%|    | 97/163 [00:12<00:08,  8.21it/s]Evaluating:  60%|    | 98/163 [00:12<00:07,  8.23it/s]Evaluating:  61%|    | 99/163 [00:12<00:07,  8.24it/s]Evaluating:  61%|   | 100/163 [00:12<00:07,  8.26it/s]Evaluating:  62%|   | 101/163 [00:12<00:07,  8.25it/s]Evaluating:  63%|   | 102/163 [00:13<00:07,  8.31it/s]Evaluating:  63%|   | 103/163 [00:13<00:07,  8.33it/s]Evaluating:  64%|   | 104/163 [00:13<00:07,  8.33it/s]Evaluating:  64%|   | 105/163 [00:13<00:06,  8.33it/s]Evaluating:  65%|   | 106/163 [00:13<00:06,  8.31it/s]Evaluating:  66%|   | 107/163 [00:13<00:06,  8.29it/s]Evaluating:  66%|   | 108/163 [00:13<00:06,  8.35it/s]Evaluating:  67%|   | 109/163 [00:13<00:06,  8.36it/s]Evaluating:  67%|   | 110/163 [00:14<00:06,  8.35it/s]Evaluating:  68%|   | 111/163 [00:14<00:06,  8.36it/s]Evaluating:  69%|   | 112/163 [00:14<00:06,  8.33it/s]Evaluating:  69%|   | 113/163 [00:14<00:05,  8.34it/s]Evaluating:  70%|   | 114/163 [00:14<00:05,  8.31it/s]Evaluating:  71%|   | 115/163 [00:14<00:05,  8.33it/s]Evaluating:  71%|   | 116/163 [00:14<00:05,  8.35it/s]Evaluating:  72%|  | 117/163 [00:14<00:05,  8.29it/s]Evaluating:  72%|  | 118/163 [00:15<00:05,  8.30it/s]Evaluating:  73%|  | 119/163 [00:15<00:05,  8.28it/s]Evaluating:  74%|  | 120/163 [00:15<00:05,  8.32it/s]Evaluating:  74%|  | 121/163 [00:15<00:05,  8.33it/s]Evaluating:  75%|  | 122/163 [00:15<00:04,  8.36it/s]Evaluating:  75%|  | 123/163 [00:15<00:04,  8.38it/s]Evaluating:  76%|  | 124/163 [00:15<00:04,  8.32it/s]Evaluating:  77%|  | 125/163 [00:15<00:04,  8.29it/s]Evaluating:  77%|  | 126/163 [00:15<00:04,  8.27it/s]Evaluating:  78%|  | 127/163 [00:16<00:04,  8.25it/s]Evaluating:  79%|  | 128/163 [00:16<00:04,  8.23it/s]Evaluating:  79%|  | 129/163 [00:16<00:04,  8.21it/s]Evaluating:  80%|  | 130/163 [00:16<00:04,  8.23it/s]Evaluating:  80%|  | 131/163 [00:16<00:03,  8.22it/s]Evaluating:  81%|  | 132/163 [00:16<00:03,  8.26it/s]Evaluating:  82%| | 133/163 [00:16<00:03,  8.29it/s]Evaluating:  82%| | 134/163 [00:16<00:03,  8.28it/s]Evaluating:  83%| | 135/163 [00:17<00:03,  8.31it/s]Evaluating:  83%| | 136/163 [00:17<00:03,  8.31it/s]Evaluating:  84%| | 137/163 [00:17<00:03,  8.34it/s]Evaluating:  85%| | 138/163 [00:17<00:03,  8.32it/s]Evaluating:  85%| | 139/163 [00:17<00:02,  8.34it/s]Evaluating:  86%| | 140/163 [00:17<00:02,  8.35it/s]Evaluating:  87%| | 141/163 [00:17<00:02,  8.36it/s]Evaluating:  87%| | 142/163 [00:17<00:02,  8.39it/s]Evaluating:  88%| | 143/163 [00:18<00:02,  8.41it/s]Evaluating:  88%| | 144/163 [00:18<00:02,  8.40it/s]Evaluating:  89%| | 145/163 [00:18<00:02,  8.42it/s]Evaluating:  90%| | 146/163 [00:18<00:02,  8.40it/s]Evaluating:  90%| | 147/163 [00:18<00:01,  8.43it/s]Evaluating:  91%| | 148/163 [00:18<00:01,  8.39it/s]Evaluating:  91%|| 149/163 [00:18<00:01,  8.41it/s]Evaluating:  92%|| 150/163 [00:18<00:01,  8.43it/s]Evaluating:  93%|| 151/163 [00:18<00:01,  8.45it/s]Evaluating:  93%|| 152/163 [00:19<00:01,  8.46it/s]Evaluating:  94%|| 153/163 [00:19<00:01,  8.47it/s]Evaluating:  94%|| 154/163 [00:19<00:01,  8.39it/s]Evaluating:  95%|| 155/163 [00:19<00:00,  8.39it/s]Evaluating:  96%|| 156/163 [00:19<00:00,  8.40it/s]Evaluating:  96%|| 157/163 [00:19<00:00,  8.40it/s]Evaluating:  97%|| 158/163 [00:19<00:00,  8.40it/s]Evaluating:  98%|| 159/163 [00:19<00:00,  8.39it/s]Evaluating:  98%|| 160/163 [00:20<00:00,  8.39it/s]Evaluating:  99%|| 161/163 [00:20<00:00,  8.39it/s]Evaluating:  99%|| 162/163 [00:20<00:00,  8.40it/s]Evaluating: 100%|| 163/163 [00:20<00:00,  8.70it/s]Evaluating: 100%|| 163/163 [00:20<00:00,  7.99it/s]
05/08/2022 20:34:19 - INFO - __main__ -     Evaluation done in total 20.391130 secs (0.015649 sec per example)
05/08/2022 20:34:23 - INFO - __main__ -   Results: {'exact': 58.739495798319325, 'f1': 74.83263548550438, 'total': 1190, 'HasAns_exact': 58.739495798319325, 'HasAns_f1': 74.83263548550438, 'HasAns_total': 1190, 'best_exact': 58.739495798319325, 'best_exact_thresh': 0.0, 'best_f1': 74.83263548550438, 'best_f1_thresh': 0.0}
  el 
2022-05-08 20:34:25.519378: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:34:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'qa_outputs.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.word_embeddings.weight', 'qa_outputs.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:34:38 - INFO - __main__ -   lang2id = None
05/08/2022 20:34:41 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='el', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.el.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:34:41 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:34:41 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'qa_outputs.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.word_embeddings.weight', 'qa_outputs.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:34:55 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 19%|        | 9/48 [00:00<00:00, 89.39it/s] 38%|      | 18/48 [00:00<00:00, 77.94it/s] 56%|    | 27/48 [00:00<00:00, 80.13it/s] 75%|  | 36/48 [00:00<00:00, 74.45it/s] 96%|| 46/48 [00:00<00:00, 82.49it/s]100%|| 48/48 [00:00<00:00, 82.32it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<04:20,  4.56it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:06, 161.45it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:06, 181.02it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:04, 236.62it/s]convert squad examples to features:  16%|        | 193/1190 [00:00<00:03, 254.44it/s]convert squad examples to features:  19%|        | 225/1190 [00:01<00:03, 251.82it/s]convert squad examples to features:  22%|       | 257/1190 [00:01<00:04, 220.93it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:04, 220.68it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:04, 206.24it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:03, 216.39it/s]convert squad examples to features:  32%|      | 385/1190 [00:02<00:07, 105.58it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:06, 112.80it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:05, 123.71it/s]convert squad examples to features:  40%|      | 481/1190 [00:03<00:06, 110.72it/s]convert squad examples to features:  46%|     | 545/1190 [00:03<00:04, 143.68it/s]convert squad examples to features:  48%|     | 577/1190 [00:03<00:03, 154.32it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:03, 154.85it/s]convert squad examples to features:  54%|    | 641/1190 [00:04<00:03, 154.72it/s]convert squad examples to features:  57%|    | 673/1190 [00:04<00:03, 146.60it/s]convert squad examples to features:  59%|    | 705/1190 [00:04<00:03, 158.36it/s]convert squad examples to features:  62%|   | 737/1190 [00:04<00:02, 172.89it/s]convert squad examples to features:  65%|   | 769/1190 [00:04<00:02, 183.61it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:02, 188.31it/s]convert squad examples to features:  70%|   | 833/1190 [00:05<00:02, 163.92it/s]convert squad examples to features:  73%|  | 865/1190 [00:05<00:01, 174.08it/s]convert squad examples to features:  75%|  | 897/1190 [00:05<00:01, 156.99it/s]convert squad examples to features:  78%|  | 929/1190 [00:05<00:01, 152.72it/s]convert squad examples to features:  81%|  | 961/1190 [00:05<00:01, 163.52it/s]convert squad examples to features:  83%| | 993/1190 [00:06<00:01, 180.40it/s]convert squad examples to features:  86%| | 1025/1190 [00:06<00:00, 190.42it/s]convert squad examples to features:  89%| | 1057/1190 [00:06<00:00, 191.93it/s]convert squad examples to features:  92%|| 1089/1190 [00:06<00:00, 203.65it/s]convert squad examples to features:  94%|| 1121/1190 [00:06<00:00, 188.47it/s]convert squad examples to features:  97%|| 1153/1190 [00:06<00:00, 163.64it/s]convert squad examples to features: 100%|| 1190/1190 [00:06<00:00, 170.72it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 466382.15it/s]
05/08/2022 20:35:02 - INFO - __main__ -   Saving features into cached file ./cached_xquad.el.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_el
05/08/2022 20:35:04 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:35:04 - INFO - __main__ -     Num examples = 1488
05/08/2022 20:35:04 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/186 [00:00<?, ?it/s]Evaluating:   1%|          | 1/186 [00:00<02:01,  1.52it/s]Evaluating:   1%|          | 2/186 [00:00<01:03,  2.89it/s]Evaluating:   2%|         | 3/186 [00:00<00:44,  4.13it/s]Evaluating:   2%|         | 4/186 [00:01<00:35,  5.17it/s]Evaluating:   3%|         | 5/186 [00:01<00:30,  6.02it/s]Evaluating:   3%|         | 6/186 [00:01<00:26,  6.69it/s]Evaluating:   4%|         | 7/186 [00:01<00:24,  7.18it/s]Evaluating:   4%|         | 8/186 [00:01<00:23,  7.47it/s]Evaluating:   5%|         | 9/186 [00:01<00:22,  7.75it/s]Evaluating:   5%|         | 10/186 [00:01<00:22,  7.93it/s]Evaluating:   6%|         | 11/186 [00:01<00:21,  8.04it/s]Evaluating:   6%|         | 12/186 [00:01<00:21,  8.16it/s]Evaluating:   7%|         | 13/186 [00:02<00:21,  8.20it/s]Evaluating:   8%|         | 14/186 [00:02<00:20,  8.30it/s]Evaluating:   8%|         | 15/186 [00:02<00:20,  8.37it/s]Evaluating:   9%|         | 16/186 [00:02<00:20,  8.35it/s]Evaluating:   9%|         | 17/186 [00:02<00:20,  8.41it/s]Evaluating:  10%|         | 18/186 [00:02<00:19,  8.41it/s]Evaluating:  10%|         | 19/186 [00:02<00:19,  8.37it/s]Evaluating:  11%|         | 20/186 [00:02<00:19,  8.42it/s]Evaluating:  11%|        | 21/186 [00:03<00:19,  8.45it/s]Evaluating:  12%|        | 22/186 [00:03<00:19,  8.49it/s]Evaluating:  12%|        | 23/186 [00:03<00:19,  8.51it/s]Evaluating:  13%|        | 24/186 [00:03<00:18,  8.54it/s]Evaluating:  13%|        | 25/186 [00:03<00:18,  8.52it/s]Evaluating:  14%|        | 26/186 [00:03<00:18,  8.51it/s]Evaluating:  15%|        | 27/186 [00:03<00:18,  8.52it/s]Evaluating:  15%|        | 28/186 [00:03<00:18,  8.51it/s]Evaluating:  16%|        | 29/186 [00:03<00:18,  8.52it/s]Evaluating:  16%|        | 30/186 [00:04<00:18,  8.53it/s]Evaluating:  17%|        | 31/186 [00:04<00:18,  8.54it/s]Evaluating:  17%|        | 32/186 [00:04<00:18,  8.53it/s]Evaluating:  18%|        | 33/186 [00:04<00:17,  8.53it/s]Evaluating:  18%|        | 34/186 [00:04<00:17,  8.53it/s]Evaluating:  19%|        | 35/186 [00:04<00:17,  8.53it/s]Evaluating:  19%|        | 36/186 [00:04<00:17,  8.48it/s]Evaluating:  20%|        | 37/186 [00:04<00:17,  8.48it/s]Evaluating:  20%|        | 38/186 [00:05<00:17,  8.46it/s]Evaluating:  21%|        | 39/186 [00:05<00:17,  8.42it/s]Evaluating:  22%|       | 40/186 [00:05<00:17,  8.33it/s]Evaluating:  22%|       | 41/186 [00:05<00:17,  8.32it/s]Evaluating:  23%|       | 42/186 [00:05<00:17,  8.38it/s]Evaluating:  23%|       | 43/186 [00:05<00:16,  8.42it/s]Evaluating:  24%|       | 44/186 [00:05<00:16,  8.43it/s]Evaluating:  24%|       | 45/186 [00:05<00:16,  8.42it/s]Evaluating:  25%|       | 46/186 [00:05<00:16,  8.41it/s]Evaluating:  25%|       | 47/186 [00:06<00:16,  8.41it/s]Evaluating:  26%|       | 48/186 [00:06<00:16,  8.42it/s]Evaluating:  26%|       | 49/186 [00:06<00:16,  8.45it/s]Evaluating:  27%|       | 50/186 [00:06<00:16,  8.46it/s]Evaluating:  27%|       | 51/186 [00:06<00:15,  8.45it/s]Evaluating:  28%|       | 52/186 [00:06<00:15,  8.41it/s]Evaluating:  28%|       | 53/186 [00:06<00:15,  8.37it/s]Evaluating:  29%|       | 54/186 [00:06<00:15,  8.35it/s]Evaluating:  30%|       | 55/186 [00:07<00:15,  8.23it/s]Evaluating:  30%|       | 56/186 [00:07<00:15,  8.28it/s]Evaluating:  31%|       | 57/186 [00:07<00:15,  8.29it/s]Evaluating:  31%|       | 58/186 [00:07<00:15,  8.32it/s]Evaluating:  32%|      | 59/186 [00:07<00:15,  8.35it/s]Evaluating:  32%|      | 60/186 [00:07<00:15,  8.31it/s]Evaluating:  33%|      | 61/186 [00:07<00:15,  8.27it/s]Evaluating:  33%|      | 62/186 [00:07<00:14,  8.27it/s]Evaluating:  34%|      | 63/186 [00:08<00:14,  8.23it/s]Evaluating:  34%|      | 64/186 [00:08<00:15,  8.12it/s]Evaluating:  35%|      | 65/186 [00:08<00:14,  8.12it/s]Evaluating:  35%|      | 66/186 [00:08<00:14,  8.14it/s]Evaluating:  36%|      | 67/186 [00:08<00:14,  8.20it/s]Evaluating:  37%|      | 68/186 [00:08<00:14,  8.22it/s]Evaluating:  37%|      | 69/186 [00:08<00:14,  8.20it/s]Evaluating:  38%|      | 70/186 [00:08<00:14,  8.15it/s]Evaluating:  38%|      | 71/186 [00:09<00:14,  8.14it/s]Evaluating:  39%|      | 72/186 [00:09<00:13,  8.14it/s]Evaluating:  39%|      | 73/186 [00:09<00:13,  8.20it/s]Evaluating:  40%|      | 74/186 [00:09<00:13,  8.26it/s]Evaluating:  40%|      | 75/186 [00:09<00:13,  8.30it/s]Evaluating:  41%|      | 76/186 [00:09<00:13,  8.30it/s]Evaluating:  41%|     | 77/186 [00:09<00:13,  8.30it/s]Evaluating:  42%|     | 78/186 [00:09<00:12,  8.32it/s]Evaluating:  42%|     | 79/186 [00:09<00:12,  8.32it/s]Evaluating:  43%|     | 80/186 [00:10<00:12,  8.36it/s]Evaluating:  44%|     | 81/186 [00:10<00:12,  8.38it/s]Evaluating:  44%|     | 82/186 [00:10<00:12,  8.41it/s]Evaluating:  45%|     | 83/186 [00:10<00:12,  8.38it/s]Evaluating:  45%|     | 84/186 [00:10<00:12,  8.39it/s]Evaluating:  46%|     | 85/186 [00:10<00:12,  8.38it/s]Evaluating:  46%|     | 86/186 [00:10<00:11,  8.41it/s]Evaluating:  47%|     | 87/186 [00:10<00:11,  8.38it/s]Evaluating:  47%|     | 88/186 [00:11<00:11,  8.34it/s]Evaluating:  48%|     | 89/186 [00:11<00:11,  8.30it/s]Evaluating:  48%|     | 90/186 [00:11<00:11,  8.19it/s]Evaluating:  49%|     | 91/186 [00:11<00:11,  8.24it/s]Evaluating:  49%|     | 92/186 [00:11<00:11,  8.29it/s]Evaluating:  50%|     | 93/186 [00:11<00:11,  8.27it/s]Evaluating:  51%|     | 94/186 [00:11<00:11,  8.30it/s]Evaluating:  51%|     | 95/186 [00:11<00:10,  8.31it/s]Evaluating:  52%|    | 96/186 [00:12<00:10,  8.36it/s]Evaluating:  52%|    | 97/186 [00:12<00:10,  8.37it/s]Evaluating:  53%|    | 98/186 [00:12<00:10,  8.38it/s]Evaluating:  53%|    | 99/186 [00:12<00:10,  8.28it/s]Evaluating:  54%|    | 100/186 [00:12<00:10,  8.26it/s]Evaluating:  54%|    | 101/186 [00:12<00:10,  8.31it/s]Evaluating:  55%|    | 102/186 [00:12<00:10,  8.34it/s]Evaluating:  55%|    | 103/186 [00:12<00:09,  8.36it/s]Evaluating:  56%|    | 104/186 [00:12<00:09,  8.36it/s]Evaluating:  56%|    | 105/186 [00:13<00:09,  8.27it/s]Evaluating:  57%|    | 106/186 [00:13<00:09,  8.23it/s]Evaluating:  58%|    | 107/186 [00:13<00:09,  8.28it/s]Evaluating:  58%|    | 108/186 [00:13<00:09,  8.30it/s]Evaluating:  59%|    | 109/186 [00:13<00:09,  8.30it/s]Evaluating:  59%|    | 110/186 [00:13<00:09,  8.27it/s]Evaluating:  60%|    | 111/186 [00:13<00:09,  8.30it/s]Evaluating:  60%|    | 112/186 [00:13<00:08,  8.29it/s]Evaluating:  61%|    | 113/186 [00:14<00:08,  8.29it/s]Evaluating:  61%|   | 114/186 [00:14<00:08,  8.18it/s]Evaluating:  62%|   | 115/186 [00:14<00:08,  8.21it/s]Evaluating:  62%|   | 116/186 [00:14<00:08,  8.28it/s]Evaluating:  63%|   | 117/186 [00:14<00:08,  8.31it/s]Evaluating:  63%|   | 118/186 [00:14<00:08,  8.34it/s]Evaluating:  64%|   | 119/186 [00:14<00:08,  8.36it/s]Evaluating:  65%|   | 120/186 [00:14<00:07,  8.32it/s]Evaluating:  65%|   | 121/186 [00:15<00:07,  8.20it/s]Evaluating:  66%|   | 122/186 [00:15<00:07,  8.25it/s]Evaluating:  66%|   | 123/186 [00:15<00:07,  8.31it/s]Evaluating:  67%|   | 124/186 [00:15<00:07,  8.34it/s]Evaluating:  67%|   | 125/186 [00:15<00:07,  8.38it/s]Evaluating:  68%|   | 126/186 [00:15<00:07,  8.33it/s]Evaluating:  68%|   | 127/186 [00:15<00:07,  8.24it/s]Evaluating:  69%|   | 128/186 [00:15<00:07,  8.22it/s]Evaluating:  69%|   | 129/186 [00:16<00:06,  8.20it/s]Evaluating:  70%|   | 130/186 [00:16<00:06,  8.24it/s]Evaluating:  70%|   | 131/186 [00:16<00:06,  8.30it/s]Evaluating:  71%|   | 132/186 [00:16<00:06,  8.34it/s]Evaluating:  72%|  | 133/186 [00:16<00:06,  8.36it/s]Evaluating:  72%|  | 134/186 [00:16<00:06,  8.36it/s]Evaluating:  73%|  | 135/186 [00:16<00:06,  8.35it/s]Evaluating:  73%|  | 136/186 [00:16<00:05,  8.34it/s]Evaluating:  74%|  | 137/186 [00:16<00:05,  8.32it/s]Evaluating:  74%|  | 138/186 [00:17<00:05,  8.28it/s]Evaluating:  75%|  | 139/186 [00:17<00:05,  8.31it/s]Evaluating:  75%|  | 140/186 [00:17<00:05,  8.35it/s]Evaluating:  76%|  | 141/186 [00:17<00:05,  8.37it/s]Evaluating:  76%|  | 142/186 [00:17<00:05,  8.40it/s]Evaluating:  77%|  | 143/186 [00:17<00:05,  8.42it/s]Evaluating:  77%|  | 144/186 [00:17<00:04,  8.42it/s]Evaluating:  78%|  | 145/186 [00:17<00:04,  8.41it/s]Evaluating:  78%|  | 146/186 [00:18<00:04,  8.40it/s]Evaluating:  79%|  | 147/186 [00:18<00:04,  8.40it/s]Evaluating:  80%|  | 148/186 [00:18<00:04,  8.36it/s]Evaluating:  80%|  | 149/186 [00:18<00:04,  8.36it/s]Evaluating:  81%|  | 150/186 [00:18<00:04,  8.36it/s]Evaluating:  81%|  | 151/186 [00:18<00:04,  8.38it/s]Evaluating:  82%| | 152/186 [00:18<00:04,  8.39it/s]Evaluating:  82%| | 153/186 [00:18<00:03,  8.36it/s]Evaluating:  83%| | 154/186 [00:18<00:03,  8.39it/s]Evaluating:  83%| | 155/186 [00:19<00:03,  8.42it/s]Evaluating:  84%| | 156/186 [00:19<00:03,  8.39it/s]Evaluating:  84%| | 157/186 [00:19<00:03,  8.41it/s]Evaluating:  85%| | 158/186 [00:19<00:03,  8.40it/s]Evaluating:  85%| | 159/186 [00:19<00:03,  8.39it/s]Evaluating:  86%| | 160/186 [00:19<00:03,  8.03it/s]Evaluating:  87%| | 161/186 [00:19<00:03,  8.16it/s]Evaluating:  87%| | 162/186 [00:19<00:02,  8.25it/s]Evaluating:  88%| | 163/186 [00:20<00:02,  8.26it/s]Evaluating:  88%| | 164/186 [00:20<00:02,  8.31it/s]Evaluating:  89%| | 165/186 [00:20<00:02,  8.35it/s]Evaluating:  89%| | 166/186 [00:20<00:02,  8.39it/s]Evaluating:  90%| | 167/186 [00:20<00:02,  8.39it/s]Evaluating:  90%| | 168/186 [00:20<00:02,  8.41it/s]Evaluating:  91%| | 169/186 [00:20<00:02,  8.34it/s]Evaluating:  91%|| 170/186 [00:20<00:01,  8.37it/s]Evaluating:  92%|| 171/186 [00:21<00:01,  8.40it/s]Evaluating:  92%|| 172/186 [00:21<00:01,  8.42it/s]Evaluating:  93%|| 173/186 [00:21<00:01,  8.44it/s]Evaluating:  94%|| 174/186 [00:21<00:01,  8.44it/s]Evaluating:  94%|| 175/186 [00:21<00:01,  8.43it/s]Evaluating:  95%|| 176/186 [00:21<00:01,  8.38it/s]Evaluating:  95%|| 177/186 [00:21<00:01,  8.39it/s]Evaluating:  96%|| 178/186 [00:21<00:00,  8.39it/s]Evaluating:  96%|| 179/186 [00:21<00:00,  8.38it/s]Evaluating:  97%|| 180/186 [00:22<00:00,  8.36it/s]Evaluating:  97%|| 181/186 [00:22<00:00,  8.39it/s]Evaluating:  98%|| 182/186 [00:22<00:00,  8.36it/s]Evaluating:  98%|| 183/186 [00:22<00:00,  8.29it/s]Evaluating:  99%|| 184/186 [00:22<00:00,  8.19it/s]Evaluating:  99%|| 185/186 [00:22<00:00,  8.25it/s]Evaluating: 100%|| 186/186 [00:22<00:00,  8.31it/s]Evaluating: 100%|| 186/186 [00:22<00:00,  8.15it/s]
05/08/2022 20:35:27 - INFO - __main__ -     Evaluation done in total 22.826600 secs (0.015340 sec per example)
05/08/2022 20:35:31 - INFO - __main__ -   Results: {'exact': 55.04201680672269, 'f1': 72.72299572868003, 'total': 1190, 'HasAns_exact': 55.04201680672269, 'HasAns_f1': 72.72299572868003, 'HasAns_total': 1190, 'best_exact': 55.04201680672269, 'best_exact_thresh': 0.0, 'best_f1': 72.72299572868003, 'best_f1_thresh': 0.0}
  ru 
2022-05-08 20:35:34.015847: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:35:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'qa_outputs.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:35:47 - INFO - __main__ -   lang2id = None
05/08/2022 20:35:50 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='ru', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.ru.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:35:50 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:35:50 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'qa_outputs.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.output.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:36:03 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 27%|       | 13/48 [00:00<00:00, 128.73it/s] 54%|    | 26/48 [00:00<00:00, 90.11it/s]  77%|  | 37/48 [00:00<00:00, 94.52it/s]100%|| 48/48 [00:00<00:00, 103.63it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<03:36,  5.48it/s]convert squad examples to features:   3%|         | 33/1190 [00:00<00:08, 142.45it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:06, 177.90it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:05, 197.86it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:03, 268.46it/s]convert squad examples to features:  16%|        | 193/1190 [00:00<00:03, 274.74it/s]convert squad examples to features:  22%|       | 257/1190 [00:01<00:03, 272.71it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:03, 268.36it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:03, 256.64it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:03, 258.07it/s]convert squad examples to features:  32%|      | 385/1190 [00:02<00:05, 134.78it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:05, 152.43it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:04, 165.57it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:04, 174.54it/s]convert squad examples to features:  43%|     | 513/1190 [00:02<00:04, 152.10it/s]convert squad examples to features:  48%|     | 577/1190 [00:02<00:02, 208.88it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:02, 208.55it/s]convert squad examples to features:  54%|    | 641/1190 [00:03<00:02, 202.15it/s]convert squad examples to features:  57%|    | 673/1190 [00:03<00:02, 193.54it/s]convert squad examples to features:  59%|    | 705/1190 [00:03<00:02, 195.88it/s]convert squad examples to features:  62%|   | 737/1190 [00:03<00:02, 204.53it/s]convert squad examples to features:  65%|   | 769/1190 [00:03<00:02, 194.84it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:01, 212.92it/s]convert squad examples to features:  70%|   | 833/1190 [00:04<00:01, 181.27it/s]convert squad examples to features:  73%|  | 865/1190 [00:04<00:01, 201.28it/s]convert squad examples to features:  75%|  | 897/1190 [00:04<00:01, 180.01it/s]convert squad examples to features:  78%|  | 929/1190 [00:04<00:01, 182.35it/s]convert squad examples to features:  81%|  | 961/1190 [00:04<00:01, 201.29it/s]convert squad examples to features:  83%| | 993/1190 [00:05<00:00, 212.01it/s]convert squad examples to features:  86%| | 1025/1190 [00:05<00:00, 233.30it/s]convert squad examples to features:  89%| | 1057/1190 [00:05<00:00, 252.24it/s]convert squad examples to features:  92%|| 1089/1190 [00:05<00:00, 264.27it/s]convert squad examples to features:  94%|| 1121/1190 [00:05<00:00, 253.63it/s]convert squad examples to features:  97%|| 1153/1190 [00:05<00:00, 218.78it/s]convert squad examples to features: 100%|| 1190/1190 [00:05<00:00, 209.38it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 590677.13it/s]
05/08/2022 20:36:10 - INFO - __main__ -   Saving features into cached file ./cached_xquad.ru.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_ru
05/08/2022 20:36:11 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:36:11 - INFO - __main__ -     Num examples = 1332
05/08/2022 20:36:11 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/167 [00:00<?, ?it/s]Evaluating:   1%|          | 1/167 [00:00<02:41,  1.03it/s]Evaluating:   1%|          | 2/167 [00:01<01:18,  2.09it/s]Evaluating:   2%|         | 3/167 [00:01<00:51,  3.17it/s]Evaluating:   2%|         | 4/167 [00:01<00:38,  4.20it/s]Evaluating:   3%|         | 5/167 [00:01<00:31,  5.14it/s]Evaluating:   4%|         | 6/167 [00:01<00:27,  5.94it/s]Evaluating:   4%|         | 7/167 [00:01<00:24,  6.56it/s]Evaluating:   5%|         | 8/167 [00:01<00:22,  7.07it/s]Evaluating:   5%|         | 9/167 [00:01<00:21,  7.43it/s]Evaluating:   6%|         | 10/167 [00:02<00:21,  7.47it/s]Evaluating:   7%|         | 11/167 [00:02<00:20,  7.76it/s]Evaluating:   7%|         | 12/167 [00:02<00:19,  7.98it/s]Evaluating:   8%|         | 13/167 [00:02<00:19,  8.09it/s]Evaluating:   8%|         | 14/167 [00:02<00:18,  8.11it/s]Evaluating:   9%|         | 15/167 [00:02<00:18,  8.17it/s]Evaluating:  10%|         | 16/167 [00:02<00:18,  8.08it/s]Evaluating:  10%|         | 17/167 [00:02<00:18,  8.07it/s]Evaluating:  11%|         | 18/167 [00:03<00:18,  8.20it/s]Evaluating:  11%|        | 19/167 [00:03<00:17,  8.28it/s]Evaluating:  12%|        | 20/167 [00:03<00:17,  8.34it/s]Evaluating:  13%|        | 21/167 [00:03<00:17,  8.37it/s]Evaluating:  13%|        | 22/167 [00:03<00:17,  8.34it/s]Evaluating:  14%|        | 23/167 [00:03<00:17,  8.39it/s]Evaluating:  14%|        | 24/167 [00:03<00:17,  8.39it/s]Evaluating:  15%|        | 25/167 [00:03<00:16,  8.36it/s]Evaluating:  16%|        | 26/167 [00:03<00:16,  8.34it/s]Evaluating:  16%|        | 27/167 [00:04<00:16,  8.31it/s]Evaluating:  17%|        | 28/167 [00:04<00:16,  8.31it/s]Evaluating:  17%|        | 29/167 [00:04<00:16,  8.34it/s]Evaluating:  18%|        | 30/167 [00:04<00:16,  8.34it/s]Evaluating:  19%|        | 31/167 [00:04<00:16,  8.37it/s]Evaluating:  19%|        | 32/167 [00:04<00:16,  8.41it/s]Evaluating:  20%|        | 33/167 [00:04<00:15,  8.43it/s]Evaluating:  20%|        | 34/167 [00:04<00:15,  8.46it/s]Evaluating:  21%|        | 35/167 [00:05<00:15,  8.48it/s]Evaluating:  22%|       | 36/167 [00:05<00:15,  8.49it/s]Evaluating:  22%|       | 37/167 [00:05<00:15,  8.41it/s]Evaluating:  23%|       | 38/167 [00:05<00:15,  8.41it/s]Evaluating:  23%|       | 39/167 [00:05<00:15,  8.36it/s]Evaluating:  24%|       | 40/167 [00:05<00:15,  8.37it/s]Evaluating:  25%|       | 41/167 [00:05<00:14,  8.41it/s]Evaluating:  25%|       | 42/167 [00:05<00:14,  8.43it/s]Evaluating:  26%|       | 43/167 [00:06<00:14,  8.44it/s]Evaluating:  26%|       | 44/167 [00:06<00:14,  8.45it/s]Evaluating:  27%|       | 45/167 [00:06<00:14,  8.43it/s]Evaluating:  28%|       | 46/167 [00:06<00:14,  8.42it/s]Evaluating:  28%|       | 47/167 [00:06<00:14,  8.41it/s]Evaluating:  29%|       | 48/167 [00:06<00:14,  8.45it/s]Evaluating:  29%|       | 49/167 [00:06<00:13,  8.47it/s]Evaluating:  30%|       | 50/167 [00:06<00:13,  8.48it/s]Evaluating:  31%|       | 51/167 [00:06<00:13,  8.49it/s]Evaluating:  31%|       | 52/167 [00:07<00:13,  8.47it/s]Evaluating:  32%|      | 53/167 [00:07<00:13,  8.47it/s]Evaluating:  32%|      | 54/167 [00:07<00:13,  8.45it/s]Evaluating:  33%|      | 55/167 [00:07<00:13,  8.36it/s]Evaluating:  34%|      | 56/167 [00:07<00:13,  8.26it/s]Evaluating:  34%|      | 57/167 [00:07<00:13,  8.21it/s]Evaluating:  35%|      | 58/167 [00:07<00:13,  8.22it/s]Evaluating:  35%|      | 59/167 [00:07<00:13,  8.20it/s]Evaluating:  36%|      | 60/167 [00:08<00:13,  8.20it/s]Evaluating:  37%|      | 61/167 [00:08<00:12,  8.22it/s]Evaluating:  37%|      | 62/167 [00:08<00:12,  8.24it/s]Evaluating:  38%|      | 63/167 [00:08<00:12,  8.23it/s]Evaluating:  38%|      | 64/167 [00:08<00:12,  8.24it/s]Evaluating:  39%|      | 65/167 [00:08<00:12,  8.27it/s]Evaluating:  40%|      | 66/167 [00:08<00:12,  8.26it/s]Evaluating:  40%|      | 67/167 [00:08<00:12,  8.30it/s]Evaluating:  41%|      | 68/167 [00:09<00:11,  8.31it/s]Evaluating:  41%|     | 69/167 [00:09<00:11,  8.26it/s]Evaluating:  42%|     | 70/167 [00:09<00:11,  8.26it/s]Evaluating:  43%|     | 71/167 [00:09<00:11,  8.30it/s]Evaluating:  43%|     | 72/167 [00:09<00:11,  8.31it/s]Evaluating:  44%|     | 73/167 [00:09<00:11,  8.28it/s]Evaluating:  44%|     | 74/167 [00:09<00:11,  8.21it/s]Evaluating:  45%|     | 75/167 [00:09<00:11,  8.25it/s]Evaluating:  46%|     | 76/167 [00:09<00:10,  8.28it/s]Evaluating:  46%|     | 77/167 [00:10<00:10,  8.31it/s]Evaluating:  47%|     | 78/167 [00:10<00:10,  8.35it/s]Evaluating:  47%|     | 79/167 [00:10<00:10,  8.33it/s]Evaluating:  48%|     | 80/167 [00:10<00:10,  8.29it/s]Evaluating:  49%|     | 81/167 [00:10<00:10,  8.32it/s]Evaluating:  49%|     | 82/167 [00:10<00:10,  8.29it/s]Evaluating:  50%|     | 83/167 [00:10<00:10,  8.34it/s]Evaluating:  50%|     | 84/167 [00:10<00:09,  8.37it/s]Evaluating:  51%|     | 85/167 [00:11<00:09,  8.39it/s]Evaluating:  51%|    | 86/167 [00:11<00:09,  8.39it/s]Evaluating:  52%|    | 87/167 [00:11<00:09,  8.39it/s]Evaluating:  53%|    | 88/167 [00:11<00:09,  8.40it/s]Evaluating:  53%|    | 89/167 [00:11<00:09,  8.40it/s]Evaluating:  54%|    | 90/167 [00:11<00:09,  8.36it/s]Evaluating:  54%|    | 91/167 [00:11<00:09,  8.37it/s]Evaluating:  55%|    | 92/167 [00:11<00:08,  8.34it/s]Evaluating:  56%|    | 93/167 [00:12<00:08,  8.38it/s]Evaluating:  56%|    | 94/167 [00:12<00:08,  8.38it/s]Evaluating:  57%|    | 95/167 [00:12<00:08,  8.28it/s]Evaluating:  57%|    | 96/167 [00:12<00:08,  8.32it/s]Evaluating:  58%|    | 97/167 [00:12<00:08,  8.24it/s]Evaluating:  59%|    | 98/167 [00:12<00:08,  8.27it/s]Evaluating:  59%|    | 99/167 [00:12<00:08,  8.28it/s]Evaluating:  60%|    | 100/167 [00:12<00:08,  8.33it/s]Evaluating:  60%|    | 101/167 [00:12<00:07,  8.38it/s]Evaluating:  61%|    | 102/167 [00:13<00:07,  8.41it/s]Evaluating:  62%|   | 103/167 [00:13<00:07,  8.39it/s]Evaluating:  62%|   | 104/167 [00:13<00:07,  8.41it/s]Evaluating:  63%|   | 105/167 [00:13<00:07,  8.42it/s]Evaluating:  63%|   | 106/167 [00:13<00:07,  8.43it/s]Evaluating:  64%|   | 107/167 [00:13<00:07,  8.40it/s]Evaluating:  65%|   | 108/167 [00:13<00:07,  8.41it/s]Evaluating:  65%|   | 109/167 [00:13<00:06,  8.37it/s]Evaluating:  66%|   | 110/167 [00:14<00:06,  8.40it/s]Evaluating:  66%|   | 111/167 [00:14<00:06,  8.41it/s]Evaluating:  67%|   | 112/167 [00:14<00:06,  8.42it/s]Evaluating:  68%|   | 113/167 [00:14<00:06,  8.34it/s]Evaluating:  68%|   | 114/167 [00:14<00:06,  8.16it/s]Evaluating:  69%|   | 115/167 [00:14<00:06,  8.20it/s]Evaluating:  69%|   | 116/167 [00:14<00:06,  8.23it/s]Evaluating:  70%|   | 117/167 [00:14<00:06,  8.28it/s]Evaluating:  71%|   | 118/167 [00:15<00:05,  8.33it/s]Evaluating:  71%|  | 119/167 [00:15<00:05,  8.34it/s]Evaluating:  72%|  | 120/167 [00:15<00:05,  8.28it/s]Evaluating:  72%|  | 121/167 [00:15<00:05,  8.23it/s]Evaluating:  73%|  | 122/167 [00:15<00:05,  8.27it/s]Evaluating:  74%|  | 123/167 [00:15<00:05,  8.33it/s]Evaluating:  74%|  | 124/167 [00:15<00:05,  8.36it/s]Evaluating:  75%|  | 125/167 [00:15<00:05,  8.27it/s]Evaluating:  75%|  | 126/167 [00:15<00:04,  8.20it/s]Evaluating:  76%|  | 127/167 [00:16<00:04,  8.17it/s]Evaluating:  77%|  | 128/167 [00:16<00:04,  8.13it/s]Evaluating:  77%|  | 129/167 [00:16<00:04,  8.09it/s]Evaluating:  78%|  | 130/167 [00:16<00:04,  8.08it/s]Evaluating:  78%|  | 131/167 [00:16<00:04,  8.12it/s]Evaluating:  79%|  | 132/167 [00:16<00:04,  8.17it/s]Evaluating:  80%|  | 133/167 [00:16<00:04,  8.24it/s]Evaluating:  80%|  | 134/167 [00:16<00:03,  8.26it/s]Evaluating:  81%|  | 135/167 [00:17<00:03,  8.28it/s]Evaluating:  81%| | 136/167 [00:17<00:03,  8.29it/s]Evaluating:  82%| | 137/167 [00:17<00:03,  8.32it/s]Evaluating:  83%| | 138/167 [00:17<00:03,  8.32it/s]Evaluating:  83%| | 139/167 [00:17<00:03,  8.35it/s]Evaluating:  84%| | 140/167 [00:17<00:03,  8.38it/s]Evaluating:  84%| | 141/167 [00:17<00:03,  8.39it/s]Evaluating:  85%| | 142/167 [00:17<00:03,  8.30it/s]Evaluating:  86%| | 143/167 [00:18<00:02,  8.36it/s]Evaluating:  86%| | 144/167 [00:18<00:02,  8.38it/s]Evaluating:  87%| | 145/167 [00:18<00:02,  8.36it/s]Evaluating:  87%| | 146/167 [00:18<00:02,  8.38it/s]Evaluating:  88%| | 147/167 [00:18<00:02,  8.34it/s]Evaluating:  89%| | 148/167 [00:18<00:02,  8.32it/s]Evaluating:  89%| | 149/167 [00:18<00:02,  8.36it/s]Evaluating:  90%| | 150/167 [00:18<00:02,  8.36it/s]Evaluating:  90%| | 151/167 [00:18<00:01,  8.38it/s]Evaluating:  91%| | 152/167 [00:19<00:01,  8.42it/s]Evaluating:  92%|| 153/167 [00:19<00:01,  8.44it/s]Evaluating:  92%|| 154/167 [00:19<00:01,  8.45it/s]Evaluating:  93%|| 155/167 [00:19<00:01,  8.45it/s]Evaluating:  93%|| 156/167 [00:19<00:01,  8.45it/s]Evaluating:  94%|| 157/167 [00:19<00:01,  8.46it/s]Evaluating:  95%|| 158/167 [00:19<00:01,  8.46it/s]Evaluating:  95%|| 159/167 [00:19<00:00,  8.40it/s]Evaluating:  96%|| 160/167 [00:20<00:00,  8.41it/s]Evaluating:  96%|| 161/167 [00:20<00:00,  8.43it/s]Evaluating:  97%|| 162/167 [00:20<00:00,  8.43it/s]Evaluating:  98%|| 163/167 [00:20<00:00,  8.43it/s]Evaluating:  98%|| 164/167 [00:20<00:00,  8.38it/s]Evaluating:  99%|| 165/167 [00:20<00:00,  8.31it/s]Evaluating:  99%|| 166/167 [00:20<00:00,  8.31it/s]Evaluating: 100%|| 167/167 [00:20<00:00,  8.01it/s]
05/08/2022 20:36:32 - INFO - __main__ -     Evaluation done in total 20.841883 secs (0.015647 sec per example)
05/08/2022 20:36:36 - INFO - __main__ -   Results: {'exact': 56.890756302521005, 'f1': 73.74209850039627, 'total': 1190, 'HasAns_exact': 56.890756302521005, 'HasAns_f1': 73.74209850039627, 'HasAns_total': 1190, 'best_exact': 56.890756302521005, 'best_exact_thresh': 0.0, 'best_f1': 73.74209850039627, 'best_f1_thresh': 0.0}
  tr 
2022-05-08 20:36:38.870872: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:36:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'qa_outputs.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:36:52 - INFO - __main__ -   lang2id = None
05/08/2022 20:36:56 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='tr', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.tr.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:36:56 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:36:56 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'qa_outputs.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'qa_outputs.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:37:10 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 29%|       | 14/48 [00:00<00:00, 135.67it/s] 58%|    | 28/48 [00:00<00:00, 99.22it/s]  81%| | 39/48 [00:00<00:00, 87.87it/s]100%|| 48/48 [00:00<00:00, 96.15it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<03:16,  6.06it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:04, 229.04it/s]convert squad examples to features:  11%|         | 129/1190 [00:00<00:03, 279.96it/s]convert squad examples to features:  16%|        | 193/1190 [00:00<00:03, 323.79it/s]convert squad examples to features:  19%|        | 226/1190 [00:00<00:02, 323.26it/s]convert squad examples to features:  22%|       | 259/1190 [00:00<00:02, 316.56it/s]convert squad examples to features:  24%|       | 291/1190 [00:01<00:03, 296.76it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:03, 270.76it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:02, 281.01it/s]convert squad examples to features:  32%|      | 385/1190 [00:01<00:04, 166.11it/s]convert squad examples to features:  35%|      | 417/1190 [00:01<00:04, 181.70it/s]convert squad examples to features:  38%|      | 449/1190 [00:01<00:03, 188.63it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:03, 200.70it/s]convert squad examples to features:  43%|     | 513/1190 [00:02<00:03, 211.18it/s]convert squad examples to features:  46%|     | 545/1190 [00:02<00:04, 153.45it/s]convert squad examples to features:  51%|     | 609/1190 [00:02<00:02, 215.70it/s]convert squad examples to features:  54%|    | 641/1190 [00:02<00:02, 216.67it/s]convert squad examples to features:  57%|    | 673/1190 [00:03<00:02, 212.42it/s]convert squad examples to features:  59%|    | 705/1190 [00:03<00:02, 226.90it/s]convert squad examples to features:  65%|   | 769/1190 [00:03<00:01, 250.58it/s]convert squad examples to features:  70%|   | 833/1190 [00:03<00:01, 232.70it/s]convert squad examples to features:  73%|  | 865/1190 [00:03<00:01, 231.31it/s]convert squad examples to features:  75%|  | 897/1190 [00:04<00:01, 205.03it/s]convert squad examples to features:  78%|  | 929/1190 [00:04<00:01, 200.20it/s]convert squad examples to features:  81%|  | 961/1190 [00:04<00:01, 218.74it/s]convert squad examples to features:  83%| | 993/1190 [00:04<00:00, 214.76it/s]convert squad examples to features:  86%| | 1025/1190 [00:04<00:00, 221.97it/s]convert squad examples to features:  89%| | 1057/1190 [00:04<00:00, 242.78it/s]convert squad examples to features:  92%|| 1089/1190 [00:04<00:00, 242.56it/s]convert squad examples to features:  94%|| 1121/1190 [00:04<00:00, 253.67it/s]convert squad examples to features:  97%|| 1153/1190 [00:05<00:00, 256.45it/s]convert squad examples to features: 100%|| 1190/1190 [00:05<00:00, 235.29it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 649475.83it/s]
05/08/2022 20:37:16 - INFO - __main__ -   Saving features into cached file ./cached_xquad.tr.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_tr
05/08/2022 20:37:17 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:37:17 - INFO - __main__ -     Num examples = 1274
05/08/2022 20:37:17 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/160 [00:00<?, ?it/s]Evaluating:   1%|          | 1/160 [00:00<01:38,  1.61it/s]Evaluating:   1%|         | 2/160 [00:00<00:52,  2.98it/s]Evaluating:   2%|         | 3/160 [00:00<00:37,  4.21it/s]Evaluating:   2%|         | 4/160 [00:00<00:29,  5.25it/s]Evaluating:   3%|         | 5/160 [00:01<00:25,  6.10it/s]Evaluating:   4%|         | 6/160 [00:01<00:22,  6.74it/s]Evaluating:   4%|         | 7/160 [00:01<00:21,  7.21it/s]Evaluating:   5%|         | 8/160 [00:01<00:20,  7.57it/s]Evaluating:   6%|         | 9/160 [00:01<00:19,  7.81it/s]Evaluating:   6%|         | 10/160 [00:01<00:18,  7.99it/s]Evaluating:   7%|         | 11/160 [00:01<00:18,  8.16it/s]Evaluating:   8%|         | 12/160 [00:01<00:17,  8.24it/s]Evaluating:   8%|         | 13/160 [00:02<00:17,  8.28it/s]Evaluating:   9%|         | 14/160 [00:02<00:17,  8.35it/s]Evaluating:   9%|         | 15/160 [00:02<00:17,  8.39it/s]Evaluating:  10%|         | 16/160 [00:02<00:17,  8.43it/s]Evaluating:  11%|         | 17/160 [00:02<00:16,  8.48it/s]Evaluating:  11%|        | 18/160 [00:02<00:16,  8.52it/s]Evaluating:  12%|        | 19/160 [00:02<00:16,  8.47it/s]Evaluating:  12%|        | 20/160 [00:02<00:16,  8.36it/s]Evaluating:  13%|        | 21/160 [00:03<00:16,  8.28it/s]Evaluating:  14%|        | 22/160 [00:03<00:16,  8.33it/s]Evaluating:  14%|        | 23/160 [00:03<00:16,  8.35it/s]Evaluating:  15%|        | 24/160 [00:03<00:17,  7.99it/s]Evaluating:  16%|        | 25/160 [00:03<00:16,  8.15it/s]Evaluating:  16%|        | 26/160 [00:03<00:16,  8.24it/s]Evaluating:  17%|        | 27/160 [00:03<00:16,  8.27it/s]Evaluating:  18%|        | 28/160 [00:03<00:15,  8.34it/s]Evaluating:  18%|        | 29/160 [00:03<00:15,  8.36it/s]Evaluating:  19%|        | 30/160 [00:04<00:15,  8.34it/s]Evaluating:  19%|        | 31/160 [00:04<00:15,  8.30it/s]Evaluating:  20%|        | 32/160 [00:04<00:15,  8.35it/s]Evaluating:  21%|        | 33/160 [00:04<00:15,  8.38it/s]Evaluating:  21%|       | 34/160 [00:04<00:15,  8.37it/s]Evaluating:  22%|       | 35/160 [00:04<00:14,  8.40it/s]Evaluating:  22%|       | 36/160 [00:04<00:14,  8.27it/s]Evaluating:  23%|       | 37/160 [00:04<00:14,  8.27it/s]Evaluating:  24%|       | 38/160 [00:05<00:14,  8.32it/s]Evaluating:  24%|       | 39/160 [00:05<00:14,  8.26it/s]Evaluating:  25%|       | 40/160 [00:05<00:14,  8.21it/s]Evaluating:  26%|       | 41/160 [00:05<00:14,  8.28it/s]Evaluating:  26%|       | 42/160 [00:05<00:14,  8.34it/s]Evaluating:  27%|       | 43/160 [00:05<00:13,  8.39it/s]Evaluating:  28%|       | 44/160 [00:05<00:13,  8.42it/s]Evaluating:  28%|       | 45/160 [00:05<00:13,  8.45it/s]Evaluating:  29%|       | 46/160 [00:06<00:13,  8.47it/s]Evaluating:  29%|       | 47/160 [00:06<00:13,  8.49it/s]Evaluating:  30%|       | 48/160 [00:06<00:13,  8.48it/s]Evaluating:  31%|       | 49/160 [00:06<00:13,  8.50it/s]Evaluating:  31%|      | 50/160 [00:06<00:12,  8.50it/s]Evaluating:  32%|      | 51/160 [00:06<00:12,  8.49it/s]Evaluating:  32%|      | 52/160 [00:06<00:12,  8.44it/s]Evaluating:  33%|      | 53/160 [00:06<00:12,  8.39it/s]Evaluating:  34%|      | 54/160 [00:06<00:12,  8.37it/s]Evaluating:  34%|      | 55/160 [00:07<00:12,  8.38it/s]Evaluating:  35%|      | 56/160 [00:07<00:12,  8.38it/s]Evaluating:  36%|      | 57/160 [00:07<00:12,  8.37it/s]Evaluating:  36%|      | 58/160 [00:07<00:12,  8.40it/s]Evaluating:  37%|      | 59/160 [00:07<00:12,  8.41it/s]Evaluating:  38%|      | 60/160 [00:07<00:11,  8.43it/s]Evaluating:  38%|      | 61/160 [00:07<00:11,  8.46it/s]Evaluating:  39%|      | 62/160 [00:07<00:11,  8.47it/s]Evaluating:  39%|      | 63/160 [00:08<00:11,  8.45it/s]Evaluating:  40%|      | 64/160 [00:08<00:11,  8.45it/s]Evaluating:  41%|      | 65/160 [00:08<00:11,  8.47it/s]Evaluating:  41%|     | 66/160 [00:08<00:11,  8.44it/s]Evaluating:  42%|     | 67/160 [00:08<00:11,  8.41it/s]Evaluating:  42%|     | 68/160 [00:08<00:10,  8.39it/s]Evaluating:  43%|     | 69/160 [00:08<00:10,  8.36it/s]Evaluating:  44%|     | 70/160 [00:08<00:10,  8.22it/s]Evaluating:  44%|     | 71/160 [00:08<00:10,  8.22it/s]Evaluating:  45%|     | 72/160 [00:09<00:10,  8.28it/s]Evaluating:  46%|     | 73/160 [00:09<00:10,  8.23it/s]Evaluating:  46%|     | 74/160 [00:09<00:10,  8.30it/s]Evaluating:  47%|     | 75/160 [00:09<00:10,  8.33it/s]Evaluating:  48%|     | 76/160 [00:09<00:10,  8.32it/s]Evaluating:  48%|     | 77/160 [00:09<00:09,  8.33it/s]Evaluating:  49%|     | 78/160 [00:09<00:09,  8.34it/s]Evaluating:  49%|     | 79/160 [00:09<00:09,  8.29it/s]Evaluating:  50%|     | 80/160 [00:10<00:09,  8.29it/s]Evaluating:  51%|     | 81/160 [00:10<00:09,  8.32it/s]Evaluating:  51%|    | 82/160 [00:10<00:09,  8.34it/s]Evaluating:  52%|    | 83/160 [00:10<00:09,  8.35it/s]Evaluating:  52%|    | 84/160 [00:10<00:09,  8.34it/s]Evaluating:  53%|    | 85/160 [00:10<00:08,  8.36it/s]Evaluating:  54%|    | 86/160 [00:10<00:08,  8.34it/s]Evaluating:  54%|    | 87/160 [00:10<00:08,  8.36it/s]Evaluating:  55%|    | 88/160 [00:11<00:08,  8.37it/s]Evaluating:  56%|    | 89/160 [00:11<00:08,  8.41it/s]Evaluating:  56%|    | 90/160 [00:11<00:08,  8.41it/s]Evaluating:  57%|    | 91/160 [00:11<00:08,  8.43it/s]Evaluating:  57%|    | 92/160 [00:11<00:08,  8.45it/s]Evaluating:  58%|    | 93/160 [00:11<00:07,  8.43it/s]Evaluating:  59%|    | 94/160 [00:11<00:07,  8.38it/s]Evaluating:  59%|    | 95/160 [00:11<00:07,  8.41it/s]Evaluating:  60%|    | 96/160 [00:11<00:07,  8.41it/s]Evaluating:  61%|    | 97/160 [00:12<00:07,  8.39it/s]Evaluating:  61%|   | 98/160 [00:12<00:07,  8.37it/s]Evaluating:  62%|   | 99/160 [00:12<00:07,  8.36it/s]Evaluating:  62%|   | 100/160 [00:12<00:07,  8.39it/s]Evaluating:  63%|   | 101/160 [00:12<00:07,  8.41it/s]Evaluating:  64%|   | 102/160 [00:12<00:06,  8.41it/s]Evaluating:  64%|   | 103/160 [00:12<00:06,  8.41it/s]Evaluating:  65%|   | 104/160 [00:12<00:06,  8.38it/s]Evaluating:  66%|   | 105/160 [00:13<00:06,  8.38it/s]Evaluating:  66%|   | 106/160 [00:13<00:06,  8.38it/s]Evaluating:  67%|   | 107/160 [00:13<00:06,  8.39it/s]Evaluating:  68%|   | 108/160 [00:13<00:06,  8.40it/s]Evaluating:  68%|   | 109/160 [00:13<00:06,  8.36it/s]Evaluating:  69%|   | 110/160 [00:13<00:06,  8.32it/s]Evaluating:  69%|   | 111/160 [00:13<00:05,  8.35it/s]Evaluating:  70%|   | 112/160 [00:13<00:05,  8.39it/s]Evaluating:  71%|   | 113/160 [00:14<00:05,  8.40it/s]Evaluating:  71%|  | 114/160 [00:14<00:05,  8.39it/s]Evaluating:  72%|  | 115/160 [00:14<00:05,  8.32it/s]Evaluating:  72%|  | 116/160 [00:14<00:05,  8.24it/s]Evaluating:  73%|  | 117/160 [00:14<00:05,  8.25it/s]Evaluating:  74%|  | 118/160 [00:14<00:05,  8.22it/s]Evaluating:  74%|  | 119/160 [00:14<00:04,  8.22it/s]Evaluating:  75%|  | 120/160 [00:14<00:04,  8.25it/s]Evaluating:  76%|  | 121/160 [00:14<00:04,  8.25it/s]Evaluating:  76%|  | 122/160 [00:15<00:04,  8.24it/s]Evaluating:  77%|  | 123/160 [00:15<00:04,  8.25it/s]Evaluating:  78%|  | 124/160 [00:15<00:04,  8.28it/s]Evaluating:  78%|  | 125/160 [00:15<00:04,  8.30it/s]Evaluating:  79%|  | 126/160 [00:15<00:04,  8.25it/s]Evaluating:  79%|  | 127/160 [00:15<00:04,  8.22it/s]Evaluating:  80%|  | 128/160 [00:15<00:03,  8.26it/s]Evaluating:  81%|  | 129/160 [00:15<00:03,  8.29it/s]Evaluating:  81%| | 130/160 [00:16<00:03,  8.30it/s]Evaluating:  82%| | 131/160 [00:16<00:03,  8.21it/s]Evaluating:  82%| | 132/160 [00:16<00:03,  8.28it/s]Evaluating:  83%| | 133/160 [00:16<00:03,  8.33it/s]Evaluating:  84%| | 134/160 [00:16<00:03,  8.32it/s]Evaluating:  84%| | 135/160 [00:16<00:02,  8.35it/s]Evaluating:  85%| | 136/160 [00:16<00:02,  8.38it/s]Evaluating:  86%| | 137/160 [00:16<00:02,  8.32it/s]Evaluating:  86%| | 138/160 [00:17<00:02,  8.28it/s]Evaluating:  87%| | 139/160 [00:17<00:02,  8.33it/s]Evaluating:  88%| | 140/160 [00:17<00:02,  8.35it/s]Evaluating:  88%| | 141/160 [00:17<00:02,  8.40it/s]Evaluating:  89%| | 142/160 [00:17<00:02,  8.38it/s]Evaluating:  89%| | 143/160 [00:17<00:02,  8.37it/s]Evaluating:  90%| | 144/160 [00:17<00:01,  8.35it/s]Evaluating:  91%| | 145/160 [00:17<00:01,  8.34it/s]Evaluating:  91%|| 146/160 [00:17<00:01,  8.37it/s]Evaluating:  92%|| 147/160 [00:18<00:01,  8.39it/s]Evaluating:  92%|| 148/160 [00:18<00:01,  8.37it/s]Evaluating:  93%|| 149/160 [00:18<00:01,  8.38it/s]Evaluating:  94%|| 150/160 [00:18<00:01,  8.34it/s]Evaluating:  94%|| 151/160 [00:18<00:01,  8.28it/s]Evaluating:  95%|| 152/160 [00:18<00:00,  8.32it/s]Evaluating:  96%|| 153/160 [00:18<00:00,  8.34it/s]Evaluating:  96%|| 154/160 [00:18<00:00,  8.36it/s]Evaluating:  97%|| 155/160 [00:19<00:00,  8.33it/s]Evaluating:  98%|| 156/160 [00:19<00:00,  8.30it/s]Evaluating:  98%|| 157/160 [00:19<00:00,  8.32it/s]Evaluating:  99%|| 158/160 [00:19<00:00,  8.34it/s]Evaluating:  99%|| 159/160 [00:19<00:00,  8.37it/s]Evaluating: 100%|| 160/160 [00:19<00:00,  8.17it/s]
05/08/2022 20:37:36 - INFO - __main__ -     Evaluation done in total 19.577054 secs (0.015367 sec per example)
05/08/2022 20:37:40 - INFO - __main__ -   Results: {'exact': 50.84033613445378, 'f1': 67.50240171454271, 'total': 1190, 'HasAns_exact': 50.84033613445378, 'HasAns_f1': 67.50240171454271, 'HasAns_total': 1190, 'best_exact': 50.84033613445378, 'best_exact_thresh': 0.0, 'best_f1': 67.50240171454271, 'best_f1_thresh': 0.0}
  ar 
2022-05-08 20:37:43.796114: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:37:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'qa_outputs.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'qa_outputs.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:37:56 - INFO - __main__ -   lang2id = None
05/08/2022 20:37:59 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='ar', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.ar.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:37:59 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:37:59 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'qa_outputs.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'qa_outputs.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:38:12 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 31%|      | 15/48 [00:00<00:00, 149.63it/s] 62%|   | 30/48 [00:00<00:00, 98.00it/s]  92%|| 44/48 [00:00<00:00, 112.70it/s]100%|| 48/48 [00:00<00:00, 116.41it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<03:53,  5.09it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:05, 188.01it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:05, 217.21it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:03, 267.10it/s]convert squad examples to features:  19%|        | 225/1190 [00:00<00:03, 262.24it/s]convert squad examples to features:  22%|       | 257/1190 [00:01<00:03, 254.75it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:03, 237.70it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:03, 242.80it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:03, 256.47it/s]convert squad examples to features:  32%|      | 385/1190 [00:01<00:05, 142.18it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:05, 153.64it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:04, 164.69it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:04, 167.55it/s]convert squad examples to features:  43%|     | 513/1190 [00:02<00:04, 148.17it/s]convert squad examples to features:  48%|     | 577/1190 [00:02<00:02, 209.39it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:02, 198.84it/s]convert squad examples to features:  54%|    | 641/1190 [00:03<00:02, 199.21it/s]convert squad examples to features:  57%|    | 673/1190 [00:03<00:02, 185.78it/s]convert squad examples to features:  59%|    | 705/1190 [00:03<00:02, 190.28it/s]convert squad examples to features:  62%|   | 737/1190 [00:03<00:02, 198.84it/s]convert squad examples to features:  65%|   | 769/1190 [00:03<00:01, 218.43it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:01, 225.50it/s]convert squad examples to features:  70%|   | 833/1190 [00:04<00:01, 215.32it/s]convert squad examples to features:  73%|  | 865/1190 [00:04<00:01, 218.08it/s]convert squad examples to features:  75%|  | 897/1190 [00:04<00:01, 223.74it/s]convert squad examples to features:  78%|  | 929/1190 [00:04<00:01, 230.71it/s]convert squad examples to features:  81%|  | 961/1190 [00:04<00:00, 238.15it/s]convert squad examples to features:  83%| | 993/1190 [00:04<00:00, 230.54it/s]convert squad examples to features:  86%| | 1025/1190 [00:04<00:00, 238.86it/s]convert squad examples to features:  89%| | 1057/1190 [00:05<00:00, 244.36it/s]convert squad examples to features:  92%|| 1089/1190 [00:05<00:00, 242.51it/s]convert squad examples to features:  94%|| 1121/1190 [00:05<00:00, 225.24it/s]convert squad examples to features:  97%|| 1153/1190 [00:05<00:00, 230.86it/s]convert squad examples to features: 100%|| 1190/1190 [00:05<00:00, 215.21it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 415727.28it/s]
05/08/2022 20:38:18 - INFO - __main__ -   Saving features into cached file ./cached_xquad.ar.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_ar
05/08/2022 20:38:20 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:38:20 - INFO - __main__ -     Num examples = 1318
05/08/2022 20:38:20 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]Evaluating:   1%|          | 1/165 [00:01<02:44,  1.00s/it]Evaluating:   1%|          | 2/165 [00:01<01:18,  2.07it/s]Evaluating:   2%|         | 3/165 [00:01<00:51,  3.17it/s]Evaluating:   2%|         | 4/165 [00:01<00:38,  4.20it/s]Evaluating:   3%|         | 5/165 [00:01<00:31,  5.12it/s]Evaluating:   4%|         | 6/165 [00:01<00:27,  5.89it/s]Evaluating:   4%|         | 7/165 [00:01<00:24,  6.55it/s]Evaluating:   5%|         | 8/165 [00:01<00:22,  7.07it/s]Evaluating:   5%|         | 9/165 [00:01<00:20,  7.46it/s]Evaluating:   6%|         | 10/165 [00:02<00:20,  7.67it/s]Evaluating:   7%|         | 11/165 [00:02<00:19,  7.81it/s]Evaluating:   7%|         | 12/165 [00:02<00:19,  8.00it/s]Evaluating:   8%|         | 13/165 [00:02<00:18,  8.05it/s]Evaluating:   8%|         | 14/165 [00:02<00:18,  8.17it/s]Evaluating:   9%|         | 15/165 [00:02<00:18,  8.23it/s]Evaluating:  10%|         | 16/165 [00:02<00:18,  8.25it/s]Evaluating:  10%|         | 17/165 [00:02<00:17,  8.34it/s]Evaluating:  11%|         | 18/165 [00:03<00:17,  8.39it/s]Evaluating:  12%|        | 19/165 [00:03<00:17,  8.41it/s]Evaluating:  12%|        | 20/165 [00:03<00:17,  8.37it/s]Evaluating:  13%|        | 21/165 [00:03<00:17,  8.41it/s]Evaluating:  13%|        | 22/165 [00:03<00:16,  8.42it/s]Evaluating:  14%|        | 23/165 [00:03<00:16,  8.43it/s]Evaluating:  15%|        | 24/165 [00:03<00:17,  7.99it/s]Evaluating:  15%|        | 25/165 [00:03<00:17,  8.09it/s]Evaluating:  16%|        | 26/165 [00:04<00:17,  8.12it/s]Evaluating:  16%|        | 27/165 [00:04<00:16,  8.21it/s]Evaluating:  17%|        | 28/165 [00:04<00:16,  8.27it/s]Evaluating:  18%|        | 29/165 [00:04<00:16,  8.28it/s]Evaluating:  18%|        | 30/165 [00:04<00:16,  8.28it/s]Evaluating:  19%|        | 31/165 [00:04<00:16,  8.28it/s]Evaluating:  19%|        | 32/165 [00:04<00:15,  8.34it/s]Evaluating:  20%|        | 33/165 [00:04<00:15,  8.36it/s]Evaluating:  21%|        | 34/165 [00:04<00:15,  8.41it/s]Evaluating:  21%|        | 35/165 [00:05<00:15,  8.44it/s]Evaluating:  22%|       | 36/165 [00:05<00:15,  8.44it/s]Evaluating:  22%|       | 37/165 [00:05<00:15,  8.43it/s]Evaluating:  23%|       | 38/165 [00:05<00:15,  8.42it/s]Evaluating:  24%|       | 39/165 [00:05<00:15,  8.39it/s]Evaluating:  24%|       | 40/165 [00:05<00:14,  8.41it/s]Evaluating:  25%|       | 41/165 [00:05<00:14,  8.41it/s]Evaluating:  25%|       | 42/165 [00:05<00:14,  8.38it/s]Evaluating:  26%|       | 43/165 [00:06<00:14,  8.23it/s]Evaluating:  27%|       | 44/165 [00:06<00:14,  8.24it/s]Evaluating:  27%|       | 45/165 [00:06<00:14,  8.29it/s]Evaluating:  28%|       | 46/165 [00:06<00:14,  8.35it/s]Evaluating:  28%|       | 47/165 [00:06<00:14,  8.40it/s]Evaluating:  29%|       | 48/165 [00:06<00:13,  8.42it/s]Evaluating:  30%|       | 49/165 [00:06<00:13,  8.43it/s]Evaluating:  30%|       | 50/165 [00:06<00:13,  8.45it/s]Evaluating:  31%|       | 51/165 [00:06<00:13,  8.47it/s]Evaluating:  32%|      | 52/165 [00:07<00:13,  8.43it/s]Evaluating:  32%|      | 53/165 [00:07<00:13,  8.42it/s]Evaluating:  33%|      | 54/165 [00:07<00:13,  8.40it/s]Evaluating:  33%|      | 55/165 [00:07<00:13,  8.40it/s]Evaluating:  34%|      | 56/165 [00:07<00:12,  8.41it/s]Evaluating:  35%|      | 57/165 [00:07<00:12,  8.38it/s]Evaluating:  35%|      | 58/165 [00:07<00:12,  8.39it/s]Evaluating:  36%|      | 59/165 [00:07<00:12,  8.40it/s]Evaluating:  36%|      | 60/165 [00:08<00:12,  8.41it/s]Evaluating:  37%|      | 61/165 [00:08<00:12,  8.41it/s]Evaluating:  38%|      | 62/165 [00:08<00:12,  8.40it/s]Evaluating:  38%|      | 63/165 [00:08<00:12,  8.40it/s]Evaluating:  39%|      | 64/165 [00:08<00:12,  8.40it/s]Evaluating:  39%|      | 65/165 [00:08<00:11,  8.34it/s]Evaluating:  40%|      | 66/165 [00:08<00:11,  8.34it/s]Evaluating:  41%|      | 67/165 [00:08<00:11,  8.38it/s]Evaluating:  41%|      | 68/165 [00:09<00:11,  8.38it/s]Evaluating:  42%|     | 69/165 [00:09<00:11,  8.37it/s]Evaluating:  42%|     | 70/165 [00:09<00:11,  8.38it/s]Evaluating:  43%|     | 71/165 [00:09<00:11,  8.38it/s]Evaluating:  44%|     | 72/165 [00:09<00:11,  8.38it/s]Evaluating:  44%|     | 73/165 [00:09<00:10,  8.39it/s]Evaluating:  45%|     | 74/165 [00:09<00:10,  8.33it/s]Evaluating:  45%|     | 75/165 [00:09<00:10,  8.37it/s]Evaluating:  46%|     | 76/165 [00:09<00:10,  8.33it/s]Evaluating:  47%|     | 77/165 [00:10<00:10,  8.37it/s]Evaluating:  47%|     | 78/165 [00:10<00:10,  8.37it/s]Evaluating:  48%|     | 79/165 [00:10<00:10,  8.25it/s]Evaluating:  48%|     | 80/165 [00:10<00:10,  8.20it/s]Evaluating:  49%|     | 81/165 [00:10<00:10,  8.24it/s]Evaluating:  50%|     | 82/165 [00:10<00:10,  8.25it/s]Evaluating:  50%|     | 83/165 [00:10<00:09,  8.30it/s]Evaluating:  51%|     | 84/165 [00:10<00:09,  8.33it/s]Evaluating:  52%|    | 85/165 [00:11<00:09,  8.26it/s]Evaluating:  52%|    | 86/165 [00:11<00:09,  8.28it/s]Evaluating:  53%|    | 87/165 [00:11<00:09,  8.25it/s]Evaluating:  53%|    | 88/165 [00:11<00:09,  8.26it/s]Evaluating:  54%|    | 89/165 [00:11<00:09,  8.22it/s]Evaluating:  55%|    | 90/165 [00:11<00:09,  8.28it/s]Evaluating:  55%|    | 91/165 [00:11<00:08,  8.30it/s]Evaluating:  56%|    | 92/165 [00:11<00:08,  8.29it/s]Evaluating:  56%|    | 93/165 [00:12<00:08,  8.27it/s]Evaluating:  57%|    | 94/165 [00:12<00:08,  8.31it/s]Evaluating:  58%|    | 95/165 [00:12<00:08,  8.30it/s]Evaluating:  58%|    | 96/165 [00:12<00:08,  8.26it/s]Evaluating:  59%|    | 97/165 [00:12<00:08,  8.19it/s]Evaluating:  59%|    | 98/165 [00:12<00:08,  8.22it/s]Evaluating:  60%|    | 99/165 [00:12<00:08,  8.11it/s]Evaluating:  61%|    | 100/165 [00:12<00:07,  8.20it/s]Evaluating:  61%|    | 101/165 [00:12<00:07,  8.23it/s]Evaluating:  62%|   | 102/165 [00:13<00:07,  8.23it/s]Evaluating:  62%|   | 103/165 [00:13<00:07,  8.22it/s]Evaluating:  63%|   | 104/165 [00:13<00:07,  8.23it/s]Evaluating:  64%|   | 105/165 [00:13<00:07,  8.27it/s]Evaluating:  64%|   | 106/165 [00:13<00:07,  8.25it/s]Evaluating:  65%|   | 107/165 [00:13<00:07,  8.16it/s]Evaluating:  65%|   | 108/165 [00:13<00:06,  8.17it/s]Evaluating:  66%|   | 109/165 [00:13<00:06,  8.21it/s]Evaluating:  67%|   | 110/165 [00:14<00:06,  8.26it/s]Evaluating:  67%|   | 111/165 [00:14<00:06,  8.25it/s]Evaluating:  68%|   | 112/165 [00:14<00:06,  8.14it/s]Evaluating:  68%|   | 113/165 [00:14<00:06,  8.17it/s]Evaluating:  69%|   | 114/165 [00:14<00:06,  8.21it/s]Evaluating:  70%|   | 115/165 [00:14<00:06,  8.24it/s]Evaluating:  70%|   | 116/165 [00:14<00:05,  8.23it/s]Evaluating:  71%|   | 117/165 [00:14<00:05,  8.26it/s]Evaluating:  72%|  | 118/165 [00:15<00:05,  8.29it/s]Evaluating:  72%|  | 119/165 [00:15<00:05,  8.27it/s]Evaluating:  73%|  | 120/165 [00:15<00:05,  8.19it/s]Evaluating:  73%|  | 121/165 [00:15<00:05,  8.17it/s]Evaluating:  74%|  | 122/165 [00:15<00:05,  8.24it/s]Evaluating:  75%|  | 123/165 [00:15<00:05,  8.30it/s]Evaluating:  75%|  | 124/165 [00:15<00:04,  8.34it/s]Evaluating:  76%|  | 125/165 [00:15<00:04,  8.36it/s]Evaluating:  76%|  | 126/165 [00:16<00:04,  8.34it/s]Evaluating:  77%|  | 127/165 [00:16<00:04,  8.31it/s]Evaluating:  78%|  | 128/165 [00:16<00:04,  8.32it/s]Evaluating:  78%|  | 129/165 [00:16<00:04,  8.29it/s]Evaluating:  79%|  | 130/165 [00:16<00:04,  8.23it/s]Evaluating:  79%|  | 131/165 [00:16<00:04,  8.27it/s]Evaluating:  80%|  | 132/165 [00:16<00:03,  8.27it/s]Evaluating:  81%|  | 133/165 [00:16<00:03,  8.24it/s]Evaluating:  81%|  | 134/165 [00:16<00:03,  8.23it/s]Evaluating:  82%| | 135/165 [00:17<00:03,  8.22it/s]Evaluating:  82%| | 136/165 [00:17<00:03,  8.23it/s]Evaluating:  83%| | 137/165 [00:17<00:03,  8.29it/s]Evaluating:  84%| | 138/165 [00:17<00:03,  8.31it/s]Evaluating:  84%| | 139/165 [00:17<00:03,  8.29it/s]Evaluating:  85%| | 140/165 [00:17<00:03,  8.33it/s]Evaluating:  85%| | 141/165 [00:17<00:02,  8.36it/s]Evaluating:  86%| | 142/165 [00:17<00:02,  8.38it/s]Evaluating:  87%| | 143/165 [00:18<00:02,  8.31it/s]Evaluating:  87%| | 144/165 [00:18<00:02,  8.33it/s]Evaluating:  88%| | 145/165 [00:18<00:02,  8.35it/s]Evaluating:  88%| | 146/165 [00:18<00:02,  8.38it/s]Evaluating:  89%| | 147/165 [00:18<00:02,  8.40it/s]Evaluating:  90%| | 148/165 [00:18<00:02,  8.38it/s]Evaluating:  90%| | 149/165 [00:18<00:01,  8.41it/s]Evaluating:  91%| | 150/165 [00:18<00:01,  8.39it/s]Evaluating:  92%|| 151/165 [00:19<00:01,  8.31it/s]Evaluating:  92%|| 152/165 [00:19<00:01,  8.24it/s]Evaluating:  93%|| 153/165 [00:19<00:01,  8.31it/s]Evaluating:  93%|| 154/165 [00:19<00:01,  8.31it/s]Evaluating:  94%|| 155/165 [00:19<00:01,  8.36it/s]Evaluating:  95%|| 156/165 [00:19<00:01,  8.32it/s]Evaluating:  95%|| 157/165 [00:19<00:00,  8.36it/s]Evaluating:  96%|| 158/165 [00:19<00:00,  8.39it/s]Evaluating:  96%|| 159/165 [00:19<00:00,  8.38it/s]Evaluating:  97%|| 160/165 [00:20<00:00,  8.40it/s]Evaluating:  98%|| 161/165 [00:20<00:00,  8.42it/s]Evaluating:  98%|| 162/165 [00:20<00:00,  8.42it/s]Evaluating:  99%|| 163/165 [00:20<00:00,  8.39it/s]Evaluating:  99%|| 164/165 [00:20<00:00,  8.40it/s]Evaluating: 100%|| 165/165 [00:20<00:00,  7.98it/s]
05/08/2022 20:38:40 - INFO - __main__ -     Evaluation done in total 20.677853 secs (0.015689 sec per example)
05/08/2022 20:38:44 - INFO - __main__ -   Results: {'exact': 49.15966386554622, 'f1': 66.08083433778512, 'total': 1190, 'HasAns_exact': 49.15966386554622, 'HasAns_f1': 66.08083433778512, 'HasAns_total': 1190, 'best_exact': 49.15966386554622, 'best_exact_thresh': 0.0, 'best_f1': 66.08083433778512, 'best_f1_thresh': 0.0}
  vi 
2022-05-08 20:38:46.786518: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:38:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'qa_outputs.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'qa_outputs.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:38:59 - INFO - __main__ -   lang2id = None
05/08/2022 20:39:02 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='vi', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.vi.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:39:02 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:39:02 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'qa_outputs.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.2.attention.self.query.weight', 'qa_outputs.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:39:15 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 126.56it/s] 60%|    | 29/48 [00:00<00:00, 115.99it/s] 92%|| 44/48 [00:00<00:00, 129.31it/s]100%|| 48/48 [00:00<00:00, 128.60it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<04:52,  4.07it/s]convert squad examples to features:   3%|         | 33/1190 [00:00<00:09, 119.23it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:06, 166.12it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:06, 178.79it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:04, 231.19it/s]convert squad examples to features:  16%|        | 193/1190 [00:00<00:04, 248.72it/s]convert squad examples to features:  19%|        | 225/1190 [00:01<00:03, 245.57it/s]convert squad examples to features:  22%|       | 257/1190 [00:01<00:04, 228.83it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:03, 234.63it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:04, 209.24it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:03, 218.22it/s]convert squad examples to features:  32%|      | 385/1190 [00:02<00:07, 111.72it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:06, 125.04it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:05, 133.24it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:04, 145.42it/s]convert squad examples to features:  43%|     | 513/1190 [00:03<00:05, 125.79it/s]convert squad examples to features:  48%|     | 577/1190 [00:03<00:03, 160.83it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:03, 151.38it/s]convert squad examples to features:  54%|    | 641/1190 [00:03<00:03, 155.94it/s]convert squad examples to features:  57%|    | 673/1190 [00:04<00:03, 159.67it/s]convert squad examples to features:  59%|    | 705/1190 [00:04<00:02, 167.61it/s]convert squad examples to features:  62%|   | 737/1190 [00:04<00:02, 182.62it/s]convert squad examples to features:  65%|   | 769/1190 [00:04<00:02, 191.96it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:01, 209.31it/s]convert squad examples to features:  70%|   | 833/1190 [00:04<00:01, 189.53it/s]convert squad examples to features:  73%|  | 865/1190 [00:05<00:01, 187.27it/s]convert squad examples to features:  75%|  | 897/1190 [00:05<00:01, 179.27it/s]convert squad examples to features:  78%|  | 929/1190 [00:05<00:01, 178.28it/s]convert squad examples to features:  81%|  | 961/1190 [00:05<00:01, 194.19it/s]convert squad examples to features:  83%| | 993/1190 [00:05<00:01, 193.82it/s]convert squad examples to features:  86%| | 1025/1190 [00:05<00:00, 192.83it/s]convert squad examples to features:  89%| | 1057/1190 [00:06<00:00, 199.34it/s]convert squad examples to features:  92%|| 1089/1190 [00:06<00:00, 192.14it/s]convert squad examples to features:  94%|| 1121/1190 [00:06<00:00, 189.75it/s]convert squad examples to features:  97%|| 1153/1190 [00:06<00:00, 186.15it/s]convert squad examples to features: 100%|| 1190/1190 [00:06<00:00, 180.18it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 488664.75it/s]
05/08/2022 20:39:23 - INFO - __main__ -   Saving features into cached file ./cached_xquad.vi.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_vi
05/08/2022 20:39:24 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:39:24 - INFO - __main__ -     Num examples = 1314
05/08/2022 20:39:24 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]Evaluating:   1%|          | 1/165 [00:00<01:55,  1.42it/s]Evaluating:   1%|          | 2/165 [00:00<00:59,  2.74it/s]Evaluating:   2%|         | 3/165 [00:00<00:41,  3.94it/s]Evaluating:   2%|         | 4/165 [00:01<00:32,  4.96it/s]Evaluating:   3%|         | 5/165 [00:01<00:27,  5.77it/s]Evaluating:   4%|         | 6/165 [00:01<00:24,  6.48it/s]Evaluating:   4%|         | 7/165 [00:01<00:22,  7.04it/s]Evaluating:   5%|         | 8/165 [00:01<00:21,  7.41it/s]Evaluating:   5%|         | 9/165 [00:01<00:20,  7.75it/s]Evaluating:   6%|         | 10/165 [00:01<00:19,  7.88it/s]Evaluating:   7%|         | 11/165 [00:01<00:19,  8.05it/s]Evaluating:   7%|         | 12/165 [00:02<00:18,  8.20it/s]Evaluating:   8%|         | 13/165 [00:02<00:18,  8.29it/s]Evaluating:   8%|         | 14/165 [00:02<00:18,  8.30it/s]Evaluating:   9%|         | 15/165 [00:02<00:18,  8.32it/s]Evaluating:  10%|         | 16/165 [00:02<00:17,  8.32it/s]Evaluating:  10%|         | 17/165 [00:02<00:17,  8.36it/s]Evaluating:  11%|         | 18/165 [00:02<00:17,  8.37it/s]Evaluating:  12%|        | 19/165 [00:02<00:17,  8.40it/s]Evaluating:  12%|        | 20/165 [00:02<00:17,  8.43it/s]Evaluating:  13%|        | 21/165 [00:03<00:17,  8.46it/s]Evaluating:  13%|        | 22/165 [00:03<00:16,  8.45it/s]Evaluating:  14%|        | 23/165 [00:03<00:16,  8.38it/s]Evaluating:  15%|        | 24/165 [00:03<00:16,  8.35it/s]Evaluating:  15%|        | 25/165 [00:03<00:17,  7.80it/s]Evaluating:  16%|        | 26/165 [00:03<00:17,  7.97it/s]Evaluating:  16%|        | 27/165 [00:03<00:17,  8.07it/s]Evaluating:  17%|        | 28/165 [00:03<00:16,  8.19it/s]Evaluating:  18%|        | 29/165 [00:04<00:16,  8.26it/s]Evaluating:  18%|        | 30/165 [00:04<00:16,  8.30it/s]Evaluating:  19%|        | 31/165 [00:04<00:16,  8.37it/s]Evaluating:  19%|        | 32/165 [00:04<00:15,  8.37it/s]Evaluating:  20%|        | 33/165 [00:04<00:15,  8.38it/s]Evaluating:  21%|        | 34/165 [00:04<00:15,  8.33it/s]Evaluating:  21%|        | 35/165 [00:04<00:15,  8.36it/s]Evaluating:  22%|       | 36/165 [00:04<00:15,  8.40it/s]Evaluating:  22%|       | 37/165 [00:05<00:15,  8.43it/s]Evaluating:  23%|       | 38/165 [00:05<00:15,  8.45it/s]Evaluating:  24%|       | 39/165 [00:05<00:14,  8.47it/s]Evaluating:  24%|       | 40/165 [00:05<00:14,  8.47it/s]Evaluating:  25%|       | 41/165 [00:05<00:14,  8.46it/s]Evaluating:  25%|       | 42/165 [00:05<00:14,  8.48it/s]Evaluating:  26%|       | 43/165 [00:05<00:14,  8.43it/s]Evaluating:  27%|       | 44/165 [00:05<00:14,  8.46it/s]Evaluating:  27%|       | 45/165 [00:05<00:14,  8.46it/s]Evaluating:  28%|       | 46/165 [00:06<00:14,  8.47it/s]Evaluating:  28%|       | 47/165 [00:06<00:13,  8.49it/s]Evaluating:  29%|       | 48/165 [00:06<00:13,  8.50it/s]Evaluating:  30%|       | 49/165 [00:06<00:13,  8.49it/s]Evaluating:  30%|       | 50/165 [00:06<00:13,  8.44it/s]Evaluating:  31%|       | 51/165 [00:06<00:13,  8.45it/s]Evaluating:  32%|      | 52/165 [00:06<00:13,  8.44it/s]Evaluating:  32%|      | 53/165 [00:06<00:13,  8.40it/s]Evaluating:  33%|      | 54/165 [00:07<00:13,  8.40it/s]Evaluating:  33%|      | 55/165 [00:07<00:13,  8.38it/s]Evaluating:  34%|      | 56/165 [00:07<00:12,  8.39it/s]Evaluating:  35%|      | 57/165 [00:07<00:12,  8.39it/s]Evaluating:  35%|      | 58/165 [00:07<00:12,  8.39it/s]Evaluating:  36%|      | 59/165 [00:07<00:12,  8.38it/s]Evaluating:  36%|      | 60/165 [00:07<00:12,  8.38it/s]Evaluating:  37%|      | 61/165 [00:07<00:12,  8.36it/s]Evaluating:  38%|      | 62/165 [00:08<00:12,  8.28it/s]Evaluating:  38%|      | 63/165 [00:08<00:12,  8.31it/s]Evaluating:  39%|      | 64/165 [00:08<00:12,  8.32it/s]Evaluating:  39%|      | 65/165 [00:08<00:11,  8.34it/s]Evaluating:  40%|      | 66/165 [00:08<00:11,  8.35it/s]Evaluating:  41%|      | 67/165 [00:08<00:11,  8.38it/s]Evaluating:  41%|      | 68/165 [00:08<00:11,  8.40it/s]Evaluating:  42%|     | 69/165 [00:08<00:11,  8.39it/s]Evaluating:  42%|     | 70/165 [00:08<00:11,  8.27it/s]Evaluating:  43%|     | 71/165 [00:09<00:11,  8.27it/s]Evaluating:  44%|     | 72/165 [00:09<00:11,  8.26it/s]Evaluating:  44%|     | 73/165 [00:09<00:11,  8.26it/s]Evaluating:  45%|     | 74/165 [00:09<00:11,  8.25it/s]Evaluating:  45%|     | 75/165 [00:09<00:10,  8.32it/s]Evaluating:  46%|     | 76/165 [00:09<00:10,  8.36it/s]Evaluating:  47%|     | 77/165 [00:09<00:10,  8.30it/s]Evaluating:  47%|     | 78/165 [00:09<00:10,  8.36it/s]Evaluating:  48%|     | 79/165 [00:10<00:10,  8.34it/s]Evaluating:  48%|     | 80/165 [00:10<00:10,  8.28it/s]Evaluating:  49%|     | 81/165 [00:10<00:10,  8.29it/s]Evaluating:  50%|     | 82/165 [00:10<00:10,  8.29it/s]Evaluating:  50%|     | 83/165 [00:10<00:09,  8.30it/s]Evaluating:  51%|     | 84/165 [00:10<00:09,  8.30it/s]Evaluating:  52%|    | 85/165 [00:10<00:09,  8.32it/s]Evaluating:  52%|    | 86/165 [00:10<00:09,  8.30it/s]Evaluating:  53%|    | 87/165 [00:11<00:09,  8.34it/s]Evaluating:  53%|    | 88/165 [00:11<00:09,  8.35it/s]Evaluating:  54%|    | 89/165 [00:11<00:09,  8.38it/s]Evaluating:  55%|    | 90/165 [00:11<00:08,  8.40it/s]Evaluating:  55%|    | 91/165 [00:11<00:08,  8.41it/s]Evaluating:  56%|    | 92/165 [00:11<00:08,  8.41it/s]Evaluating:  56%|    | 93/165 [00:11<00:08,  8.40it/s]Evaluating:  57%|    | 94/165 [00:11<00:08,  8.40it/s]Evaluating:  58%|    | 95/165 [00:11<00:08,  8.41it/s]Evaluating:  58%|    | 96/165 [00:12<00:08,  8.42it/s]Evaluating:  59%|    | 97/165 [00:12<00:08,  8.41it/s]Evaluating:  59%|    | 98/165 [00:12<00:08,  8.31it/s]Evaluating:  60%|    | 99/165 [00:12<00:07,  8.35it/s]Evaluating:  61%|    | 100/165 [00:12<00:07,  8.38it/s]Evaluating:  61%|    | 101/165 [00:12<00:07,  8.40it/s]Evaluating:  62%|   | 102/165 [00:12<00:07,  8.37it/s]Evaluating:  62%|   | 103/165 [00:12<00:07,  8.37it/s]Evaluating:  63%|   | 104/165 [00:13<00:07,  8.33it/s]Evaluating:  64%|   | 105/165 [00:13<00:07,  8.32it/s]Evaluating:  64%|   | 106/165 [00:13<00:07,  8.33it/s]Evaluating:  65%|   | 107/165 [00:13<00:06,  8.35it/s]Evaluating:  65%|   | 108/165 [00:13<00:06,  8.30it/s]Evaluating:  66%|   | 109/165 [00:13<00:06,  8.32it/s]Evaluating:  67%|   | 110/165 [00:13<00:06,  8.34it/s]Evaluating:  67%|   | 111/165 [00:13<00:06,  8.29it/s]Evaluating:  68%|   | 112/165 [00:13<00:06,  8.25it/s]Evaluating:  68%|   | 113/165 [00:14<00:06,  8.22it/s]Evaluating:  69%|   | 114/165 [00:14<00:06,  8.25it/s]Evaluating:  70%|   | 115/165 [00:14<00:06,  8.31it/s]Evaluating:  70%|   | 116/165 [00:14<00:05,  8.35it/s]Evaluating:  71%|   | 117/165 [00:14<00:05,  8.37it/s]Evaluating:  72%|  | 118/165 [00:14<00:05,  8.39it/s]Evaluating:  72%|  | 119/165 [00:14<00:05,  8.40it/s]Evaluating:  73%|  | 120/165 [00:14<00:05,  8.38it/s]Evaluating:  73%|  | 121/165 [00:15<00:05,  8.27it/s]Evaluating:  74%|  | 122/165 [00:15<00:05,  8.27it/s]Evaluating:  75%|  | 123/165 [00:15<00:05,  8.31it/s]Evaluating:  75%|  | 124/165 [00:15<00:04,  8.35it/s]Evaluating:  76%|  | 125/165 [00:15<00:04,  8.38it/s]Evaluating:  76%|  | 126/165 [00:15<00:04,  8.37it/s]Evaluating:  77%|  | 127/165 [00:15<00:04,  8.35it/s]Evaluating:  78%|  | 128/165 [00:15<00:04,  8.36it/s]Evaluating:  78%|  | 129/165 [00:16<00:04,  8.35it/s]Evaluating:  79%|  | 130/165 [00:16<00:04,  8.31it/s]Evaluating:  79%|  | 131/165 [00:16<00:04,  8.27it/s]Evaluating:  80%|  | 132/165 [00:16<00:03,  8.31it/s]Evaluating:  81%|  | 133/165 [00:16<00:03,  8.32it/s]Evaluating:  81%|  | 134/165 [00:16<00:03,  8.37it/s]Evaluating:  82%| | 135/165 [00:16<00:03,  8.38it/s]Evaluating:  82%| | 136/165 [00:16<00:03,  8.35it/s]Evaluating:  83%| | 137/165 [00:16<00:03,  8.28it/s]Evaluating:  84%| | 138/165 [00:17<00:03,  8.33it/s]Evaluating:  84%| | 139/165 [00:17<00:03,  8.36it/s]Evaluating:  85%| | 140/165 [00:17<00:02,  8.40it/s]Evaluating:  85%| | 141/165 [00:17<00:02,  8.42it/s]Evaluating:  86%| | 142/165 [00:17<00:02,  8.44it/s]Evaluating:  87%| | 143/165 [00:17<00:02,  8.45it/s]Evaluating:  87%| | 144/165 [00:17<00:02,  8.45it/s]Evaluating:  88%| | 145/165 [00:17<00:02,  8.44it/s]Evaluating:  88%| | 146/165 [00:18<00:02,  8.46it/s]Evaluating:  89%| | 147/165 [00:18<00:02,  8.47it/s]Evaluating:  90%| | 148/165 [00:18<00:02,  8.48it/s]Evaluating:  90%| | 149/165 [00:18<00:01,  8.49it/s]Evaluating:  91%| | 150/165 [00:18<00:01,  8.49it/s]Evaluating:  92%|| 151/165 [00:18<00:01,  8.48it/s]Evaluating:  92%|| 152/165 [00:18<00:01,  8.47it/s]Evaluating:  93%|| 153/165 [00:18<00:01,  8.43it/s]Evaluating:  93%|| 154/165 [00:19<00:01,  8.45it/s]Evaluating:  94%|| 155/165 [00:19<00:01,  8.44it/s]Evaluating:  95%|| 156/165 [00:19<00:01,  8.38it/s]Evaluating:  95%|| 157/165 [00:19<00:00,  8.38it/s]Evaluating:  96%|| 158/165 [00:19<00:00,  8.36it/s]Evaluating:  96%|| 159/165 [00:19<00:00,  8.39it/s]Evaluating:  97%|| 160/165 [00:19<00:00,  8.39it/s]Evaluating:  98%|| 161/165 [00:19<00:00,  8.36it/s]Evaluating:  98%|| 162/165 [00:19<00:00,  8.37it/s]Evaluating:  99%|| 163/165 [00:20<00:00,  8.33it/s]Evaluating:  99%|| 164/165 [00:20<00:00,  8.36it/s]Evaluating: 100%|| 165/165 [00:20<00:00,  8.15it/s]
05/08/2022 20:39:44 - INFO - __main__ -     Evaluation done in total 20.238433 secs (0.015402 sec per example)
05/08/2022 20:39:48 - INFO - __main__ -   Results: {'exact': 53.69747899159664, 'f1': 72.93386131481397, 'total': 1190, 'HasAns_exact': 53.69747899159664, 'HasAns_f1': 72.93386131481397, 'HasAns_total': 1190, 'best_exact': 53.69747899159664, 'best_exact_thresh': 0.0, 'best_f1': 72.93386131481397, 'best_f1_thresh': 0.0}
  th 
2022-05-08 20:39:50.825963: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:39:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'qa_outputs.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'qa_outputs.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.attention.output.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.3.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:40:03 - INFO - __main__ -   lang2id = None
05/08/2022 20:40:07 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='th', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.th.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:40:07 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:40:07 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'qa_outputs.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'qa_outputs.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.attention.output.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.3.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:40:20 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 27%|       | 13/48 [00:00<00:00, 127.44it/s] 54%|    | 26/48 [00:00<00:00, 99.86it/s]  77%|  | 37/48 [00:00<00:00, 100.23it/s]100%|| 48/48 [00:00<00:00, 100.94it/s]100%|| 48/48 [00:00<00:00, 102.36it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<03:13,  6.16it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:05, 218.27it/s]convert squad examples to features:  11%|         | 129/1190 [00:00<00:03, 310.27it/s]convert squad examples to features:  16%|        | 193/1190 [00:00<00:02, 401.00it/s]convert squad examples to features:  22%|       | 257/1190 [00:00<00:02, 386.59it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:02, 378.56it/s]convert squad examples to features:  32%|      | 385/1190 [00:01<00:03, 247.99it/s]convert squad examples to features:  35%|      | 417/1190 [00:01<00:03, 248.01it/s]convert squad examples to features:  40%|      | 481/1190 [00:01<00:02, 262.09it/s]convert squad examples to features:  46%|     | 545/1190 [00:02<00:02, 218.36it/s]convert squad examples to features:  51%|     | 609/1190 [00:02<00:02, 269.37it/s]convert squad examples to features:  57%|    | 673/1190 [00:02<00:01, 275.53it/s]convert squad examples to features:  59%|    | 706/1190 [00:02<00:01, 275.74it/s]convert squad examples to features:  62%|   | 737/1190 [00:02<00:01, 274.79it/s]convert squad examples to features:  67%|   | 801/1190 [00:02<00:01, 300.01it/s]convert squad examples to features:  70%|   | 833/1190 [00:02<00:01, 288.34it/s]convert squad examples to features:  73%|  | 865/1190 [00:03<00:01, 288.08it/s]convert squad examples to features:  75%|  | 897/1190 [00:03<00:01, 268.17it/s]convert squad examples to features:  81%|  | 961/1190 [00:03<00:00, 297.23it/s]convert squad examples to features:  86%| | 1025/1190 [00:03<00:00, 321.14it/s]convert squad examples to features:  92%|| 1089/1190 [00:03<00:00, 321.62it/s]convert squad examples to features:  94%|| 1121/1190 [00:03<00:00, 316.97it/s]convert squad examples to features:  97%|| 1153/1190 [00:04<00:00, 312.52it/s]convert squad examples to features: 100%|| 1190/1190 [00:04<00:00, 295.51it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 551089.96it/s]
05/08/2022 20:40:25 - INFO - __main__ -   Saving features into cached file ./cached_xquad.th.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_th
05/08/2022 20:40:26 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:40:26 - INFO - __main__ -     Num examples = 1314
05/08/2022 20:40:26 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]Evaluating:   1%|          | 1/165 [00:00<01:46,  1.53it/s]Evaluating:   1%|          | 2/165 [00:00<00:55,  2.92it/s]Evaluating:   2%|         | 3/165 [00:00<00:39,  4.15it/s]Evaluating:   2%|         | 4/165 [00:01<00:31,  5.17it/s]Evaluating:   3%|         | 5/165 [00:01<00:26,  6.03it/s]Evaluating:   4%|         | 6/165 [00:01<00:23,  6.72it/s]Evaluating:   4%|         | 7/165 [00:01<00:21,  7.24it/s]Evaluating:   5%|         | 8/165 [00:01<00:20,  7.62it/s]Evaluating:   5%|         | 9/165 [00:01<00:19,  7.90it/s]Evaluating:   6%|         | 10/165 [00:01<00:19,  8.00it/s]Evaluating:   7%|         | 11/165 [00:01<00:18,  8.14it/s]Evaluating:   7%|         | 12/165 [00:01<00:18,  8.26it/s]Evaluating:   8%|         | 13/165 [00:02<00:18,  8.35it/s]Evaluating:   8%|         | 14/165 [00:02<00:17,  8.42it/s]Evaluating:   9%|         | 15/165 [00:02<00:17,  8.45it/s]Evaluating:  10%|         | 16/165 [00:02<00:17,  8.42it/s]Evaluating:  10%|         | 17/165 [00:02<00:17,  8.40it/s]Evaluating:  11%|         | 18/165 [00:02<00:17,  8.41it/s]Evaluating:  12%|        | 19/165 [00:02<00:17,  8.41it/s]Evaluating:  12%|        | 20/165 [00:02<00:17,  8.44it/s]Evaluating:  13%|        | 21/165 [00:03<00:17,  8.46it/s]Evaluating:  13%|        | 22/165 [00:03<00:16,  8.47it/s]Evaluating:  14%|        | 23/165 [00:03<00:16,  8.48it/s]Evaluating:  15%|        | 24/165 [00:03<00:16,  8.48it/s]Evaluating:  15%|        | 25/165 [00:03<00:17,  7.89it/s]Evaluating:  16%|        | 26/165 [00:03<00:17,  8.07it/s]Evaluating:  16%|        | 27/165 [00:03<00:16,  8.18it/s]Evaluating:  17%|        | 28/165 [00:03<00:16,  8.24it/s]Evaluating:  18%|        | 29/165 [00:03<00:16,  8.23it/s]Evaluating:  18%|        | 30/165 [00:04<00:16,  8.32it/s]Evaluating:  19%|        | 31/165 [00:04<00:16,  8.36it/s]Evaluating:  19%|        | 32/165 [00:04<00:15,  8.38it/s]Evaluating:  20%|        | 33/165 [00:04<00:15,  8.37it/s]Evaluating:  21%|        | 34/165 [00:04<00:15,  8.42it/s]Evaluating:  21%|        | 35/165 [00:04<00:15,  8.45it/s]Evaluating:  22%|       | 36/165 [00:04<00:15,  8.45it/s]Evaluating:  22%|       | 37/165 [00:04<00:15,  8.45it/s]Evaluating:  23%|       | 38/165 [00:05<00:15,  8.42it/s]Evaluating:  24%|       | 39/165 [00:05<00:15,  8.35it/s]Evaluating:  24%|       | 40/165 [00:05<00:14,  8.39it/s]Evaluating:  25%|       | 41/165 [00:05<00:14,  8.38it/s]Evaluating:  25%|       | 42/165 [00:05<00:14,  8.41it/s]Evaluating:  26%|       | 43/165 [00:05<00:14,  8.39it/s]Evaluating:  27%|       | 44/165 [00:05<00:14,  8.38it/s]Evaluating:  27%|       | 45/165 [00:05<00:14,  8.41it/s]Evaluating:  28%|       | 46/165 [00:06<00:14,  8.41it/s]Evaluating:  28%|       | 47/165 [00:06<00:13,  8.45it/s]Evaluating:  29%|       | 48/165 [00:06<00:13,  8.44it/s]Evaluating:  30%|       | 49/165 [00:06<00:13,  8.46it/s]Evaluating:  30%|       | 50/165 [00:06<00:13,  8.46it/s]Evaluating:  31%|       | 51/165 [00:06<00:13,  8.46it/s]Evaluating:  32%|      | 52/165 [00:06<00:13,  8.44it/s]Evaluating:  32%|      | 53/165 [00:06<00:13,  8.39it/s]Evaluating:  33%|      | 54/165 [00:06<00:13,  8.37it/s]Evaluating:  33%|      | 55/165 [00:07<00:13,  8.37it/s]Evaluating:  34%|      | 56/165 [00:07<00:13,  8.34it/s]Evaluating:  35%|      | 57/165 [00:07<00:12,  8.34it/s]Evaluating:  35%|      | 58/165 [00:07<00:12,  8.37it/s]Evaluating:  36%|      | 59/165 [00:07<00:12,  8.39it/s]Evaluating:  36%|      | 60/165 [00:07<00:12,  8.39it/s]Evaluating:  37%|      | 61/165 [00:07<00:12,  8.41it/s]Evaluating:  38%|      | 62/165 [00:07<00:12,  8.44it/s]Evaluating:  38%|      | 63/165 [00:08<00:12,  8.44it/s]Evaluating:  39%|      | 64/165 [00:08<00:11,  8.47it/s]Evaluating:  39%|      | 65/165 [00:08<00:11,  8.48it/s]Evaluating:  40%|      | 66/165 [00:08<00:11,  8.49it/s]Evaluating:  41%|      | 67/165 [00:08<00:11,  8.50it/s]Evaluating:  41%|      | 68/165 [00:08<00:11,  8.48it/s]Evaluating:  42%|     | 69/165 [00:08<00:11,  8.46it/s]Evaluating:  42%|     | 70/165 [00:08<00:11,  8.45it/s]Evaluating:  43%|     | 71/165 [00:08<00:11,  8.47it/s]Evaluating:  44%|     | 72/165 [00:09<00:10,  8.46it/s]Evaluating:  44%|     | 73/165 [00:09<00:10,  8.48it/s]Evaluating:  45%|     | 74/165 [00:09<00:10,  8.49it/s]Evaluating:  45%|     | 75/165 [00:09<00:10,  8.49it/s]Evaluating:  46%|     | 76/165 [00:09<00:10,  8.49it/s]Evaluating:  47%|     | 77/165 [00:09<00:10,  8.45it/s]Evaluating:  47%|     | 78/165 [00:09<00:10,  8.42it/s]Evaluating:  48%|     | 79/165 [00:09<00:10,  8.44it/s]Evaluating:  48%|     | 80/165 [00:10<00:10,  8.45it/s]Evaluating:  49%|     | 81/165 [00:10<00:09,  8.46it/s]Evaluating:  50%|     | 82/165 [00:10<00:09,  8.44it/s]Evaluating:  50%|     | 83/165 [00:10<00:09,  8.39it/s]Evaluating:  51%|     | 84/165 [00:10<00:09,  8.35it/s]Evaluating:  52%|    | 85/165 [00:10<00:09,  8.39it/s]Evaluating:  52%|    | 86/165 [00:10<00:09,  8.39it/s]Evaluating:  53%|    | 87/165 [00:10<00:09,  8.36it/s]Evaluating:  53%|    | 88/165 [00:10<00:09,  8.38it/s]Evaluating:  54%|    | 89/165 [00:11<00:09,  8.38it/s]Evaluating:  55%|    | 90/165 [00:11<00:08,  8.39it/s]Evaluating:  55%|    | 91/165 [00:11<00:08,  8.39it/s]Evaluating:  56%|    | 92/165 [00:11<00:08,  8.35it/s]Evaluating:  56%|    | 93/165 [00:11<00:08,  8.38it/s]Evaluating:  57%|    | 94/165 [00:11<00:08,  8.40it/s]Evaluating:  58%|    | 95/165 [00:11<00:08,  8.39it/s]Evaluating:  58%|    | 96/165 [00:11<00:08,  8.40it/s]Evaluating:  59%|    | 97/165 [00:12<00:08,  8.38it/s]Evaluating:  59%|    | 98/165 [00:12<00:08,  8.36it/s]Evaluating:  60%|    | 99/165 [00:12<00:07,  8.38it/s]Evaluating:  61%|    | 100/165 [00:12<00:07,  8.39it/s]Evaluating:  61%|    | 101/165 [00:12<00:07,  8.40it/s]Evaluating:  62%|   | 102/165 [00:12<00:07,  8.30it/s]Evaluating:  62%|   | 103/165 [00:12<00:07,  8.33it/s]Evaluating:  63%|   | 104/165 [00:12<00:07,  8.32it/s]Evaluating:  64%|   | 105/165 [00:13<00:07,  8.33it/s]Evaluating:  64%|   | 106/165 [00:13<00:07,  8.36it/s]Evaluating:  65%|   | 107/165 [00:13<00:06,  8.39it/s]Evaluating:  65%|   | 108/165 [00:13<00:06,  8.41it/s]Evaluating:  66%|   | 109/165 [00:13<00:06,  8.38it/s]Evaluating:  67%|   | 110/165 [00:13<00:06,  8.41it/s]Evaluating:  67%|   | 111/165 [00:13<00:06,  8.39it/s]Evaluating:  68%|   | 112/165 [00:13<00:06,  8.40it/s]Evaluating:  68%|   | 113/165 [00:13<00:06,  8.41it/s]Evaluating:  69%|   | 114/165 [00:14<00:06,  8.42it/s]Evaluating:  70%|   | 115/165 [00:14<00:05,  8.41it/s]Evaluating:  70%|   | 116/165 [00:14<00:05,  8.42it/s]Evaluating:  71%|   | 117/165 [00:14<00:05,  8.40it/s]Evaluating:  72%|  | 118/165 [00:14<00:05,  8.39it/s]Evaluating:  72%|  | 119/165 [00:14<00:05,  8.34it/s]Evaluating:  73%|  | 120/165 [00:14<00:05,  8.28it/s]Evaluating:  73%|  | 121/165 [00:14<00:05,  8.24it/s]Evaluating:  74%|  | 122/165 [00:15<00:05,  8.25it/s]Evaluating:  75%|  | 123/165 [00:15<00:05,  8.25it/s]Evaluating:  75%|  | 124/165 [00:15<00:04,  8.30it/s]Evaluating:  76%|  | 125/165 [00:15<00:04,  8.29it/s]Evaluating:  76%|  | 126/165 [00:15<00:04,  8.27it/s]Evaluating:  77%|  | 127/165 [00:15<00:04,  8.29it/s]Evaluating:  78%|  | 128/165 [00:15<00:04,  8.34it/s]Evaluating:  78%|  | 129/165 [00:15<00:04,  8.34it/s]Evaluating:  79%|  | 130/165 [00:16<00:04,  8.35it/s]Evaluating:  79%|  | 131/165 [00:16<00:04,  8.36it/s]Evaluating:  80%|  | 132/165 [00:16<00:03,  8.33it/s]Evaluating:  81%|  | 133/165 [00:16<00:03,  8.35it/s]Evaluating:  81%|  | 134/165 [00:16<00:03,  8.33it/s]Evaluating:  82%| | 135/165 [00:16<00:03,  8.38it/s]Evaluating:  82%| | 136/165 [00:16<00:03,  8.41it/s]Evaluating:  83%| | 137/165 [00:16<00:03,  8.42it/s]Evaluating:  84%| | 138/165 [00:16<00:03,  8.41it/s]Evaluating:  84%| | 139/165 [00:17<00:03,  8.28it/s]Evaluating:  85%| | 140/165 [00:17<00:03,  8.30it/s]Evaluating:  85%| | 141/165 [00:17<00:02,  8.35it/s]Evaluating:  86%| | 142/165 [00:17<00:02,  8.39it/s]Evaluating:  87%| | 143/165 [00:17<00:02,  8.41it/s]Evaluating:  87%| | 144/165 [00:17<00:02,  8.35it/s]Evaluating:  88%| | 145/165 [00:17<00:02,  8.33it/s]Evaluating:  88%| | 146/165 [00:17<00:02,  8.28it/s]Evaluating:  89%| | 147/165 [00:18<00:02,  8.27it/s]Evaluating:  90%| | 148/165 [00:18<00:02,  8.26it/s]Evaluating:  90%| | 149/165 [00:18<00:01,  8.25it/s]Evaluating:  91%| | 150/165 [00:18<00:01,  8.29it/s]Evaluating:  92%|| 151/165 [00:18<00:01,  8.32it/s]Evaluating:  92%|| 152/165 [00:18<00:01,  8.37it/s]Evaluating:  93%|| 153/165 [00:18<00:01,  8.38it/s]Evaluating:  93%|| 154/165 [00:18<00:01,  8.37it/s]Evaluating:  94%|| 155/165 [00:19<00:01,  8.38it/s]Evaluating:  95%|| 156/165 [00:19<00:01,  8.33it/s]Evaluating:  95%|| 157/165 [00:19<00:00,  8.32it/s]Evaluating:  96%|| 158/165 [00:19<00:00,  8.27it/s]Evaluating:  96%|| 159/165 [00:19<00:00,  8.22it/s]Evaluating:  97%|| 160/165 [00:19<00:00,  8.26it/s]Evaluating:  98%|| 161/165 [00:19<00:00,  8.28it/s]Evaluating:  98%|| 162/165 [00:19<00:00,  8.27it/s]Evaluating:  99%|| 163/165 [00:19<00:00,  8.29it/s]Evaluating:  99%|| 164/165 [00:20<00:00,  8.29it/s]Evaluating: 100%|| 165/165 [00:20<00:00,  8.19it/s]
05/08/2022 20:40:46 - INFO - __main__ -     Evaluation done in total 20.147642 secs (0.015333 sec per example)
05/08/2022 20:40:52 - INFO - __main__ -   Results: {'exact': 57.142857142857146, 'f1': 66.8156171559532, 'total': 1190, 'HasAns_exact': 57.142857142857146, 'HasAns_f1': 66.8156171559532, 'HasAns_total': 1190, 'best_exact': 57.142857142857146, 'best_exact_thresh': 0.0, 'best_f1': 66.8156171559532, 'best_f1_thresh': 0.0}
  zh 
2022-05-08 20:40:55.181047: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:40:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'qa_outputs.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'qa_outputs.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:41:07 - INFO - __main__ -   lang2id = None
05/08/2022 20:41:11 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='zh', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.zh.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:41:11 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:41:11 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'qa_outputs.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'qa_outputs.weight', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.pooler.dense.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:41:25 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 48%|     | 23/48 [00:00<00:00, 228.94it/s]100%|| 48/48 [00:00<00:00, 254.13it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   3%|         | 33/1190 [00:00<00:04, 256.67it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:03, 316.11it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:02, 426.09it/s]convert squad examples to features:  22%|       | 257/1190 [00:00<00:01, 542.08it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:01, 509.70it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:02, 361.83it/s]convert squad examples to features:  38%|      | 449/1190 [00:01<00:01, 381.65it/s]convert squad examples to features:  43%|     | 513/1190 [00:01<00:01, 398.58it/s]convert squad examples to features:  48%|     | 577/1190 [00:01<00:01, 317.17it/s]convert squad examples to features:  57%|    | 673/1190 [00:01<00:01, 377.20it/s]convert squad examples to features:  62%|   | 737/1190 [00:01<00:01, 417.05it/s]convert squad examples to features:  67%|   | 801/1190 [00:01<00:00, 459.36it/s]convert squad examples to features:  73%|  | 865/1190 [00:02<00:00, 453.42it/s]convert squad examples to features:  78%|  | 929/1190 [00:02<00:00, 452.67it/s]convert squad examples to features:  83%| | 993/1190 [00:02<00:00, 478.95it/s]convert squad examples to features:  92%|| 1089/1190 [00:02<00:00, 514.22it/s]convert squad examples to features:  97%|| 1153/1190 [00:02<00:00, 514.58it/s]convert squad examples to features: 100%|| 1190/1190 [00:02<00:00, 448.85it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 664433.14it/s]
05/08/2022 20:41:28 - INFO - __main__ -   Saving features into cached file ./cached_xquad.zh.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_zh
05/08/2022 20:41:29 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:41:29 - INFO - __main__ -     Num examples = 1246
05/08/2022 20:41:29 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/156 [00:00<?, ?it/s]Evaluating:   1%|          | 1/156 [00:00<01:52,  1.37it/s]Evaluating:   1%|         | 2/156 [00:00<00:56,  2.71it/s]Evaluating:   2%|         | 3/156 [00:00<00:39,  3.90it/s]Evaluating:   3%|         | 4/156 [00:01<00:30,  4.94it/s]Evaluating:   3%|         | 5/156 [00:01<00:25,  5.85it/s]Evaluating:   4%|         | 6/156 [00:01<00:22,  6.56it/s]Evaluating:   4%|         | 7/156 [00:01<00:21,  7.06it/s]Evaluating:   5%|         | 8/156 [00:01<00:19,  7.46it/s]Evaluating:   6%|         | 9/156 [00:01<00:18,  7.75it/s]Evaluating:   6%|         | 10/156 [00:01<00:18,  7.93it/s]Evaluating:   7%|         | 11/156 [00:01<00:17,  8.11it/s]Evaluating:   8%|         | 12/156 [00:02<00:17,  8.22it/s]Evaluating:   8%|         | 13/156 [00:02<00:17,  8.26it/s]Evaluating:   9%|         | 14/156 [00:02<00:17,  8.30it/s]Evaluating:  10%|         | 15/156 [00:02<00:16,  8.36it/s]Evaluating:  10%|         | 16/156 [00:02<00:16,  8.40it/s]Evaluating:  11%|         | 17/156 [00:02<00:16,  8.41it/s]Evaluating:  12%|        | 18/156 [00:02<00:16,  8.34it/s]Evaluating:  12%|        | 19/156 [00:02<00:16,  8.32it/s]Evaluating:  13%|        | 20/156 [00:02<00:16,  8.37it/s]Evaluating:  13%|        | 21/156 [00:03<00:16,  8.39it/s]Evaluating:  14%|        | 22/156 [00:03<00:15,  8.44it/s]Evaluating:  15%|        | 23/156 [00:03<00:15,  8.48it/s]Evaluating:  15%|        | 24/156 [00:03<00:15,  8.51it/s]Evaluating:  16%|        | 25/156 [00:03<00:15,  8.43it/s]Evaluating:  17%|        | 26/156 [00:03<00:15,  8.45it/s]Evaluating:  17%|        | 27/156 [00:03<00:15,  8.38it/s]Evaluating:  18%|        | 28/156 [00:03<00:15,  8.32it/s]Evaluating:  19%|        | 29/156 [00:04<00:15,  8.33it/s]Evaluating:  19%|        | 30/156 [00:04<00:15,  8.29it/s]Evaluating:  20%|        | 31/156 [00:04<00:14,  8.35it/s]Evaluating:  21%|        | 32/156 [00:04<00:14,  8.41it/s]Evaluating:  21%|        | 33/156 [00:04<00:14,  8.45it/s]Evaluating:  22%|       | 34/156 [00:04<00:14,  8.48it/s]Evaluating:  22%|       | 35/156 [00:04<00:14,  8.44it/s]Evaluating:  23%|       | 36/156 [00:04<00:14,  8.36it/s]Evaluating:  24%|       | 37/156 [00:05<00:14,  8.31it/s]Evaluating:  24%|       | 38/156 [00:05<00:14,  8.31it/s]Evaluating:  25%|       | 39/156 [00:05<00:13,  8.38it/s]Evaluating:  26%|       | 40/156 [00:05<00:14,  7.84it/s]Evaluating:  26%|       | 41/156 [00:05<00:14,  8.02it/s]Evaluating:  27%|       | 42/156 [00:05<00:13,  8.15it/s]Evaluating:  28%|       | 43/156 [00:05<00:13,  8.22it/s]Evaluating:  28%|       | 44/156 [00:05<00:13,  8.32it/s]Evaluating:  29%|       | 45/156 [00:05<00:13,  8.40it/s]Evaluating:  29%|       | 46/156 [00:06<00:13,  8.44it/s]Evaluating:  30%|       | 47/156 [00:06<00:12,  8.45it/s]Evaluating:  31%|       | 48/156 [00:06<00:12,  8.44it/s]Evaluating:  31%|      | 49/156 [00:06<00:12,  8.45it/s]Evaluating:  32%|      | 50/156 [00:06<00:12,  8.43it/s]Evaluating:  33%|      | 51/156 [00:06<00:12,  8.34it/s]Evaluating:  33%|      | 52/156 [00:06<00:12,  8.34it/s]Evaluating:  34%|      | 53/156 [00:06<00:12,  8.37it/s]Evaluating:  35%|      | 54/156 [00:07<00:12,  8.36it/s]Evaluating:  35%|      | 55/156 [00:07<00:12,  8.34it/s]Evaluating:  36%|      | 56/156 [00:07<00:11,  8.37it/s]Evaluating:  37%|      | 57/156 [00:07<00:11,  8.41it/s]Evaluating:  37%|      | 58/156 [00:07<00:11,  8.41it/s]Evaluating:  38%|      | 59/156 [00:07<00:11,  8.37it/s]Evaluating:  38%|      | 60/156 [00:07<00:11,  8.40it/s]Evaluating:  39%|      | 61/156 [00:07<00:11,  8.45it/s]Evaluating:  40%|      | 62/156 [00:08<00:11,  8.47it/s]Evaluating:  40%|      | 63/156 [00:08<00:11,  8.43it/s]Evaluating:  41%|      | 64/156 [00:08<00:10,  8.37it/s]Evaluating:  42%|     | 65/156 [00:08<00:10,  8.39it/s]Evaluating:  42%|     | 66/156 [00:08<00:10,  8.38it/s]Evaluating:  43%|     | 67/156 [00:08<00:10,  8.31it/s]Evaluating:  44%|     | 68/156 [00:08<00:10,  8.32it/s]Evaluating:  44%|     | 69/156 [00:08<00:10,  8.32it/s]Evaluating:  45%|     | 70/156 [00:08<00:10,  8.37it/s]Evaluating:  46%|     | 71/156 [00:09<00:10,  8.41it/s]Evaluating:  46%|     | 72/156 [00:09<00:09,  8.42it/s]Evaluating:  47%|     | 73/156 [00:09<00:09,  8.43it/s]Evaluating:  47%|     | 74/156 [00:09<00:09,  8.46it/s]Evaluating:  48%|     | 75/156 [00:09<00:09,  8.48it/s]Evaluating:  49%|     | 76/156 [00:09<00:09,  8.48it/s]Evaluating:  49%|     | 77/156 [00:09<00:09,  8.49it/s]Evaluating:  50%|     | 78/156 [00:09<00:09,  8.50it/s]Evaluating:  51%|     | 79/156 [00:10<00:09,  8.47it/s]Evaluating:  51%|    | 80/156 [00:10<00:09,  8.36it/s]Evaluating:  52%|    | 81/156 [00:10<00:09,  8.32it/s]Evaluating:  53%|    | 82/156 [00:10<00:08,  8.37it/s]Evaluating:  53%|    | 83/156 [00:10<00:08,  8.39it/s]Evaluating:  54%|    | 84/156 [00:10<00:08,  8.31it/s]Evaluating:  54%|    | 85/156 [00:10<00:08,  8.32it/s]Evaluating:  55%|    | 86/156 [00:10<00:08,  8.36it/s]Evaluating:  56%|    | 87/156 [00:10<00:08,  8.38it/s]Evaluating:  56%|    | 88/156 [00:11<00:08,  8.41it/s]Evaluating:  57%|    | 89/156 [00:11<00:07,  8.44it/s]Evaluating:  58%|    | 90/156 [00:11<00:07,  8.46it/s]Evaluating:  58%|    | 91/156 [00:11<00:07,  8.43it/s]Evaluating:  59%|    | 92/156 [00:11<00:07,  8.40it/s]Evaluating:  60%|    | 93/156 [00:11<00:07,  8.44it/s]Evaluating:  60%|    | 94/156 [00:11<00:07,  8.46it/s]Evaluating:  61%|    | 95/156 [00:11<00:07,  8.48it/s]Evaluating:  62%|   | 96/156 [00:12<00:07,  8.48it/s]Evaluating:  62%|   | 97/156 [00:12<00:06,  8.49it/s]Evaluating:  63%|   | 98/156 [00:12<00:06,  8.50it/s]Evaluating:  63%|   | 99/156 [00:12<00:06,  8.49it/s]Evaluating:  64%|   | 100/156 [00:12<00:06,  8.49it/s]Evaluating:  65%|   | 101/156 [00:12<00:06,  8.51it/s]Evaluating:  65%|   | 102/156 [00:12<00:06,  8.51it/s]Evaluating:  66%|   | 103/156 [00:12<00:06,  8.53it/s]Evaluating:  67%|   | 104/156 [00:12<00:06,  8.53it/s]Evaluating:  67%|   | 105/156 [00:13<00:05,  8.53it/s]Evaluating:  68%|   | 106/156 [00:13<00:05,  8.52it/s]Evaluating:  69%|   | 107/156 [00:13<00:05,  8.52it/s]Evaluating:  69%|   | 108/156 [00:13<00:05,  8.52it/s]Evaluating:  70%|   | 109/156 [00:13<00:05,  8.49it/s]Evaluating:  71%|   | 110/156 [00:13<00:05,  8.50it/s]Evaluating:  71%|   | 111/156 [00:13<00:05,  8.47it/s]Evaluating:  72%|  | 112/156 [00:13<00:05,  8.42it/s]Evaluating:  72%|  | 113/156 [00:14<00:05,  8.41it/s]Evaluating:  73%|  | 114/156 [00:14<00:05,  8.39it/s]Evaluating:  74%|  | 115/156 [00:14<00:04,  8.36it/s]Evaluating:  74%|  | 116/156 [00:14<00:04,  8.38it/s]Evaluating:  75%|  | 117/156 [00:14<00:04,  8.39it/s]Evaluating:  76%|  | 118/156 [00:14<00:04,  8.32it/s]Evaluating:  76%|  | 119/156 [00:14<00:04,  8.30it/s]Evaluating:  77%|  | 120/156 [00:14<00:04,  8.28it/s]Evaluating:  78%|  | 121/156 [00:15<00:04,  8.33it/s]Evaluating:  78%|  | 122/156 [00:15<00:04,  8.37it/s]Evaluating:  79%|  | 123/156 [00:15<00:03,  8.37it/s]Evaluating:  79%|  | 124/156 [00:15<00:03,  8.38it/s]Evaluating:  80%|  | 125/156 [00:15<00:03,  8.36it/s]Evaluating:  81%|  | 126/156 [00:15<00:03,  8.39it/s]Evaluating:  81%| | 127/156 [00:15<00:03,  8.42it/s]Evaluating:  82%| | 128/156 [00:15<00:03,  8.39it/s]Evaluating:  83%| | 129/156 [00:15<00:03,  8.35it/s]Evaluating:  83%| | 130/156 [00:16<00:03,  8.39it/s]Evaluating:  84%| | 131/156 [00:16<00:02,  8.40it/s]Evaluating:  85%| | 132/156 [00:16<00:02,  8.36it/s]Evaluating:  85%| | 133/156 [00:16<00:02,  8.39it/s]Evaluating:  86%| | 134/156 [00:16<00:02,  8.40it/s]Evaluating:  87%| | 135/156 [00:16<00:02,  8.41it/s]Evaluating:  87%| | 136/156 [00:16<00:02,  8.42it/s]Evaluating:  88%| | 137/156 [00:16<00:02,  8.39it/s]Evaluating:  88%| | 138/156 [00:17<00:02,  8.38it/s]Evaluating:  89%| | 139/156 [00:17<00:02,  8.41it/s]Evaluating:  90%| | 140/156 [00:17<00:01,  8.45it/s]Evaluating:  90%| | 141/156 [00:17<00:01,  8.46it/s]Evaluating:  91%| | 142/156 [00:17<00:01,  8.37it/s]Evaluating:  92%|| 143/156 [00:17<00:01,  8.30it/s]Evaluating:  92%|| 144/156 [00:17<00:01,  8.34it/s]Evaluating:  93%|| 145/156 [00:17<00:01,  8.32it/s]Evaluating:  94%|| 146/156 [00:17<00:01,  8.36it/s]Evaluating:  94%|| 147/156 [00:18<00:01,  8.38it/s]Evaluating:  95%|| 148/156 [00:18<00:00,  8.42it/s]Evaluating:  96%|| 149/156 [00:18<00:00,  8.44it/s]Evaluating:  96%|| 150/156 [00:18<00:00,  8.45it/s]Evaluating:  97%|| 151/156 [00:18<00:00,  8.41it/s]Evaluating:  97%|| 152/156 [00:18<00:00,  8.42it/s]Evaluating:  98%|| 153/156 [00:18<00:00,  8.40it/s]Evaluating:  99%|| 154/156 [00:18<00:00,  8.41it/s]Evaluating:  99%|| 155/156 [00:19<00:00,  8.40it/s]Evaluating: 100%|| 156/156 [00:19<00:00,  8.14it/s]
05/08/2022 20:41:48 - INFO - __main__ -     Evaluation done in total 19.155924 secs (0.015374 sec per example)
05/08/2022 20:42:14 - INFO - __main__ -   Results: {'exact': 43.865546218487395, 'f1': 52.129251700680236, 'total': 1190, 'HasAns_exact': 43.865546218487395, 'HasAns_f1': 52.129251700680236, 'HasAns_total': 1190, 'best_exact': 43.865546218487395, 'best_exact_thresh': 0.0, 'best_f1': 52.129251700680236, 'best_f1_thresh': 0.0}
  hi 
2022-05-08 20:42:17.244483: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
05/08/2022 20:42:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
You are using a model of type xlm-roberta to instantiate a model of type xlm. This is not supported for all configurations of models and can yield errors.
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.pooler.dense.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'qa_outputs.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'qa_outputs.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:42:30 - INFO - __main__ -   lang2id = None
05/08/2022 20:42:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, eval_lang='hi', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='xlm-KG', model_name_or_path='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4', model_type='xlm', modelkg_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/adapter/xlm_adapter', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4/predictions/xquad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/download//xquad//xquad.hi.json', save_steps=50, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, train_lang='en', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)
05/08/2022 20:42:33 - INFO - __main__ -   Loading checkpoint /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 for evaluation
05/08/2022 20:42:33 - INFO - __main__ -   Evaluate the following checkpoints: ['/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4']
Some weights of the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 were not used when initializing XLMRobertaModel: ['base_model.encoder.layer.4.output.dense.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.key.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.encoder.layer.9.intermediate.dense.bias', 'base_model.encoder.layer.5.output.dense.weight', 'base_model.encoder.layer.1.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.attention.output.dense.bias', 'base_model.encoder.layer.4.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.attention.self.query.weight', 'base_model.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.dense.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.query.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.value.bias', 'base_model.encoder.layer.2.attention.self.value.bias', 'base_model.embeddings.position_ids', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.intermediate.dense.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.intermediate.dense.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.11.attention.self.query.weight', 'base_model.encoder.layer.4.intermediate.dense.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.output.LayerNorm.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.3.attention.output.dense.weight', 'base_model.encoder.layer.4.attention.self.key.weight', 'base_model.encoder.layer.0.attention.output.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.self.query.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.intermediate.dense.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.attention.self.key.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.1.attention.self.value.weight', 'base_model.encoder.layer.9.output.dense.weight', 'base_model.encoder.layer.7.attention.self.value.bias', 'base_model.encoder.layer.0.attention.self.query.bias', 'base_model.encoder.layer.7.attention.self.key.bias', 'base_model.encoder.layer.9.attention.output.dense.weight', 'base_model.encoder.layer.7.output.dense.weight', 'base_model.encoder.layer.0.output.LayerNorm.weight', 'base_model.encoder.layer.8.attention.output.dense.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.4.attention.self.query.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.dense.weight', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.encoder.layer.11.attention.self.value.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.intermediate.dense.bias', 'base_model.encoder.layer.9.attention.self.key.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.5.attention.self.query.bias', 'base_model.encoder.layer.8.attention.self.key.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.attention.self.query.bias', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.1.output.dense.weight', 'base_model.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.query.bias', 'base_model.encoder.layer.7.output.dense.bias', 'base_model.encoder.layer.11.intermediate.dense.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.8.intermediate.dense.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.token_type_embeddings.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.0.attention.self.key.bias', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.bias', 'base_model.pooler.dense.bias', 'base_model.pooler.dense.weight', 'base_model.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.encoder.layer.10.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.10.output.dense.bias', 'base_model.encoder.layer.3.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.self.query.bias', 'base_model.encoder.layer.5.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.self.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.5.output.dense.bias', 'base_model.encoder.layer.0.intermediate.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.output.LayerNorm.bias', 'base_model.encoder.layer.4.attention.self.key.bias', 'base_model.encoder.layer.8.attention.self.value.bias', 'base_model.encoder.layer.11.attention.self.value.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.1.attention.self.value.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.1.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.5.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.11.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.intermediate.dense.bias', 'base_model.encoder.layer.0.attention.self.value.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.intermediate.dense.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.11.attention.output.dense.bias', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.5.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.embeddings.position_embeddings.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.8.attention.self.value.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.bias', 'base_model.embeddings.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.0.attention.self.key.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.9.intermediate.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.1.attention.output.dense.bias', 'base_model.encoder.layer.11.attention.output.dense.weight', 'base_model.encoder.layer.4.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.encoder.layer.5.attention.self.value.bias', 'base_model.encoder.layer.0.output.dense.weight', 'base_model.encoder.layer.4.attention.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.encoder.layer.0.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.9.attention.output.dense.bias', 'base_model.encoder.layer.3.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.output.dense.bias', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.9.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.2.attention.self.value.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.key.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.attention.self.value.weight', 'base_model.encoder.layer.7.attention.self.value.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.query.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.6.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.output.dense.weight', 'base_model.encoder.layer.11.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.2.attention.self.query.weight', 'base_model.encoder.layer.8.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.10.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.self.value.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.intermediate.dense.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.6.attention.self.value.weight', 'base_model.encoder.layer.11.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.10.intermediate.dense.bias', 'base_model.encoder.layer.10.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.3.intermediate.dense.weight', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.7.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.0.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.0.attention.self.query.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.attention.self.key.weight', 'base_model.encoder.layer.1.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.3.attention.output.dense.bias', 'base_model.encoder.layer.6.output.dense.weight', 'base_model.encoder.layer.3.attention.self.key.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.10.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.LayerNorm.weight', 'qa_outputs.bias', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.0.output.dense.bias', 'base_model.encoder.layer.8.intermediate.dense.bias', 'base_model.encoder.layer.10.attention.output.dense.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.3.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.4.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.0.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.6.intermediate.dense.bias', 'base_model.encoder.layer.7.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.5.output.LayerNorm.weight', 'base_model.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.encoder.layer.8.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.1.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.5.attention.self.query.weight', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.attention.self.key.weight', 'base_model.encoder.layer.10.attention.self.key.weight', 'base_model.encoder.layer.9.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.3.attention.self.query.weight', 'base_model.encoder.layer.8.output.LayerNorm.weight', 'base_model.encoder.layer.11.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.dense.bias', 'base_model.encoder.layer.10.intermediate.dense.weight', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.6.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_down.0.weight', 'base_model.embeddings.word_embeddings.weight', 'base_model.encoder.layer.5.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.3.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.2.output.dense.weight', 'base_model.encoder.layer.6.attention.output.dense.weight', 'base_model.encoder.layer.1.output.LayerNorm.bias', 'base_model.encoder.layer.6.attention.self.key.bias', 'base_model.encoder.layer.3.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.intermediate.dense.weight', 'base_model.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.dense.bias', 'base_model.encoder.layer.4.output.LayerNorm.weight', 'base_model.encoder.layer.2.output.dense.bias', 'base_model.encoder.layer.6.attention.output.dense.bias', 'base_model.encoder.layer.2.attention.self.query.bias', 'base_model.encoder.layer.5.attention.self.key.weight', 'base_model.embeddings.LayerNorm.bias', 'base_model.encoder.layer.0.output.adapters.ep.adapter_down.0.weight', 'base_model.encoder.layer.4.output.LayerNorm.bias', 'base_model.encoder.layer.7.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.dense.weight', 'base_model.encoder.layer.2.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.11.output.adapters.ep.adapter_up.bias', 'base_model.encoder.layer.10.attention.self.key.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.2.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.2.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.11.output.dense.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.5.output.adapters.tp.adapter_up.bias', 'base_model.encoder.layer.2.attention.output.dense.weight', 'base_model.encoder.layer.10.attention.self.query.weight', 'base_model.encoder.layer.0.output.LayerNorm.bias', 'base_model.encoder.layer.11.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.dense.bias', 'base_model.encoder.layer.5.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.10.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.8.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.2.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.9.output.adapters.ep.adapter_up.weight', 'base_model.encoder.layer.8.attention.self.query.weight', 'base_model.encoder.layer.8.attention.self.key.bias', 'base_model.encoder.layer.10.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.value.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.dense.weight', 'base_model.encoder.layer.7.output.adapters.es.adapter_up.weight', 'base_model.encoder.layer.0.output.adapters.ts.adapter_down.0.weight', 'qa_outputs.weight', 'base_model.encoder.layer.1.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.11.output.LayerNorm.weight', 'base_model.encoder.layer.3.output.dense.weight', 'base_model.encoder.layer.1.attention.self.key.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.7.attention.output.dense.bias', 'base_model.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.3.output.dense.bias', 'base_model.encoder.layer.10.attention.self.value.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.2.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.intermediate.dense.bias', 'base_model.encoder.layer.8.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.2.output.adapters.ep.adapter_down.0.bias', 'base_model.encoder.layer.10.attention.output.dense.weight', 'base_model.encoder.layer.3.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.4.attention.self.query.bias', 'base_model.encoder.layer.4.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.3.attention.self.key.weight', 'base_model.encoder.layer.4.output.adapters.es.adapter_down.0.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_up.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.3.output.LayerNorm.bias', 'base_model.encoder.layer.10.output.adapters.es.adapter_up.bias', 'base_model.encoder.layer.3.output.LayerNorm.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.bias', 'base_model.encoder.layer.7.output.adapter_fusion_layer.ep,tp,es,ts.value.weight', 'base_model.encoder.layer.1.output.adapters.tp.adapter_down.0.weight', 'base_model.encoder.layer.9.output.LayerNorm.bias', 'base_model.encoder.layer.6.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.LayerNorm.weight', 'base_model.encoder.layer.5.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.8.attention.self.query.bias', 'base_model.encoder.layer.11.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.encoder.layer.6.attention.self.value.bias', 'base_model.encoder.layer.9.output.adapter_fusion_layer.ep,tp,es,ts.key.weight', 'base_model.encoder.layer.0.attention.output.dense.bias', 'base_model.encoder.layer.9.attention.self.key.bias', 'base_model.encoder.layer.0.attention.self.value.bias', 'base_model.encoder.layer.5.attention.self.value.weight', 'base_model.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.encoder.layer.8.output.dense.bias', 'base_model.encoder.layer.2.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.0.output.adapter_fusion_layer.ep,tp,es,ts.key.bias', 'base_model.encoder.layer.6.output.LayerNorm.bias', 'base_model.encoder.layer.9.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.6.output.adapter_fusion_layer.ep,tp,es,ts.query.bias', 'base_model.encoder.layer.2.output.LayerNorm.bias', 'base_model.encoder.layer.10.attention.self.query.bias', 'base_model.encoder.layer.9.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.6.intermediate.dense.weight', 'base_model.encoder.layer.10.attention.self.value.weight', 'base_model.encoder.layer.4.output.adapters.ts.adapter_down.0.weight', 'base_model.encoder.layer.7.output.adapters.tp.adapter_up.weight', 'base_model.encoder.layer.11.output.adapters.ts.adapter_up.weight', 'base_model.encoder.layer.8.output.adapters.es.adapter_down.0.weight', 'base_model.encoder.layer.4.intermediate.dense.weight', 'base_model.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.encoder.layer.1.output.adapters.ts.adapter_down.0.bias', 'base_model.encoder.layer.1.output.adapter_fusion_layer.ep,tp,es,ts.query.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaModel were not initialized from the model checkpoint at /cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4 and are newly initialized: ['encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
05/08/2022 20:42:47 - INFO - __main__ -   Creating features from dataset file at .
/cluster/home/yifhou/scratch_yifan/qa/xtreme_xlm/outputs-adapter//squad/xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4
XLMConfig {
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "asm": false,
  "attention_dropout": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "bos_index": 0,
  "bos_token_id": 0,
  "causal": false,
  "dropout": 0.1,
  "emb_dim": 768,
  "embed_init_std": 0.02209708691207961,
  "end_n_top": 5,
  "eos_index": 1,
  "eos_token_id": 2,
  "gelu_activation": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_encoder": true,
  "lang_id": 0,
  "layer_norm_eps": 1e-05,
  "mask_index": 5,
  "mask_token_id": 0,
  "max_position_embeddings": 514,
  "model_type": "xlm",
  "n_heads": 12,
  "n_langs": 1,
  "n_layers": 12,
  "output_past": true,
  "pad_index": 2,
  "pad_token_id": 1,
  "sinusoidal_embeddings": false,
  "start_n_top": 5,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "unk_index": 3,
  "use_lang_emb": false,
  "vocab_size": 250002
}

  0%|          | 0/48 [00:00<?, ?it/s] 25%|       | 12/48 [00:00<00:00, 112.29it/s] 50%|     | 24/48 [00:00<00:00, 72.09it/s]  69%|   | 33/48 [00:00<00:00, 77.29it/s] 98%|| 47/48 [00:00<00:00, 96.73it/s]100%|| 48/48 [00:00<00:00, 90.80it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
convert squad examples to features:   0%|          | 1/1190 [00:00<04:56,  4.00it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:07, 153.95it/s]convert squad examples to features:   8%|         | 97/1190 [00:00<00:06, 179.59it/s]convert squad examples to features:  11%|         | 129/1190 [00:00<00:05, 211.27it/s]convert squad examples to features:  14%|        | 161/1190 [00:00<00:04, 239.41it/s]convert squad examples to features:  19%|        | 225/1190 [00:01<00:03, 262.01it/s]convert squad examples to features:  22%|       | 257/1190 [00:01<00:03, 248.05it/s]convert squad examples to features:  24%|       | 289/1190 [00:01<00:03, 243.97it/s]convert squad examples to features:  27%|       | 321/1190 [00:01<00:03, 230.90it/s]convert squad examples to features:  30%|       | 353/1190 [00:01<00:03, 224.18it/s]convert squad examples to features:  32%|      | 385/1190 [00:02<00:06, 128.75it/s]convert squad examples to features:  35%|      | 417/1190 [00:02<00:05, 143.62it/s]convert squad examples to features:  38%|      | 449/1190 [00:02<00:05, 144.38it/s]convert squad examples to features:  40%|      | 481/1190 [00:02<00:04, 160.67it/s]convert squad examples to features:  43%|     | 513/1190 [00:03<00:05, 128.94it/s]convert squad examples to features:  48%|     | 577/1190 [00:03<00:03, 179.24it/s]convert squad examples to features:  51%|     | 609/1190 [00:03<00:03, 175.42it/s]convert squad examples to features:  54%|    | 641/1190 [00:03<00:03, 176.75it/s]convert squad examples to features:  57%|    | 673/1190 [00:03<00:03, 163.50it/s]convert squad examples to features:  59%|    | 705/1190 [00:03<00:02, 176.10it/s]convert squad examples to features:  62%|   | 737/1190 [00:04<00:02, 190.63it/s]convert squad examples to features:  65%|   | 769/1190 [00:04<00:02, 208.91it/s]convert squad examples to features:  67%|   | 801/1190 [00:04<00:01, 221.50it/s]convert squad examples to features:  70%|   | 833/1190 [00:04<00:01, 204.36it/s]convert squad examples to features:  73%|  | 865/1190 [00:04<00:01, 208.73it/s]convert squad examples to features:  75%|  | 897/1190 [00:04<00:01, 184.22it/s]convert squad examples to features:  78%|  | 929/1190 [00:05<00:01, 185.76it/s]convert squad examples to features:  81%|  | 961/1190 [00:05<00:01, 200.88it/s]convert squad examples to features:  83%| | 993/1190 [00:05<00:00, 214.06it/s]convert squad examples to features:  86%| | 1025/1190 [00:05<00:00, 229.47it/s]convert squad examples to features:  89%| | 1057/1190 [00:05<00:00, 236.38it/s]convert squad examples to features:  92%|| 1089/1190 [00:05<00:00, 242.79it/s]convert squad examples to features:  94%|| 1121/1190 [00:05<00:00, 230.78it/s]convert squad examples to features:  97%|| 1153/1190 [00:06<00:00, 208.13it/s]convert squad examples to features: 100%|| 1190/1190 [00:06<00:00, 196.22it/s]
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 510245.53it/s]
05/08/2022 20:42:54 - INFO - __main__ -   Saving features into cached file ./cached_xquad.hi.json_xlm-KG_LR3e-5_EPOCH2.0_maxlen384_batchsize4_gradacc4_384_hi
05/08/2022 20:42:55 - INFO - __main__ -   ***** Running evaluation  *****
05/08/2022 20:42:55 - INFO - __main__ -     Num examples = 1382
05/08/2022 20:42:55 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/173 [00:00<?, ?it/s]Evaluating:   1%|          | 1/173 [00:00<01:55,  1.49it/s]Evaluating:   1%|          | 2/173 [00:00<01:00,  2.81it/s]Evaluating:   2%|         | 3/173 [00:00<00:42,  4.04it/s]Evaluating:   2%|         | 4/173 [00:01<00:33,  5.06it/s]Evaluating:   3%|         | 5/173 [00:01<00:28,  5.89it/s]Evaluating:   3%|         | 6/173 [00:01<00:25,  6.57it/s]Evaluating:   4%|         | 7/173 [00:01<00:23,  7.02it/s]Evaluating:   5%|         | 8/173 [00:01<00:22,  7.44it/s]Evaluating:   5%|         | 9/173 [00:01<00:21,  7.75it/s]Evaluating:   6%|         | 10/173 [00:01<00:21,  7.49it/s]Evaluating:   6%|         | 11/173 [00:01<00:21,  7.69it/s]Evaluating:   7%|         | 12/173 [00:02<00:20,  7.86it/s]Evaluating:   8%|         | 13/173 [00:02<00:20,  7.98it/s]Evaluating:   8%|         | 14/173 [00:02<00:19,  8.08it/s]Evaluating:   9%|         | 15/173 [00:02<00:19,  8.14it/s]Evaluating:   9%|         | 16/173 [00:02<00:19,  8.19it/s]Evaluating:  10%|         | 17/173 [00:02<00:19,  8.20it/s]Evaluating:  10%|         | 18/173 [00:02<00:18,  8.25it/s]Evaluating:  11%|         | 19/173 [00:02<00:18,  8.29it/s]Evaluating:  12%|        | 20/173 [00:02<00:18,  8.32it/s]Evaluating:  12%|        | 21/173 [00:03<00:18,  8.34it/s]Evaluating:  13%|        | 22/173 [00:03<00:18,  8.34it/s]Evaluating:  13%|        | 23/173 [00:03<00:17,  8.40it/s]Evaluating:  14%|        | 24/173 [00:03<00:17,  8.43it/s]Evaluating:  14%|        | 25/173 [00:03<00:17,  8.45it/s]Evaluating:  15%|        | 26/173 [00:03<00:17,  8.46it/s]Evaluating:  16%|        | 27/173 [00:03<00:17,  8.48it/s]Evaluating:  16%|        | 28/173 [00:03<00:17,  8.43it/s]Evaluating:  17%|        | 29/173 [00:04<00:17,  8.43it/s]Evaluating:  17%|        | 30/173 [00:04<00:16,  8.43it/s]Evaluating:  18%|        | 31/173 [00:04<00:16,  8.39it/s]Evaluating:  18%|        | 32/173 [00:04<00:16,  8.41it/s]Evaluating:  19%|        | 33/173 [00:04<00:16,  8.31it/s]Evaluating:  20%|        | 34/173 [00:04<00:16,  8.31it/s]Evaluating:  20%|        | 35/173 [00:04<00:16,  8.28it/s]Evaluating:  21%|        | 36/173 [00:04<00:16,  8.33it/s]Evaluating:  21%|       | 37/173 [00:05<00:16,  8.26it/s]Evaluating:  22%|       | 38/173 [00:05<00:16,  8.23it/s]Evaluating:  23%|       | 39/173 [00:05<00:16,  8.25it/s]Evaluating:  23%|       | 40/173 [00:05<00:16,  8.31it/s]Evaluating:  24%|       | 41/173 [00:05<00:15,  8.33it/s]Evaluating:  24%|       | 42/173 [00:05<00:15,  8.32it/s]Evaluating:  25%|       | 43/173 [00:05<00:15,  8.37it/s]Evaluating:  25%|       | 44/173 [00:05<00:15,  8.36it/s]Evaluating:  26%|       | 45/173 [00:05<00:15,  8.32it/s]Evaluating:  27%|       | 46/173 [00:06<00:15,  8.38it/s]Evaluating:  27%|       | 47/173 [00:06<00:15,  8.37it/s]Evaluating:  28%|       | 48/173 [00:06<00:14,  8.37it/s]Evaluating:  28%|       | 49/173 [00:06<00:14,  8.35it/s]Evaluating:  29%|       | 50/173 [00:06<00:14,  8.39it/s]Evaluating:  29%|       | 51/173 [00:06<00:14,  8.38it/s]Evaluating:  30%|       | 52/173 [00:06<00:14,  8.38it/s]Evaluating:  31%|       | 53/173 [00:06<00:14,  8.41it/s]Evaluating:  31%|       | 54/173 [00:07<00:14,  8.40it/s]Evaluating:  32%|      | 55/173 [00:07<00:14,  8.41it/s]Evaluating:  32%|      | 56/173 [00:07<00:13,  8.42it/s]Evaluating:  33%|      | 57/173 [00:07<00:13,  8.42it/s]Evaluating:  34%|      | 58/173 [00:07<00:13,  8.42it/s]Evaluating:  34%|      | 59/173 [00:07<00:13,  8.39it/s]Evaluating:  35%|      | 60/173 [00:07<00:13,  8.32it/s]Evaluating:  35%|      | 61/173 [00:07<00:13,  8.33it/s]Evaluating:  36%|      | 62/173 [00:08<00:13,  8.28it/s]Evaluating:  36%|      | 63/173 [00:08<00:13,  8.29it/s]Evaluating:  37%|      | 64/173 [00:08<00:13,  8.27it/s]Evaluating:  38%|      | 65/173 [00:08<00:12,  8.32it/s]Evaluating:  38%|      | 66/173 [00:08<00:12,  8.35it/s]Evaluating:  39%|      | 67/173 [00:08<00:12,  8.31it/s]Evaluating:  39%|      | 68/173 [00:08<00:12,  8.36it/s]Evaluating:  40%|      | 69/173 [00:08<00:12,  8.39it/s]Evaluating:  40%|      | 70/173 [00:08<00:12,  8.43it/s]Evaluating:  41%|      | 71/173 [00:09<00:12,  8.41it/s]Evaluating:  42%|     | 72/173 [00:09<00:11,  8.42it/s]Evaluating:  42%|     | 73/173 [00:09<00:11,  8.45it/s]Evaluating:  43%|     | 74/173 [00:09<00:11,  8.45it/s]Evaluating:  43%|     | 75/173 [00:09<00:11,  8.46it/s]Evaluating:  44%|     | 76/173 [00:09<00:11,  8.41it/s]Evaluating:  45%|     | 77/173 [00:09<00:11,  8.44it/s]Evaluating:  45%|     | 78/173 [00:09<00:11,  8.40it/s]Evaluating:  46%|     | 79/173 [00:10<00:11,  8.44it/s]Evaluating:  46%|     | 80/173 [00:10<00:11,  8.45it/s]Evaluating:  47%|     | 81/173 [00:10<00:10,  8.44it/s]Evaluating:  47%|     | 82/173 [00:10<00:10,  8.43it/s]Evaluating:  48%|     | 83/173 [00:10<00:10,  8.43it/s]Evaluating:  49%|     | 84/173 [00:10<00:10,  8.40it/s]Evaluating:  49%|     | 85/173 [00:10<00:10,  8.39it/s]Evaluating:  50%|     | 86/173 [00:10<00:10,  8.39it/s]Evaluating:  50%|     | 87/173 [00:10<00:10,  8.34it/s]Evaluating:  51%|     | 88/173 [00:11<00:10,  8.36it/s]Evaluating:  51%|    | 89/173 [00:11<00:09,  8.40it/s]Evaluating:  52%|    | 90/173 [00:11<00:09,  8.43it/s]Evaluating:  53%|    | 91/173 [00:11<00:09,  8.43it/s]Evaluating:  53%|    | 92/173 [00:11<00:09,  8.44it/s]Evaluating:  54%|    | 93/173 [00:11<00:09,  8.46it/s]Evaluating:  54%|    | 94/173 [00:11<00:09,  8.47it/s]Evaluating:  55%|    | 95/173 [00:11<00:09,  8.38it/s]Evaluating:  55%|    | 96/173 [00:12<00:09,  8.36it/s]Evaluating:  56%|    | 97/173 [00:12<00:09,  8.33it/s]Evaluating:  57%|    | 98/173 [00:12<00:09,  8.31it/s]Evaluating:  57%|    | 99/173 [00:12<00:08,  8.25it/s]Evaluating:  58%|    | 100/173 [00:12<00:08,  8.30it/s]Evaluating:  58%|    | 101/173 [00:12<00:08,  8.34it/s]Evaluating:  59%|    | 102/173 [00:12<00:08,  8.31it/s]Evaluating:  60%|    | 103/173 [00:12<00:08,  8.25it/s]Evaluating:  60%|    | 104/173 [00:13<00:08,  8.21it/s]Evaluating:  61%|    | 105/173 [00:13<00:08,  8.22it/s]Evaluating:  61%|   | 106/173 [00:13<00:08,  8.28it/s]Evaluating:  62%|   | 107/173 [00:13<00:07,  8.33it/s]Evaluating:  62%|   | 108/173 [00:13<00:07,  8.36it/s]Evaluating:  63%|   | 109/173 [00:13<00:07,  8.35it/s]Evaluating:  64%|   | 110/173 [00:13<00:07,  8.37it/s]Evaluating:  64%|   | 111/173 [00:13<00:07,  8.31it/s]Evaluating:  65%|   | 112/173 [00:13<00:07,  8.26it/s]Evaluating:  65%|   | 113/173 [00:14<00:07,  8.31it/s]Evaluating:  66%|   | 114/173 [00:14<00:07,  8.34it/s]Evaluating:  66%|   | 115/173 [00:14<00:06,  8.39it/s]Evaluating:  67%|   | 116/173 [00:14<00:06,  8.42it/s]Evaluating:  68%|   | 117/173 [00:14<00:06,  8.43it/s]Evaluating:  68%|   | 118/173 [00:14<00:06,  8.43it/s]Evaluating:  69%|   | 119/173 [00:14<00:06,  8.41it/s]Evaluating:  69%|   | 120/173 [00:14<00:06,  8.42it/s]Evaluating:  70%|   | 121/173 [00:15<00:06,  8.45it/s]Evaluating:  71%|   | 122/173 [00:15<00:06,  8.42it/s]Evaluating:  71%|   | 123/173 [00:15<00:05,  8.34it/s]Evaluating:  72%|  | 124/173 [00:15<00:05,  8.30it/s]Evaluating:  72%|  | 125/173 [00:15<00:05,  8.33it/s]Evaluating:  73%|  | 126/173 [00:15<00:05,  8.34it/s]Evaluating:  73%|  | 127/173 [00:15<00:05,  8.29it/s]Evaluating:  74%|  | 128/173 [00:15<00:05,  8.28it/s]Evaluating:  75%|  | 129/173 [00:16<00:05,  8.32it/s]Evaluating:  75%|  | 130/173 [00:16<00:05,  8.37it/s]Evaluating:  76%|  | 131/173 [00:16<00:05,  8.38it/s]Evaluating:  76%|  | 132/173 [00:16<00:04,  8.34it/s]Evaluating:  77%|  | 133/173 [00:16<00:04,  8.29it/s]Evaluating:  77%|  | 134/173 [00:16<00:04,  8.32it/s]Evaluating:  78%|  | 135/173 [00:16<00:04,  8.35it/s]Evaluating:  79%|  | 136/173 [00:16<00:04,  8.36it/s]Evaluating:  79%|  | 137/173 [00:16<00:04,  8.35it/s]Evaluating:  80%|  | 138/173 [00:17<00:04,  8.31it/s]Evaluating:  80%|  | 139/173 [00:17<00:04,  8.34it/s]Evaluating:  81%|  | 140/173 [00:17<00:03,  8.36it/s]Evaluating:  82%| | 141/173 [00:17<00:03,  8.39it/s]Evaluating:  82%| | 142/173 [00:17<00:03,  8.43it/s]Evaluating:  83%| | 143/173 [00:17<00:03,  8.45it/s]Evaluating:  83%| | 144/173 [00:17<00:03,  8.45it/s]Evaluating:  84%| | 145/173 [00:17<00:03,  8.45it/s]Evaluating:  84%| | 146/173 [00:18<00:03,  8.46it/s]Evaluating:  85%| | 147/173 [00:18<00:03,  8.45it/s]Evaluating:  86%| | 148/173 [00:18<00:02,  8.43it/s]Evaluating:  86%| | 149/173 [00:18<00:02,  8.41it/s]Evaluating:  87%| | 150/173 [00:18<00:02,  8.42it/s]Evaluating:  87%| | 151/173 [00:18<00:02,  8.42it/s]Evaluating:  88%| | 152/173 [00:18<00:02,  8.35it/s]Evaluating:  88%| | 153/173 [00:18<00:02,  8.34it/s]Evaluating:  89%| | 154/173 [00:19<00:02,  8.35it/s]Evaluating:  90%| | 155/173 [00:19<00:02,  8.36it/s]Evaluating:  90%| | 156/173 [00:19<00:02,  8.40it/s]Evaluating:  91%| | 157/173 [00:19<00:01,  8.41it/s]Evaluating:  91%|| 158/173 [00:19<00:01,  8.42it/s]Evaluating:  92%|| 159/173 [00:19<00:01,  8.41it/s]Evaluating:  92%|| 160/173 [00:19<00:01,  8.35it/s]Evaluating:  93%|| 161/173 [00:19<00:01,  8.31it/s]Evaluating:  94%|| 162/173 [00:19<00:01,  8.30it/s]Evaluating:  94%|| 163/173 [00:20<00:01,  8.34it/s]Evaluating:  95%|| 164/173 [00:20<00:01,  8.35it/s]Evaluating:  95%|| 165/173 [00:20<00:00,  8.38it/s]Evaluating:  96%|| 166/173 [00:20<00:00,  8.39it/s]Evaluating:  97%|| 167/173 [00:20<00:00,  8.42it/s]Evaluating:  97%|| 168/173 [00:20<00:00,  8.42it/s]Evaluating:  98%|| 169/173 [00:20<00:00,  8.39it/s]Evaluating:  98%|| 170/173 [00:20<00:00,  8.38it/s]Evaluating:  99%|| 171/173 [00:21<00:00,  8.34it/s]Evaluating:  99%|| 172/173 [00:21<00:00,  8.36it/s]Evaluating: 100%|| 173/173 [00:21<00:00,  8.14it/s]
05/08/2022 20:43:16 - INFO - __main__ -     Evaluation done in total 21.241819 secs (0.015370 sec per example)
05/08/2022 20:43:20 - INFO - __main__ -   Results: {'exact': 48.90756302521008, 'f1': 66.6182273097769, 'total': 1190, 'HasAns_exact': 48.90756302521008, 'HasAns_f1': 66.6182273097769, 'HasAns_total': 1190, 'best_exact': 48.90756302521008, 'best_exact_thresh': 0.0, 'best_f1': 66.6182273097769, 'best_f1_thresh': 0.0}
