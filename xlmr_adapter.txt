Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
====> Adapter: phrase <====
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)====> Adapter: phrase <====

====> Adapter: phrase <====
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
====> Adapter: phrase <====
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
====> Adapter: phrase <====
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
====> Adapter: phrase <====
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
====> Adapter: phrase <====
Namespace(adam_epsilon=1e-08, batch_num=96, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', neg_num=8, patience=10, tmp_dir='./tmp/xlmr_adapter', triple_epoch=10, warmup_steps=10000.0)
====> Adapter: phrase <====
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
progress (w/o context) -- adapter:  1000 / 122660  |time:  1484.4277 s |loss (u, t):  3.6804   4.3466
progress (w/o context) -- adapter:  2000 / 122660  |time:  1456.8364 s |loss (u, t):  1.4149   2.7068
progress (w/o context) -- adapter:  3000 / 122660  |time:  1454.6341 s |loss (u, t):  0.9686   2.0704
progress (w/o context) -- adapter:  4000 / 122660  |time:  1468.4108 s |loss (u, t):  0.7746   1.7602
progress (w/o context) -- adapter:  5000 / 122660  |time:  1450.5393 s |loss (u, t):  0.6635   1.6138
progress (w/o context) -- adapter:  6000 / 122660  |time:  1467.1826 s |loss (u, t):  0.5883   1.5434
progress (w/o context) -- adapter:  7000 / 122660  |time:  1444.2509 s |loss (u, t):  0.5455   1.4672
progress (w/o context) -- adapter:  8000 / 122660  |time:  1451.9398 s |loss (u, t):  0.5163   1.4335
progress (w/o context) -- adapter:  9000 / 122660  |time:  1465.9818 s |loss (u, t):  0.4897   1.3753
progress (w/o context) -- adapter:  10000 / 122660  |time:  1459.6468 s |loss (u, t):  0.4758   1.3541
progress (w/o context) -- adapter:  11000 / 122660  |time:  1452.8408 s |loss (u, t):  0.4656   1.3341
progress (w/o context) -- adapter:  12000 / 122660  |time:  1469.5588 s |loss (u, t):  0.4567   1.3185
progress (w/o context) -- adapter:  13000 / 122660  |time:  1626.7573 s |loss (u, t):  0.4448   1.3083
progress (w/o context) -- adapter:  14000 / 122660  |time:  1465.407 s |loss (u, t):  0.4347   1.2791
progress (w/o context) -- adapter:  15000 / 122660  |time:  1481.0005 s |loss (u, t):  0.4334   1.2712
progress (w/o context) -- adapter:  16000 / 122660  |time:  1464.6899 s |loss (u, t):  0.4233   1.2542
progress (w/o context) -- adapter:  17000 / 122660  |time:  1463.0282 s |loss (u, t):  0.4203   1.2402
progress (w/o context) -- adapter:  18000 / 122660  |time:  1487.4954 s |loss (u, t):  0.4151   1.2322
progress (w/o context) -- adapter:  19000 / 122660  |time:  1466.572 s |loss (u, t):  0.4135   1.2276
progress (w/o context) -- adapter:  20000 / 122660  |time:  1477.3271 s |loss (u, t):  0.4142   1.2154
progress (w/o context) -- adapter:  21000 / 122660  |time:  1461.2312 s |loss (u, t):  0.4025   1.2102
progress (w/o context) -- adapter:  22000 / 122660  |time:  1462.6109 s |loss (u, t):  0.403   1.2016
progress (w/o context) -- adapter:  23000 / 122660  |time:  1478.9652 s |loss (u, t):  0.3982   1.1976
progress (w/o context) -- adapter:  24000 / 122660  |time:  1462.6693 s |loss (u, t):  0.3985   1.1873
progress (w/o context) -- adapter:  25000 / 122660  |time:  1613.0733 s |loss (u, t):  0.393   1.1851
progress (w/o context) -- adapter:  26000 / 122660  |time:  1475.9036 s |loss (u, t):  0.3887   1.1777
progress (w/o context) -- adapter:  27000 / 122660  |time:  1461.8611 s |loss (u, t):  0.3918   1.1776
progress (w/o context) -- adapter:  28000 / 122660  |time:  1460.862 s |loss (u, t):  0.3863   1.1685
progress (w/o context) -- adapter:  29000 / 122660  |time:  1469.7038 s |loss (u, t):  0.3841   1.1561
progress (w/o context) -- adapter:  30000 / 122660  |time:  1465.7405 s |loss (u, t):  0.3831   1.1609
progress (w/o context) -- adapter:  31000 / 122660  |time:  1467.5282 s |loss (u, t):  0.381   1.1548
progress (w/o context) -- adapter:  32000 / 122660  |time:  1456.6695 s |loss (u, t):  0.3848   1.1495
progress (w/o context) -- adapter:  33000 / 122660  |time:  1457.1017 s |loss (u, t):  0.3788   1.1502
progress (w/o context) -- adapter:  34000 / 122660  |time:  1470.1587 s |loss (u, t):  0.3767   1.1477
progress (w/o context) -- adapter:  35000 / 122660  |time:  1460.6878 s |loss (u, t):  0.3772   1.1414
progress (w/o context) -- adapter:  36000 / 122660  |time:  1474.8908 s |loss (u, t):  0.3784   1.1369
progress (w/o context) -- adapter:  37000 / 122660  |time:  1615.936 s |loss (u, t):  0.3724   1.1317
progress (w/o context) -- adapter:  38000 / 122660  |time:  1461.1585 s |loss (u, t):  0.3714   1.1353
progress (w/o context) -- adapter:  39000 / 122660  |time:  1463.179 s |loss (u, t):  0.3701   1.1238
progress (w/o context) -- adapter:  40000 / 122660  |time:  1479.229 s |loss (u, t):  0.3668   1.1267
progress (w/o context) -- adapter:  41000 / 122660  |time:  1463.2045 s |loss (u, t):  0.3675   1.1211
progress (w/o context) -- adapter:  42000 / 122660  |time:  1465.4779 s |loss (u, t):  0.3673   1.1189
progress (w/o context) -- adapter:  43000 / 122660  |time:  1474.1609 s |loss (u, t):  0.3687   1.1157
progress (w/o context) -- adapter:  44000 / 122660  |time:  1455.9685 s |loss (u, t):  0.3691   1.1175
progress (w/o context) -- adapter:  45000 / 122660  |time:  1472.7195 s |loss (u, t):  0.3635   1.1111
progress (w/o context) -- adapter:  46000 / 122660  |time:  1461.9203 s |loss (u, t):  0.3622   1.11
progress (w/o context) -- adapter:  47000 / 122660  |time:  1466.9751 s |loss (u, t):  0.3645   1.105
progress (w/o context) -- adapter:  48000 / 122660  |time:  1475.2006 s |loss (u, t):  0.3628   1.1041
progress (w/o context) -- adapter:  49000 / 122660  |time:  1460.3833 s |loss (u, t):  0.3637   1.1026
progress (w/o context) -- adapter:  50000 / 122660  |time:  1615.6092 s |loss (u, t):  0.3571   1.1025
progress (w/o context) -- adapter:  51000 / 122660  |time:  1481.5241 s |loss (u, t):  0.3573   1.0953
progress (w/o context) -- adapter:  52000 / 122660  |time:  1460.1777 s |loss (u, t):  0.3584   1.0955
progress (w/o context) -- adapter:  53000 / 122660  |time:  1460.2487 s |loss (u, t):  0.3584   1.0923
progress (w/o context) -- adapter:  54000 / 122660  |time:  1475.1276 s |loss (u, t):  0.3581   1.0947
progress (w/o context) -- adapter:  55000 / 122660  |time:  1465.3796 s |loss (u, t):  0.3552   1.0978
progress (w/o context) -- adapter:  56000 / 122660  |time:  1468.3423 s |loss (u, t):  0.357   1.095
progress (w/o context) -- adapter:  57000 / 122660  |time:  1457.1637 s |loss (u, t):  0.3563   1.0919
progress (w/o context) -- adapter:  58000 / 122660  |time:  1456.3221 s |loss (u, t):  0.3512   1.096
progress (w/o context) -- adapter:  59000 / 122660  |time:  1474.4862 s |loss (u, t):  0.3547   1.0903
progress (w/o context) -- adapter:  60000 / 122660  |time:  1463.2755 s |loss (u, t):  0.3518   1.0838
progress (w/o context) -- adapter:  61000 / 122660  |time:  1455.4901 s |loss (u, t):  0.3518   1.0773
progress (w/o context) -- adapter:  62000 / 122660  |time:  1636.5513 s |loss (u, t):  0.3515   1.0811
progress (w/o context) -- adapter:  63000 / 122660  |time:  1467.0649 s |loss (u, t):  0.3493   1.0745
progress (w/o context) -- adapter:  64000 / 122660  |time:  1459.0692 s |loss (u, t):  0.3501   1.0771
progress (w/o context) -- adapter:  65000 / 122660  |time:  1467.8266 s |loss (u, t):  0.3477   1.0707
progress (w/o context) -- adapter:  66000 / 122660  |time:  1482.5507 s |loss (u, t):  0.349   1.0668
progress (w/o context) -- adapter:  67000 / 122660  |time:  1460.8146 s |loss (u, t):  0.3486   1.0713
progress (w/o context) -- adapter:  68000 / 122660  |time:  1455.2328 s |loss (u, t):  0.3492   1.0707
progress (w/o context) -- adapter:  69000 / 122660  |time:  1467.3368 s |loss (u, t):  0.3504   1.0633
progress (w/o context) -- adapter:  70000 / 122660  |time:  1467.3317 s |loss (u, t):  0.3435   1.0635
progress (w/o context) -- adapter:  71000 / 122660  |time:  1474.9101 s |loss (u, t):  0.3455   1.0621
progress (w/o context) -- adapter:  72000 / 122660  |time:  1463.8191 s |loss (u, t):  0.3463   1.0654
progress (w/o context) -- adapter:  73000 / 122660  |time:  1462.5435 s |loss (u, t):  0.3464   1.0576
progress (w/o context) -- adapter:  74000 / 122660  |time:  1633.3859 s |loss (u, t):  0.3429   1.0537
progress (w/o context) -- adapter:  75000 / 122660  |time:  1467.158 s |loss (u, t):  0.3416   1.0574
progress (w/o context) -- adapter:  76000 / 122660  |time:  1462.4451 s |loss (u, t):  0.3463   1.0551
progress (w/o context) -- adapter:  77000 / 122660  |time:  1460.6854 s |loss (u, t):  0.3412   1.0482
progress (w/o context) -- adapter:  78000 / 122660  |time:  1480.5763 s |loss (u, t):  0.3392   1.0481
progress (w/o context) -- adapter:  79000 / 122660  |time:  1467.5783 s |loss (u, t):  0.3412   1.0504
progress (w/o context) -- adapter:  80000 / 122660  |time:  1460.3119 s |loss (u, t):  0.343   1.0486
progress (w/o context) -- adapter:  81000 / 122660  |time:  1465.1563 s |loss (u, t):  0.3433   1.0462
progress (w/o context) -- adapter:  82000 / 122660  |time:  1468.6868 s |loss (u, t):  0.3398   1.0432
progress (w/o context) -- adapter:  83000 / 122660  |time:  1468.1259 s |loss (u, t):  0.3381   1.0434
progress (w/o context) -- adapter:  84000 / 122660  |time:  1464.4061 s |loss (u, t):  0.341   1.0434
progress (w/o context) -- adapter:  85000 / 122660  |time:  1463.0804 s |loss (u, t):  0.342   1.0463
progress (w/o context) -- adapter:  86000 / 122660  |time:  1631.7462 s |loss (u, t):  0.3374   1.0398
progress (w/o context) -- adapter:  87000 / 122660  |time:  1464.9778 s |loss (u, t):  0.3345   1.036
progress (w/o context) -- adapter:  88000 / 122660  |time:  1466.8503 s |loss (u, t):  0.3354   1.0337
progress (w/o context) -- adapter:  89000 / 122660  |time:  1467.6933 s |loss (u, t):  0.3386   1.0364
progress (w/o context) -- adapter:  90000 / 122660  |time:  1474.7139 s |loss (u, t):  0.3354   1.027
progress (w/o context) -- adapter:  91000 / 122660  |time:  1468.438 s |loss (u, t):  0.3372   1.031
progress (w/o context) -- adapter:  92000 / 122660  |time:  1475.7268 s |loss (u, t):  0.3367   1.0277
progress (w/o context) -- adapter:  93000 / 122660  |time:  1456.573 s |loss (u, t):  0.3372   1.03
progress (w/o context) -- adapter:  94000 / 122660  |time:  1462.1279 s |loss (u, t):  0.3367   1.0251
progress (w/o context) -- adapter:  95000 / 122660  |time:  1483.9654 s |loss (u, t):  0.332   1.0275
progress (w/o context) -- adapter:  96000 / 122660  |time:  1468.1515 s |loss (u, t):  0.3341   1.0241
progress (w/o context) -- adapter:  97000 / 122660  |time:  1474.0638 s |loss (u, t):  0.3347   1.0264
progress (w/o context) -- adapter:  98000 / 122660  |time:  1467.4271 s |loss (u, t):  0.3336   1.02
progress (w/o context) -- adapter:  99000 / 122660  |time:  1621.0194 s |loss (u, t):  0.3308   1.0182
progress (w/o context) -- adapter:  100000 / 122660  |time:  1466.8227 s |loss (u, t):  0.3331   1.0194
progress (w/o context) -- adapter:  101000 / 122660  |time:  1480.2707 s |loss (u, t):  0.3292   1.0215
progress (w/o context) -- adapter:  102000 / 122660  |time:  1467.9342 s |loss (u, t):  0.3312   1.0148
progress (w/o context) -- adapter:  103000 / 122660  |time:  1473.389 s |loss (u, t):  0.3325   1.0175
progress (w/o context) -- adapter:  104000 / 122660  |time:  1463.6327 s |loss (u, t):  0.3313   1.0147
progress (w/o context) -- adapter:  105000 / 122660  |time:  1462.114 s |loss (u, t):  0.3314   1.0182
progress (w/o context) -- adapter:  106000 / 122660  |time:  1471.8758 s |loss (u, t):  0.333   1.0142
progress (w/o context) -- adapter:  107000 / 122660  |time:  1462.0954 s |loss (u, t):  0.3282   1.0098
progress (w/o context) -- adapter:  108000 / 122660  |time:  1467.9553 s |loss (u, t):  0.3317   1.0138
progress (w/o context) -- adapter:  109000 / 122660  |time:  1474.9981 s |loss (u, t):  0.3289   1.0104
progress (w/o context) -- adapter:  110000 / 122660  |time:  1465.004 s |loss (u, t):  0.3305   1.005
progress (w/o context) -- adapter:  111000 / 122660  |time:  1633.2152 s |loss (u, t):  0.3276   1.0049
progress (w/o context) -- adapter:  112000 / 122660  |time:  1468.8231 s |loss (u, t):  0.326   1.0015
progress (w/o context) -- adapter:  113000 / 122660  |time:  1465.4934 s |loss (u, t):  0.3285   1.0132
progress (w/o context) -- adapter:  114000 / 122660  |time:  1472.369 s |loss (u, t):  0.3272   1.0029
progress (w/o context) -- adapter:  115000 / 122660  |time:  1467.4338 s |loss (u, t):  0.3266   0.9966
progress (w/o context) -- adapter:  116000 / 122660  |time:  1480.2543 s |loss (u, t):  0.3282   0.9983
progress (w/o context) -- adapter:  117000 / 122660  |time:  1454.849 s |loss (u, t):  0.328   0.9998
progress (w/o context) -- adapter:  118000 / 122660  |time:  1462.9397 s |loss (u, t):  0.3303   0.9959
progress (w/o context) -- adapter:  119000 / 122660  |time:  1475.9047 s |loss (u, t):  0.3242   0.9967
progress (w/o context) -- adapter:  120000 / 122660  |time:  1461.2018 s |loss (u, t):  0.3235   0.9968
progress (w/o context) -- adapter:  121000 / 122660  |time:  1471.8756 s |loss (u, t):  0.3255   0.9996
progress (w/o context) -- adapter:  122000 / 122660  |time:  1475.8103 s |loss (u, t):  0.327   0.9955
====> Adapter: sentence <====
====> Adapter: sentence <====
====> Adapter: sentence <====
====> Adapter: sentence <====
====> Adapter: sentence <====
====> Adapter: sentence <====
====> Adapter: sentence <====
====> Adapter: sentence <====
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'tp'.
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ts'.
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
progress (w context) -- adapter:  1000 / 50450  |time:  1038.5631 s |loss (u, t):  2.3569   2.5808
progress (w context) -- adapter:  2000 / 50450  |time:  1039.3165 s |loss (u, t):  0.6764   1.3678
progress (w context) -- adapter:  3000 / 50450  |time:  1038.4547 s |loss (u, t):  0.4269   0.9909
progress (w context) -- adapter:  4000 / 50450  |time:  1034.7455 s |loss (u, t):  0.3072   0.7935
progress (w context) -- adapter:  5000 / 50450  |time:  1038.431 s |loss (u, t):  0.2528   0.7413
progress (w context) -- adapter:  6000 / 50450  |time:  1233.7993 s |loss (u, t):  0.2162   0.6566
progress (w context) -- adapter:  7000 / 50450  |time:  1039.3523 s |loss (u, t):  0.1902   0.5903
progress (w context) -- adapter:  8000 / 50450  |time:  1039.5057 s |loss (u, t):  0.1785   0.5533
progress (w context) -- adapter:  9000 / 50450  |time:  1035.4971 s |loss (u, t):  0.1656   0.5253
progress (w context) -- adapter:  10000 / 50450  |time:  1037.7543 s |loss (u, t):  0.1601   0.5093
progress (w context) -- adapter:  11000 / 50450  |time:  1229.0073 s |loss (u, t):  0.1488   0.4859
progress (w context) -- adapter:  12000 / 50450  |time:  1045.1424 s |loss (u, t):  0.1372   0.4539
progress (w context) -- adapter:  13000 / 50450  |time:  1042.3278 s |loss (u, t):  0.1353   0.4437
progress (w context) -- adapter:  14000 / 50450  |time:  1039.9232 s |loss (u, t):  0.1314   0.4364
progress (w context) -- adapter:  15000 / 50450  |time:  1041.7974 s |loss (u, t):  0.1284   0.4178
progress (w context) -- adapter:  16000 / 50450  |time:  1235.8912 s |loss (u, t):  0.1228   0.4117
progress (w context) -- adapter:  17000 / 50450  |time:  1041.843 s |loss (u, t):  0.1209   0.3876
progress (w context) -- adapter:  18000 / 50450  |time:  1043.0597 s |loss (u, t):  0.1189   0.3817
progress (w context) -- adapter:  19000 / 50450  |time:  1037.6794 s |loss (u, t):  0.1161   0.3806
progress (w context) -- adapter:  20000 / 50450  |time:  1041.2605 s |loss (u, t):  0.1111   0.3747
progress (w context) -- adapter:  21000 / 50450  |time:  1235.5435 s |loss (u, t):  0.1113   0.3662
progress (w context) -- adapter:  22000 / 50450  |time:  1045.5592 s |loss (u, t):  0.1197   0.3452
progress (w context) -- adapter:  23000 / 50450  |time:  1043.4805 s |loss (u, t):  0.1095   0.3487
progress (w context) -- adapter:  24000 / 50450  |time:  1041.4146 s |loss (u, t):  0.109   0.3416
progress (w context) -- adapter:  25000 / 50450  |time:  1044.3291 s |loss (u, t):  0.1055   0.3403
progress (w context) -- adapter:  26000 / 50450  |time:  1240.3083 s |loss (u, t):  0.0997   0.3316
progress (w context) -- adapter:  27000 / 50450  |time:  1046.8295 s |loss (u, t):  0.0994   0.3142
progress (w context) -- adapter:  28000 / 50450  |time:  1045.6841 s |loss (u, t):  0.1047   0.3187
progress (w context) -- adapter:  29000 / 50450  |time:  1041.0571 s |loss (u, t):  0.1022   0.3179
progress (w context) -- adapter:  30000 / 50450  |time:  1044.1107 s |loss (u, t):  0.0962   0.3158
progress (w context) -- adapter:  31000 / 50450  |time:  1236.6084 s |loss (u, t):  0.099   0.3131
progress (w context) -- adapter:  32000 / 50450  |time:  1047.4634 s |loss (u, t):  0.0898   0.2976
progress (w context) -- adapter:  33000 / 50450  |time:  1045.5192 s |loss (u, t):  0.0935   0.2959
progress (w context) -- adapter:  34000 / 50450  |time:  1042.2069 s |loss (u, t):  0.0951   0.2977
progress (w context) -- adapter:  35000 / 50450  |time:  1044.0859 s |loss (u, t):  0.0936   0.2903
progress (w context) -- adapter:  36000 / 50450  |time:  1248.4721 s |loss (u, t):  0.0914   0.2883
progress (w context) -- adapter:  37000 / 50450  |time:  1049.3731 s |loss (u, t):  0.0859   0.2773
progress (w context) -- adapter:  38000 / 50450  |time:  1048.4252 s |loss (u, t):  0.0896   0.2748
progress (w context) -- adapter:  39000 / 50450  |time:  1045.6939 s |loss (u, t):  0.0868   0.2783
progress (w context) -- adapter:  40000 / 50450  |time:  1045.7708 s |loss (u, t):  0.0854   0.2743
progress (w context) -- adapter:  41000 / 50450  |time:  1239.146 s |loss (u, t):  0.0855   0.2702
progress (w context) -- adapter:  42000 / 50450  |time:  1047.658 s |loss (u, t):  0.0802   0.2611
progress (w context) -- adapter:  43000 / 50450  |time:  1046.3405 s |loss (u, t):  0.0853   0.2537
progress (w context) -- adapter:  44000 / 50450  |time:  1043.6555 s |loss (u, t):  0.0873   0.258
progress (w context) -- adapter:  45000 / 50450  |time:  1046.0179 s |loss (u, t):  0.0845   0.2526
progress (w context) -- adapter:  46000 / 50450  |time:  1239.2064 s |loss (u, t):  0.0825   0.2572
progress (w context) -- adapter:  47000 / 50450  |time:  1050.249 s |loss (u, t):  0.0788   0.246
progress (w context) -- adapter:  48000 / 50450  |time:  1046.5414 s |loss (u, t):  0.0812   0.2391
progress (w context) -- adapter:  49000 / 50450  |time:  1044.6444 s |loss (u, t):  0.0803   0.2461
progress (w context) -- adapter:  50000 / 50450  |time:  1047.4099 s |loss (u, t):  0.0775   0.2391

