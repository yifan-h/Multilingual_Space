Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=128, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_pad_token_id=0, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', neg_num=8, patience=10, tmp_dir='./tmp/xlm_80_final', triple_epoch=20, warmup_steps=10000.0)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
progress (w/o context):  1000 / 183980  |time:  660.3875 s |loss (u, t):  4.4971   4.3767
progress (w/o context):  1000 / 183980  |time:  660.4155 s |loss (u, t):  4.4969   4.3738
progress (w/o context):  1000 / 183980  |time:  660.4476 s |loss (u, t):  4.4949   4.3719
progress (w/o context):  1000 / 183980  |time:  660.4488 s |loss (u, t):  4.4985   4.3699
progress (w/o context):  1000 / 183980  |time:  660.4657 s |loss (u, t):  4.4957   4.3736
progress (w/o context):  1000 / 183980  |time:  660.4673 s |loss (u, t):  4.4961   4.3743
progress (w/o context):  1000 / 183980  |time:  660.473 s |loss (u, t):  4.497   4.3736
progress (w/o context):  1000 / 183980  |time:  660.6254 s |loss (u, t):  4.4952   4.3726
progress (w/o context):  2000 / 183980  |time:  661.6861 s |loss (u, t):  3.1212   3.6884
progress (w/o context):  2000 / 183980  |time:  661.643 s |loss (u, t):  3.1269   3.6904
progress (w/o context):  2000 / 183980  |time:  661.6615 s |loss (u, t):  3.1236   3.6856
progress (w/o context):  2000 / 183980  |time:  661.6396 s |loss (u, t):  3.1235   3.6882
progress (w/o context):  2000 / 183980  |time:  661.7408 s |loss (u, t):  3.1306   3.6934
progress (w/o context):  2000 / 183980  |time:  661.7292 s |loss (u, t):  3.1186   3.6861
progress (w/o context):  2000 / 183980  |time:  661.7139 s |loss (u, t):  3.1249   3.6908
progress (w/o context):  2000 / 183980  |time:  661.5633 s |loss (u, t):  3.1249   3.6901
progress (w/o context):  3000 / 183980  |time:  665.5795 s |loss (u, t):  2.335   3.17
progress (w/o context):  3000 / 183980  |time:  665.517 s |loss (u, t):  2.3304   3.1597
progress (w/o context):  3000 / 183980  |time:  665.59 s |loss (u, t):  2.3341   3.1735
progress (w/o context):  3000 / 183980  |time:  665.5482 s |loss (u, t):  2.3269   3.1727
progress (w/o context):  3000 / 183980  |time:  665.6158 s |loss (u, t):  2.3392   3.1797
progress (w/o context):  3000 / 183980  |time:  665.6331 s |loss (u, t):  2.333   3.1745
progress (w/o context):  3000 / 183980  |time:  665.7248 s |loss (u, t):  2.3339   3.171
progress (w/o context):  3000 / 183980  |time:  665.6417 s |loss (u, t):  2.3335   3.1602
progress (w/o context):  4000 / 183980  |time:  666.4433 s |loss (u, t):  1.8528   2.725
progress (w/o context):  4000 / 183980  |time:  666.5198 s |loss (u, t):  1.8514   2.7258
progress (w/o context):  4000 / 183980  |time:  666.524 s |loss (u, t):  1.8564   2.7221
progress (w/o context):  4000 / 183980  |time:  666.3893 s |loss (u, t):  1.8544   2.7189
progress (w/o context):  4000 / 183980  |time:  666.5477 s |loss (u, t):  1.8557   2.7169
progress (w/o context):  4000 / 183980  |time:  666.5141 s |loss (u, t):  1.8593   2.7209
progress (w/o context):  4000 / 183980  |time:  666.4431 s |loss (u, t):  1.8559   2.7093
progress (w/o context):  4000 / 183980  |time:  666.5243 s |loss (u, t):  1.855   2.7208
progress (w/o context):  5000 / 183980  |time:  669.7415 s |loss (u, t):  1.5589   2.3766
progress (w/o context):  5000 / 183980  |time:  669.7746 s |loss (u, t):  1.5578   2.3774
progress (w/o context):  5000 / 183980  |time:  669.6949 s |loss (u, t):  1.5622   2.3718
progress (w/o context):  5000 / 183980  |time:  669.743 s |loss (u, t):  1.5541   2.3662
progress (w/o context):  5000 / 183980  |time:  669.7609 s |loss (u, t):  1.5517   2.3701
progress (w/o context):  5000 / 183980  |time:  669.7474 s |loss (u, t):  1.5579   2.3782
progress (w/o context):  5000 / 183980  |time:  669.7989 s |loss (u, t):  1.5517   2.3865
progress (w/o context):  5000 / 183980  |time:  669.9182 s |loss (u, t):  1.5558   2.3766
progress (w/o context):  6000 / 183980  |time:  673.2999 s |loss (u, t):  1.3565   2.0961
progress (w/o context):  6000 / 183980  |time:  673.3238 s |loss (u, t):  1.3606   2.1019
progress (w/o context):  6000 / 183980  |time:  673.2423 s |loss (u, t):  1.3567   2.1103
progress (w/o context):  6000 / 183980  |time:  673.332 s |loss (u, t):  1.3592   2.101
progress (w/o context):  6000 / 183980  |time:  673.348 s |loss (u, t):  1.3578   2.0984
progress (w/o context):  6000 / 183980  |time:  673.3786 s |loss (u, t):  1.357   2.1087
progress (w/o context):  6000 / 183980  |time:  673.2027 s |loss (u, t):  1.3594   2.1119
progress (w/o context):  6000 / 183980  |time:  673.4525 s |loss (u, t):  1.3581   2.113
