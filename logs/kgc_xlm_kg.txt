Namespace(adam_epsilon=1e-06, batch_num=50, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=6, epoch=10, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base/', model_name='XLM-KG', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlm_80/final_v3.pt', neg_num=1, patience=2, tmp_dir='./tmp/checkpoints', weight_decay=0.005)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base/ were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base/ were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
----KGC: loss:  1.3912 | val hit@1:  0.0669 hit@10:  0.2034
KGC: [ el ] | test hit@1:  0.0659 hit@10:  0.3412
KGC: [ en ] | test hit@1:  0.0741 hit@10:  0.2188
KGC: [ es ] | test hit@1:  0.1173 hit@10:  0.3048
KGC: [ fr ] | test hit@1:  0.1575 hit@10:  0.3004
KGC: [ ja ] | test hit@1:  0.1647 hit@10:  0.2946
----KGC: loss:  0.8934 | val hit@1:  0.0602 hit@10:  0.224
KGC: [ el ] | test hit@1:  0.1318 hit@10:  0.3854
KGC: [ en ] | test hit@1:  0.0778 hit@10:  0.2507
KGC: [ es ] | test hit@1:  0.1187 hit@10:  0.3056
KGC: [ fr ] | test hit@1:  0.1393 hit@10:  0.3354
KGC: [ ja ] | test hit@1:  0.0948 hit@10:  0.3117
----KGC: loss:  0.786 | val hit@1:  0.0691 hit@10:  0.2565
KGC: [ el ] | test hit@1:  0.1386 hit@10:  0.3982
KGC: [ en ] | test hit@1:  0.0835 hit@10:  0.2665
KGC: [ es ] | test hit@1:  0.1252 hit@10:  0.3418
KGC: [ fr ] | test hit@1:  0.1573 hit@10:  0.3637
KGC: [ ja ] | test hit@1:  0.1735 hit@10:  0.3506
----KGC: loss:  0.8329 | val hit@1:  0.0608 hit@10:  0.2292
KGC: [ el ] | test hit@1:  0.1298 hit@10:  0.4189
KGC: [ en ] | test hit@1:  0.0781 hit@10:  0.2471
KGC: [ es ] | test hit@1:  0.1214 hit@10:  0.3222
KGC: [ fr ] | test hit@1:  0.1391 hit@10:  0.3409
KGC: [ ja ] | test hit@1:  0.0976 hit@10:  0.3298
----KGC: loss:  0.8796 | val hit@1:  0.0456 hit@10:  0.2023
KGC: [ el ] | test hit@1:  0.1249 hit@10:  0.4061
KGC: [ en ] | test hit@1:  0.0564 hit@10:  0.2351
KGC: [ es ] | test hit@1:  0.1135 hit@10:  0.3114
KGC: [ fr ] | test hit@1:  0.1007 hit@10:  0.3138
KGC: [ ja ] | test hit@1:  0.1656 hit@10:  0.3372
----KGC: loss:  0.8172 | val hit@1:  0.0562 hit@10:  0.2267
KGC: [ el ] | test hit@1:  0.1052 hit@10:  0.413
KGC: [ en ] | test hit@1:  0.0494 hit@10:  0.2382
KGC: [ es ] | test hit@1:  0.0605 hit@10:  0.3258
KGC: [ fr ] | test hit@1:  0.1309 hit@10:  0.338
KGC: [ ja ] | test hit@1:  0.1739 hit@10:  0.3404
----KGC: loss:  0.7472 | val hit@1:  0.0639 hit@10:  0.2298
KGC: [ el ] | test hit@1:  0.0983 hit@10:  0.4041
KGC: [ en ] | test hit@1:  0.0608 hit@10:  0.2395
KGC: [ es ] | test hit@1:  0.1081 hit@10:  0.3104
KGC: [ fr ] | test hit@1:  0.1479 hit@10:  0.3503
KGC: [ ja ] | test hit@1:  0.1688 hit@10:  0.3307
----KGC: loss:  0.948 | val hit@1:  0.0658 hit@10:  0.215
KGC: [ el ] | test hit@1:  0.0865 hit@10:  0.3638
KGC: [ en ] | test hit@1:  0.0792 hit@10:  0.2338
KGC: [ es ] | test hit@1:  0.1166 hit@10:  0.3399
KGC: [ fr ] | test hit@1:  0.1307 hit@10:  0.339
KGC: [ ja ] | test hit@1:  0.1684 hit@10:  0.3053
----KGC: loss:  0.8818 | val hit@1:  0.0441 hit@10:  0.2087
KGC: [ el ] | test hit@1:  0.0865 hit@10:  0.3166
KGC: [ en ] | test hit@1:  0.0462 hit@10:  0.2197
KGC: [ es ] | test hit@1:  0.1222 hit@10:  0.3079
KGC: [ fr ] | test hit@1:  0.1357 hit@10:  0.3205
KGC: [ ja ] | test hit@1:  0.1637 hit@10:  0.2969
----KGC: loss:  0.89 | val hit@1:  0.0522 hit@10:  0.1963
KGC: [ el ] | test hit@1:  0.1032 hit@10:  0.297
KGC: [ en ] | test hit@1:  0.0351 hit@10:  0.2061
KGC: [ es ] | test hit@1:  0.1106 hit@10:  0.2915
KGC: [ fr ] | test hit@1:  0.1386 hit@10:  0.317
KGC: [ ja ] | test hit@1:  0.1633 hit@10:  0.271
The performance (hit@1, hit@10) of language [ ja ] is:  [0.3255608028335301, 0.1735, 0.3506]
