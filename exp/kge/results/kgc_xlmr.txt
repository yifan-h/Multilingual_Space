Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=4, epoch=10, lr=1e-08, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', model_name='XLMR', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlmr_adapter', neg_num=1, patience=2, task_name='kgc', tmp_dir='./tmp/checkpoints', weight_decay=0.0001)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
...testing...
KGC: [ el ] | test hit@1:  0.04991 hit@10:  0.1793 MRR:  0.07478
KGC: [ en ] | test hit@1:  0.10411 hit@10:  0.24983 MRR:  0.12547
KGC: [ es ] | test hit@1:  0.09656 hit@10:  0.23702 MRR:  0.12207
KGC: [ fr ] | test hit@1:  0.09175 hit@10:  0.22858 MRR:  0.11605
KGC: [ ja ] | test hit@1:  0.02413 hit@10:  0.11133 MRR:  0.04616
KGC: [ ast ] | test hit@1:  0.10592 hit@10:  0.27241 MRR:  0.12947
KGC: [ ca ] | test hit@1:  0.11119 hit@10:  0.26732 MRR:  0.13445
KGC: [ da ] | test hit@1:  0.11496 hit@10:  0.28059 MRR:  0.14144
KGC: [ de ] | test hit@1:  0.09805 hit@10:  0.24513 MRR:  0.12559
KGC: [ fa ] | test hit@1:  0.05109 hit@10:  0.15629 MRR:  0.07253
KGC: [ fi ] | test hit@1:  0.08249 hit@10:  0.24245 MRR:  0.1111
KGC: [ hu ] | test hit@1:  0.10047 hit@10:  0.26661 MRR:  0.12507
KGC: [ it ] | test hit@1:  0.10376 hit@10:  0.24931 MRR:  0.12819
KGC: [ nb ] | test hit@1:  0.11336 hit@10:  0.28598 MRR:  0.13924
KGC: [ nl ] | test hit@1:  0.0855 hit@10:  0.25046 MRR:  0.11473
KGC: [ pl ] | test hit@1:  0.08739 hit@10:  0.23949 MRR:  0.11472
KGC: [ pt ] | test hit@1:  0.09642 hit@10:  0.24435 MRR:  0.12111
KGC: [ ru ] | test hit@1:  0.04849 hit@10:  0.17423 MRR:  0.0741
KGC: [ sv ] | test hit@1:  0.10992 hit@10:  0.27364 MRR:  0.13634
KGC: [ zh ] | test hit@1:  0.03396 hit@10:  0.11926 MRR:  0.05329

KGC: [ eo ] | test hit@1:  0.16822 hit@10:  0.47767 MRR:  0.20816
KGC: [ vo ] | test hit@1:  0.37195 hit@10:  0.7378 MRR:  0.41808