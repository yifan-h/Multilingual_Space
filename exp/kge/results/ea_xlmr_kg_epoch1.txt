Namespace(adam_epsilon=1e-06, batch_num=8, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=5, epoch=1, lr=1e-08, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', model_name='XLMR-KG', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlmr_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.0001)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
KGC: [ en_nb ] | hit@1:  0.95551 hit@5:  0.97426 mrr:  0.94373
KGC: [ en_da ] | hit@1:  0.95486 hit@5:  0.97183 mrr:  0.94418
KGC: [ en_zh ] | hit@1:  0.67278 hit@5:  0.75544 mrr:  0.66217
KGC: [ en_mg ] | hit@1:  0.96812 hit@5:  0.98364 mrr:  0.96273
KGC: [ en_fr ] | hit@1:  0.935 hit@5:  0.95201 mrr:  0.92779
KGC: [ en_es ] | hit@1:  0.94291 hit@5:  0.96666 mrr:  0.92714
KGC: [ en_it ] | hit@1:  0.92373 hit@5:  0.94889 mrr:  0.91014
KGC: [ en_pl ] | hit@1:  0.92607 hit@5:  0.95329 mrr:  0.91192
KGC: [ en_fa ] | hit@1:  0.83108 hit@5:  0.89435 mrr:  0.81815
KGC: [ en_ru ] | hit@1:  0.87854 hit@5:  0.92892 mrr:  0.85907
KGC: [ en_vo ] | hit@1:  0.97596 hit@5:  0.98578 mrr:  0.96345
KGC: [ en_sv ] | hit@1:  0.94414 hit@5:  0.96541 mrr:  0.93119
KGC: [ en_eo ] | hit@1:  0.89757 hit@5:  0.93279 mrr:  0.8852
KGC: [ en_io ] | hit@1:  0.97855 hit@5:  0.98815 mrr:  0.96921
KGC: [ en_cs ] | hit@1:  0.93923 hit@5:  0.96276 mrr:  0.92776
KGC: [ en_ast ] | hit@1:  0.97293 hit@5:  0.98457 mrr:  0.96328
KGC: [ en_pt ] | hit@1:  0.94426 hit@5:  0.96748 mrr:  0.93053
KGC: [ en_de ] | hit@1:  0.86781 hit@5:  0.8833 mrr:  0.8615
KGC: [ en_hu ] | hit@1:  0.93607 hit@5:  0.95851 mrr:  0.92658
KGC: [ en_ar ] | hit@1:  0.81847 hit@5:  0.88126 mrr:  0.80017
KGC: [ en_ja ] | hit@1:  0.77803 hit@5:  0.84568 mrr:  0.76372
KGC: [ en_fi ] | hit@1:  0.94187 hit@5:  0.96264 mrr:  0.93142
KGC: [ en_nl ] | hit@1:  0.94564 hit@5:  0.97079 mrr:  0.92825
KGC: [ en_yo ] | hit@1:  0.96221 hit@5:  0.97613 mrr:  0.96057
KGC: [ en_ca ] | hit@1:  0.94452 hit@5:  0.96699 mrr:  0.93402
