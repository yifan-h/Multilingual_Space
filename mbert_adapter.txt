Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Namespace(adam_epsilon=1e-08, batch_num=32, data_dir='/cluster/work/sachan/yifan/data/wikidata/sub_clean_rich10', device=-1, entity_epoch=1, lm_mask_token_id=-1, lr=0.0001, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', neg_num=8, patience=10, tmp_dir='./tmp/mbert_adapter', triple_epoch=10, warmup_steps=10000.0)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ep'.
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'tp'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'ts'.
Overwriting existing adapter 'es'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
ts
ts
ts
ts
ts
ts
ts
ts
es
es
es
es
es
es
es
es
ts
ts
ts
ts
tsts

ts
ts
es
es
es
es
es
es
es
es
Traceback (most recent call last):
  File "src/main.py", line 44, in <module>
    main_func(args)
  File "src/main.py", line 6, in main_func
    ki_mlkg(args)  # MLKG integration (pretraining)
  File "/cluster/scratch/yifhou/Multilingual_Space/src/tasks.py", line 718, in ki_mlkg
    train_fuse_sentence(args, model_mlkg)
  File "/cluster/scratch/yifhou/Multilingual_Space/src/tasks.py", line 638, in train_fuse_sentence
    outputs_lp, outputs = model_mlkg(**input_e)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/scratch/yifhou/Multilingual_Space/src/models.py", line 103, in forward
    outputs_MLLM = self.MLLM(**inputs).hidden_states
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1026, in forward
    encoder_outputs = self.encoder(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 602, in forward
    layer_outputs = layer_module(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 487, in forward
    self_attention_outputs = self.attention(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 413, in forward
    self_outputs = self.self(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 341, in forward
    attention_probs = self.dropout(attention_probs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/functional.py", line 983, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 5; 23.65 GiB total capacity; 21.23 GiB already allocated; 194.44 MiB free; 22.27 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "src/main.py", line 44, in <module>
    main_func(args)
  File "src/main.py", line 6, in main_func
    ki_mlkg(args)  # MLKG integration (pretraining)
  File "/cluster/scratch/yifhou/Multilingual_Space/src/tasks.py", line 718, in ki_mlkg
    train_fuse_sentence(args, model_mlkg)
  File "/cluster/scratch/yifhou/Multilingual_Space/src/tasks.py", line 638, in train_fuse_sentence
    outputs_lp, outputs = model_mlkg(**input_e)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/scratch/yifhou/Multilingual_Space/src/models.py", line 103, in forward
    outputs_MLLM = self.MLLM(**inputs).hidden_states
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1026, in forward
    encoder_outputs = self.encoder(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 602, in forward
    layer_outputs = layer_module(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 487, in forward
    self_attention_outputs = self.attention(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 413, in forward
    self_outputs = self.self(
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 341, in forward
    attention_probs = self.dropout(attention_probs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/functional.py", line 983, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 200.44 MiB free; 22.27 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/distributed/launch.py", line 255, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/bin/python', '-u', 'src/main.py']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Traceback (most recent call last):
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 41, in main
    args.func(args)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/accelerate/commands/launch.py", line 469, in launch_command
    multi_gpu_launcher(args)
  File "/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/accelerate/commands/launch.py", line 229, in multi_gpu_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/cluster/project/sachan/yifan/softwares/anaconda/anaconda3/envs/yf_torch/bin/python', '-m', 'torch.distributed.launch', '--use_env', '--nproc_per_node', '8', 'src/main.py']' returned non-zero exit status 1.
