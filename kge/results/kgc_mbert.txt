Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-08, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=0, epoch=50, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', model_name='mBERT', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/mbert_80/final_v3.pt', neg_num=1, patience=2, tmp_dir='./tmp/checkpoints', weight_decay=0.005)
KGC: [ el ] | training loss:  1.5592 | val loss:  1.1786
KGC: [ el ] | training loss:  1.0927 | val loss:  1.0208
KGC: [ el ] | training loss:  0.8951 | val loss:  0.9032
KGC: [ el ] | training loss:  0.8022 | val loss:  0.8777
KGC: [ el ] | training loss:  0.7071 | val loss:  0.8173
KGC: [ el ] | training loss:  0.6383 | val loss:  0.7699
KGC: [ el ] | training loss:  0.581 | val loss:  0.7522
KGC: [ el ] | training loss:  0.5354 | val loss:  0.8085
The performance (hit@1, hit@10) of language [ el ] is:  13.766  and  55.0639
KGC: [ en ] | training loss:  0.6964 | val loss:  0.5344
KGC: [ en ] | training loss:  0.4699 | val loss:  0.4689
KGC: [ en ] | training loss:  0.4011 | val loss:  0.448
KGC: [ en ] | training loss:  0.3528 | val loss:  0.4285
KGC: [ en ] | training loss:  0.3183 | val loss:  0.4248
KGC: [ en ] | training loss:  0.3039 | val loss:  0.4341
The performance (hit@1, hit@10) of language [ en ] is:  8.3065  and  37.5938
KGC: [ es ] | training loss:  0.7732 | val loss:  0.5754
KGC: [ es ] | training loss:  0.5214 | val loss:  0.5119
KGC: [ es ] | training loss:  0.4691 | val loss:  0.4736
KGC: [ es ] | training loss:  0.4292 | val loss:  0.5031
KGC: [ es ] | training loss:  0.4133 | val loss:  0.4445
KGC: [ es ] | training loss:  0.3701 | val loss:  0.4937
The performance (hit@1, hit@10) of language [ es ] is:  13.3472  and  39.5842
KGC: [ fr ] | training loss:  0.7981 | val loss:  0.6355
KGC: [ fr ] | training loss:  0.5607 | val loss:  0.5558
KGC: [ fr ] | training loss:  0.4844 | val loss:  0.5702
KGC: [ fr ] | training loss:  0.449 | val loss:  0.5251
KGC: [ fr ] | training loss:  0.4079 | val loss:  0.4966
KGC: [ fr ] | training loss:  0.3814 | val loss:  0.481
KGC: [ fr ] | training loss:  0.3616 | val loss:  0.512
The performance (hit@1, hit@10) of language [ fr ] is:  17.6696  and  46.152
KGC: [ ja ] | training loss:  1.2952 | val loss:  1.0419
KGC: [ ja ] | training loss:  0.9584 | val loss:  0.8975
KGC: [ ja ] | training loss:  0.8452 | val loss:  0.8286
KGC: [ ja ] | training loss:  0.7458 | val loss:  0.7918
KGC: [ ja ] | training loss:  0.6862 | val loss:  0.7318
KGC: [ ja ] | training loss:  0.6385 | val loss:  0.7714
KGC: [ ja ] | training loss:  0.6164 | val loss:  0.7177
KGC: [ ja ] | training loss:  0.5773 | val loss:  0.7053
KGC: [ ja ] | training loss:  0.5559 | val loss:  0.7362
The performance (hit@1, hit@10) of language [ ja ] is:  13.5985  and  42.2294
