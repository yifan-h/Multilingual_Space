Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-08, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=2, epoch=50, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', model_name='XLM', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlm_80/final_v3.pt', neg_num=1, patience=2, tmp_dir='./tmp/checkpoints', weight_decay=0.005)
KGC: [ el ] | training loss:  2.7648 | val loss:  2.4789
KGC: [ el ] | training loss:  2.1951 | val loss:  2.0345
KGC: [ el ] | training loss:  1.9387 | val loss:  1.9687
KGC: [ el ] | training loss:  1.8431 | val loss:  1.8339
KGC: [ el ] | training loss:  1.7401 | val loss:  1.7076
KGC: [ el ] | training loss:  1.6077 | val loss:  1.6039
KGC: [ el ] | training loss:  1.4992 | val loss:  1.4957
KGC: [ el ] | training loss:  1.4438 | val loss:  1.5657
KGC: [ el ] | training loss:  1.5092 | val loss:  1.5569
KGC: [ el ] | training loss:  1.4759 | val loss:  1.6887
The performance (hit@1, hit@10) of language [ el ] is:  3.9331  and  25.3687
KGC: [ en ] | training loss:  1.9239 | val loss:  1.2473
KGC: [ en ] | training loss:  1.0344 | val loss:  0.9579
KGC: [ en ] | training loss:  1.0714 | val loss:  2.8291
The performance (hit@1, hit@10) of language [ en ] is:  0.0134  and  0.4689
KGC: [ es ] | training loss:  2.8093 | val loss:  2.8127
KGC: [ es ] | training loss:  2.5039 | val loss:  2.7942
KGC: [ es ] | training loss:  2.8308 | val loss:  2.833
The performance (hit@1, hit@10) of language [ es ] is:  0.1663  and  1.0603
KGC: [ fr ] | training loss:  2.8042 | val loss:  2.8149
KGC: [ fr ] | training loss:  2.7843 | val loss:  2.7774
KGC: [ fr ] | training loss:  2.8269 | val loss:  2.8307
The performance (hit@1, hit@10) of language [ fr ] is:  0.024  and  0.3117
KGC: [ ja ] | training loss:  2.6097 | val loss:  2.0656
KGC: [ ja ] | training loss:  1.7473 | val loss:  1.5168
KGC: [ ja ] | training loss:  1.4869 | val loss:  1.4344
KGC: [ ja ] | training loss:  1.4952 | val loss:  1.4097
KGC: [ ja ] | training loss:  1.3821 | val loss:  1.3515
KGC: [ ja ] | training loss:  1.2947 | val loss:  1.2571
KGC: [ ja ] | training loss:  1.242 | val loss:  1.2443
KGC: [ ja ] | training loss:  1.2207 | val loss:  1.1976
KGC: [ ja ] | training loss:  1.1706 | val loss:  1.1671
KGC: [ ja ] | training loss:  2.589 | val loss:  2.8322
The performance (hit@1, hit@10) of language [ ja ] is:  0.0463  and  0.5088
