Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-08, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=1, epoch=50, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', model_name='mBERT-KG', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/mbert_80/final_v3.pt', neg_num=1, patience=2, tmp_dir='./tmp/checkpoints', weight_decay=0.005)
KGC: [ el ] | training loss:  1.5176 | val loss:  1.245
KGC: [ el ] | training loss:  1.0986 | val loss:  1.0659
KGC: [ el ] | training loss:  0.9871 | val loss:  0.995
KGC: [ el ] | training loss:  0.9015 | val loss:  0.9588
KGC: [ el ] | training loss:  0.8382 | val loss:  0.9225
KGC: [ el ] | training loss:  0.7653 | val loss:  0.8888
KGC: [ el ] | training loss:  0.7248 | val loss:  0.8436
KGC: [ el ] | training loss:  0.6453 | val loss:  0.8632
KGC: [ el ] | training loss:  0.6209 | val loss:  0.841
KGC: [ el ] | training loss:  0.5732 | val loss:  0.7997
KGC: [ el ] | training loss:  0.5616 | val loss:  0.799
KGC: [ el ] | training loss:  0.4918 | val loss:  0.7809
KGC: [ el ] | training loss:  0.483 | val loss:  0.836
The performance (hit@1, hit@10) of language [ el ] is:  15.2409  and  51.0324
KGC: [ en ] | training loss:  0.747 | val loss:  0.5491
KGC: [ en ] | training loss:  0.5153 | val loss:  0.4955
KGC: [ en ] | training loss:  0.4311 | val loss:  0.4792
KGC: [ en ] | training loss:  0.3882 | val loss:  0.4663
KGC: [ en ] | training loss:  0.3612 | val loss:  0.4368
KGC: [ en ] | training loss:  0.3413 | val loss:  0.4536
KGC: [ en ] | training loss:  0.3265 | val loss:  0.4235
KGC: [ en ] | training loss:  0.3502 | val loss:  0.5055
The performance (hit@1, hit@10) of language [ en ] is:  6.672  and  28.2288
KGC: [ es ] | training loss:  0.7459 | val loss:  0.5707
KGC: [ es ] | training loss:  0.5515 | val loss:  0.5273
KGC: [ es ] | training loss:  0.4968 | val loss:  0.4801
KGC: [ es ] | training loss:  0.4477 | val loss:  0.4949
KGC: [ es ] | training loss:  0.4224 | val loss:  0.4832
KGC: [ es ] | training loss:  0.3984 | val loss:  0.4807
KGC: [ es ] | training loss:  0.3847 | val loss:  0.4964
The performance (hit@1, hit@10) of language [ es ] is:  4.8233  and  38.3368
KGC: [ fr ] | training loss:  0.8023 | val loss:  0.6472
KGC: [ fr ] | training loss:  0.5849 | val loss:  0.5714
KGC: [ fr ] | training loss:  0.5157 | val loss:  0.5536
KGC: [ fr ] | training loss:  0.4813 | val loss:  0.5498
KGC: [ fr ] | training loss:  0.4473 | val loss:  0.5145
KGC: [ fr ] | training loss:  0.432 | val loss:  0.5388
KGC: [ fr ] | training loss:  0.4211 | val loss:  0.5569
The performance (hit@1, hit@10) of language [ fr ] is:  15.2002  and  40.4939
KGC: [ ja ] | training loss:  1.2708 | val loss:  1.0599
KGC: [ ja ] | training loss:  1.0035 | val loss:  0.9281
KGC: [ ja ] | training loss:  0.8594 | val loss:  0.8822
KGC: [ ja ] | training loss:  0.7753 | val loss:  0.8172
KGC: [ ja ] | training loss:  0.7018 | val loss:  0.7697
KGC: [ ja ] | training loss:  0.6624 | val loss:  0.7569
KGC: [ ja ] | training loss:  0.6225 | val loss:  0.7639
KGC: [ ja ] | training loss:  0.5837 | val loss:  0.725
KGC: [ ja ] | training loss:  0.5666 | val loss:  0.7055
KGC: [ ja ] | training loss:  0.5447 | val loss:  0.7069
KGC: [ ja ] | training loss:  0.5198 | val loss:  0.6991
KGC: [ ja ] | training loss:  0.5038 | val loss:  0.6945
KGC: [ ja ] | training loss:  0.4933 | val loss:  0.6937
KGC: [ ja ] | training loss:  0.4753 | val loss:  0.6904
KGC: [ ja ] | training loss:  0.4571 | val loss:  0.6938
The performance (hit@1, hit@10) of language [ ja ] is:  18.2239  and  41.3043