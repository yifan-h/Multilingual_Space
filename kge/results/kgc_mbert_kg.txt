Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-08, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=1, epoch=50, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/bert-base-multilingual-cased', model_name='mBERT-KG', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/mbert_80/final_v3.pt', neg_num=1, patience=2, tmp_dir='./tmp/checkpoints', weight_decay=0.005)
KGC: [ el ] | training loss:  1.4129 | val loss:  1.135 | hit@1:  0.1544 hit@10:  0.4435
KGC: [ el ] | training loss:  1.0157 | val loss:  1.0323 | hit@1:  0.1632 hit@10:  0.469
KGC: [ el ] | training loss:  0.9309 | val loss:  0.9499 | hit@1:  0.1072 hit@10:  0.4848
KGC: [ el ] | training loss:  0.8158 | val loss:  0.9028 | hit@1:  0.1377 hit@10:  0.4887
KGC: [ el ] | training loss:  0.7153 | val loss:  0.8497 | hit@1:  0.1455 hit@10:  0.5064
KGC: [ el ] | training loss:  0.6621 | val loss:  0.8136 | hit@1:  0.1701 hit@10:  0.4985
KGC: [ el ] | training loss:  0.5948 | val loss:  0.7863 | hit@1:  0.1819 hit@10:  0.5369
KGC: [ el ] | training loss:  0.5592 | val loss:  0.7965 | hit@1:  0.177 hit@10:  0.5359
KGC: [ el ] | training loss:  0.5272 | val loss:  0.8172 | hit@1:  0.178 hit@10:  0.5408
KGC: [ el ] | training loss:  0.4955 | val loss:  0.7676 | hit@1:  0.174 hit@10:  0.5379
KGC: [ el ] | training loss:  0.4431 | val loss:  0.7669 | hit@1:  0.1898 hit@10:  0.5674
KGC: [ el ] | training loss:  0.4345 | val loss:  0.8104 | hit@1:  0.1809 hit@10:  0.5015
KGC: [ el ] | training loss:  0.4263 | val loss:  0.7844 | hit@1:  0.1799 hit@10:  0.5418
KGC: [ el ] | training loss:  0.3852 | val loss:  0.7943 | hit@1:  0.179 hit@10:  0.5359
KGC: [ el ] | training loss:  0.3706 | val loss:  0.7532 | hit@1:  0.1632 hit@10:  0.5477
KGC: [ el ] | training loss:  0.3573 | val loss:  0.8452 | hit@1:  0.176 hit@10:  0.5457
KGC: [ el ] | training loss:  0.3285 | val loss:  0.8171 | hit@1:  0.1632 hit@10:  0.5211
KGC: [ el ] | training loss:  0.338 | val loss:  0.7898 | hit@1:  0.178 hit@10:  0.5644
KGC: [ el ] | training loss:  0.3224 | val loss:  0.8707 | hit@1:  0.1937 hit@10:  0.5683
KGC: [ el ] | training loss:  0.3116 | val loss:  0.7786 | hit@1:  0.1888 hit@10:  0.5526
KGC: [ el ] | training loss:  0.289 | val loss:  0.7972 | hit@1:  0.1849 hit@10:  0.5733
KGC: [ el ] | training loss:  0.2821 | val loss:  0.8799 | hit@1:  0.1908 hit@10:  0.5831
KGC: [ el ] | training loss:  0.2726 | val loss:  0.8456 | hit@1:  0.1809 hit@10:  0.5349
KGC: [ el ] | training loss:  0.2737 | val loss:  0.8517 | hit@1:  0.1829 hit@10:  0.5536
KGC: [ el ] | training loss:  0.2561 | val loss:  0.8379 | hit@1:  0.174 hit@10:  0.5516
KGC: [ el ] | training loss:  0.248 | val loss:  0.8778 | hit@1:  0.1652 hit@10:  0.5821
KGC: [ el ] | training loss:  0.242 | val loss:  0.8624 | hit@1:  0.1534 hit@10:  0.5703
KGC: [ el ] | training loss:  0.2369 | val loss:  0.84 | hit@1:  0.1691 hit@10:  0.5595
KGC: [ el ] | training loss:  0.2244 | val loss:  0.9259 | hit@1:  0.1799 hit@10:  0.5408
KGC: [ el ] | training loss:  0.2231 | val loss:  0.8773 | hit@1:  0.1976 hit@10:  0.5565
KGC: [ el ] | training loss:  0.2269 | val loss:  0.9112 | hit@1:  0.1839 hit@10:  0.5634
KGC: [ el ] | training loss:  0.2137 | val loss:  0.8696 | hit@1:  0.1829 hit@10:  0.5782
KGC: [ el ] | training loss:  0.2059 | val loss:  0.9138 | hit@1:  0.1701 hit@10:  0.5516
KGC: [ el ] | training loss:  0.2261 | val loss:  0.9152 | hit@1:  0.1829 hit@10:  0.5467
KGC: [ el ] | training loss:  0.21 | val loss:  0.9239 | hit@1:  0.1563 hit@10:  0.5408
KGC: [ el ] | training loss:  0.2131 | val loss:  0.8913 | hit@1:  0.1849 hit@10:  0.5772
KGC: [ el ] | training loss:  0.2083 | val loss:  0.8549 | hit@1:  0.1839 hit@10:  0.5506
KGC: [ el ] | training loss:  0.2101 | val loss:  0.8377 | hit@1:  0.178 hit@10:  0.5261
KGC: [ el ] | training loss:  0.2183 | val loss:  0.8885 | hit@1:  0.1721 hit@10:  0.5329
KGC: [ el ] | training loss:  0.2193 | val loss:  0.9231 | hit@1:  0.1898 hit@10:  0.5752
KGC: [ el ] | training loss:  0.2072 | val loss:  0.9013 | hit@1:  0.1819 hit@10:  0.5369
KGC: [ el ] | training loss:  0.2162 | val loss:  0.8946 | hit@1:  0.1642 hit@10:  0.5467
KGC: [ el ] | training loss:  0.2312 | val loss:  0.9239 | hit@1:  0.175 hit@10:  0.5506
KGC: [ el ] | training loss:  0.2176 | val loss:  0.8946 | hit@1:  0.1603 hit@10:  0.5644
KGC: [ el ] | training loss:  0.2142 | val loss:  0.9639 | hit@1:  0.1829 hit@10:  0.5605
KGC: [ el ] | training loss:  0.2064 | val loss:  0.8971 | hit@1:  0.1691 hit@10:  0.5477
KGC: [ el ] | training loss:  0.2154 | val loss:  0.8884 | hit@1:  0.1495 hit@10:  0.5133
KGC: [ el ] | training loss:  0.2103 | val loss:  0.9169 | hit@1:  0.1839 hit@10:  0.5447
KGC: [ el ] | training loss:  0.2156 | val loss:  0.9801 | hit@1:  0.177 hit@10:  0.5339
KGC: [ el ] | training loss:  0.2126 | val loss:  0.8858 | hit@1:  0.1849 hit@10:  0.5418
The performance (hit@1, hit@10) of language [ el ] is:  [0.7532, 0.1632, 0.5477]
KGC: [ en ] | training loss:  0.7508 | val loss:  0.563 | hit@1:  0.0943 hit@10:  0.3052
KGC: [ en ] | training loss:  0.5226 | val loss:  0.5106 | hit@1:  0.0954 hit@10:  0.2942
KGC: [ en ] | training loss:  0.4499 | val loss:  0.4868 | hit@1:  0.0694 hit@10:  0.3119
KGC: [ en ] | training loss:  0.4119 | val loss:  0.4671 | hit@1:  0.0618 hit@10:  0.2919
KGC: [ en ] | training loss:  0.3756 | val loss:  0.4598 | hit@1:  0.0484 hit@10:  0.3134
KGC: [ en ] | training loss:  0.3795 | val loss:  0.4731 | hit@1:  0.0399 hit@10:  0.235
KGC: [ en ] | training loss:  0.3583 | val loss:  0.4888 | hit@1:  0.0766 hit@10:  0.2765
KGC: [ en ] | training loss:  0.3714 | val loss:  0.5086 | hit@1:  0.0528 hit@10:  0.2369
KGC: [ en ] | training loss:  0.3858 | val loss:  0.5802 | hit@1:  0.0665 hit@10:  0.2815
KGC: [ en ] | training loss:  0.4002 | val loss:  0.5794 | hit@1:  0.0813 hit@10:  0.2645
KGC: [ en ] | training loss:  0.4501 | val loss:  0.6914 | hit@1:  0.0547 hit@10:  0.2276
KGC: [ en ] | training loss:  0.516 | val loss:  0.6783 | hit@1:  0.0568 hit@10:  0.2412
KGC: [ en ] | training loss:  0.4864 | val loss:  0.6613 | hit@1:  0.061 hit@10:  0.242
KGC: [ en ] | training loss:  0.4799 | val loss:  0.6377 | hit@1:  0.0612 hit@10:  0.2483
KGC: [ en ] | training loss:  0.4548 | val loss:  0.675 | hit@1:  0.0686 hit@10:  0.2333
KGC: [ en ] | training loss:  0.4423 | val loss:  0.6591 | hit@1:  0.0693 hit@10:  0.2362
KGC: [ en ] | training loss:  0.4268 | val loss:  0.6852 | hit@1:  0.0662 hit@10:  0.2591
KGC: [ en ] | training loss:  0.4228 | val loss:  0.6695 | hit@1:  0.0652 hit@10:  0.2618
KGC: [ en ] | training loss:  0.4011 | val loss:  0.654 | hit@1:  0.0732 hit@10:  0.278
KGC: [ en ] | training loss:  0.3916 | val loss:  0.6284 | hit@1:  0.0343 hit@10:  0.2523
KGC: [ en ] | training loss:  0.4035 | val loss:  0.7069 | hit@1:  0.0549 hit@10:  0.2361
KGC: [ en ] | training loss:  0.4308 | val loss:  0.6645 | hit@1:  0.0414 hit@10:  0.2393
KGC: [ en ] | training loss:  0.4101 | val loss:  0.6796 | hit@1:  0.0447 hit@10:  0.24
KGC: [ en ] | training loss:  0.3886 | val loss:  0.6422 | hit@1:  0.0588 hit@10:  0.2816
KGC: [ en ] | training loss:  0.3753 | val loss:  0.6358 | hit@1:  0.0492 hit@10:  0.2606
KGC: [ en ] | training loss:  0.3598 | val loss:  0.6115 | hit@1:  0.0632 hit@10:  0.2615
KGC: [ en ] | training loss:  0.3562 | val loss:  0.6318 | hit@1:  0.0682 hit@10:  0.2566
KGC: [ en ] | training loss:  0.3575 | val loss:  0.6247 | hit@1:  0.0627 hit@10:  0.2406
KGC: [ en ] | training loss:  0.368 | val loss:  0.6886 | hit@1:  0.05 hit@10:  0.227
KGC: [ en ] | training loss:  0.3792 | val loss:  0.6631 | hit@1:  0.0545 hit@10:  0.2149
KGC: [ en ] | training loss:  0.4808 | val loss:  0.7373 | hit@1:  0.0414 hit@10:  0.1972
KGC: [ en ] | training loss:  0.4468 | val loss:  0.7297 | hit@1:  0.0567 hit@10:  0.1854
KGC: [ en ] | training loss:  0.4091 | val loss:  0.6559 | hit@1:  0.0591 hit@10:  0.2505
KGC: [ en ] | training loss:  0.3575 | val loss:  0.6496 | hit@1:  0.0661 hit@10:  0.2365
KGC: [ en ] | training loss:  0.3619 | val loss:  0.6528 | hit@1:  0.0659 hit@10:  0.2714
KGC: [ en ] | training loss:  0.3485 | val loss:  0.6572 | hit@1:  0.0445 hit@10:  0.2249
KGC: [ en ] | training loss:  0.3629 | val loss:  0.6616 | hit@1:  0.0496 hit@10:  0.2471
KGC: [ en ] | training loss:  0.384 | val loss:  0.6777 | hit@1:  0.0398 hit@10:  0.1999
KGC: [ en ] | training loss:  0.3676 | val loss:  0.6638 | hit@1:  0.0383 hit@10:  0.2548
KGC: [ en ] | training loss:  0.383 | val loss:  0.6402 | hit@1:  0.0521 hit@10:  0.2247
KGC: [ en ] | training loss:  0.3522 | val loss:  0.6611 | hit@1:  0.0242 hit@10:  0.2666
KGC: [ en ] | training loss:  0.3378 | val loss:  0.6392 | hit@1:  0.0563 hit@10:  0.2617
KGC: [ en ] | training loss:  0.3391 | val loss:  0.6579 | hit@1:  0.0651 hit@10:  0.2722
KGC: [ en ] | training loss:  0.3402 | val loss:  0.6544 | hit@1:  0.0347 hit@10:  0.2468
KGC: [ en ] | training loss:  0.3429 | val loss:  0.6482 | hit@1:  0.0535 hit@10:  0.2471
KGC: [ en ] | training loss:  0.341 | val loss:  0.651 | hit@1:  0.0603 hit@10:  0.2468
KGC: [ en ] | training loss:  0.4431 | val loss:  0.7517 | hit@1:  0.0304 hit@10:  0.2267
KGC: [ en ] | training loss:  0.5877 | val loss:  0.8071 | hit@1:  0.0165 hit@10:  0.1898
KGC: [ en ] | training loss:  0.5012 | val loss:  0.7629 | hit@1:  0.0233 hit@10:  0.2024
KGC: [ en ] | training loss:  0.6629 | val loss:  1.0351 | hit@1:  0.038 hit@10:  0.1746
The performance (hit@1, hit@10) of language [ en ] is:  [0.4598, 0.0484, 0.3134]
KGC: [ es ] | training loss:  0.7866 | val loss:  0.5803 | hit@1:  0.12 hit@10:  0.3281
KGC: [ es ] | training loss:  0.5517 | val loss:  0.5407 | hit@1:  0.1154 hit@10:  0.3351
KGC: [ es ] | training loss:  0.4969 | val loss:  0.5417 | hit@1:  0.1285 hit@10:  0.373
KGC: [ es ] | training loss:  0.4693 | val loss:  0.4925 | hit@1:  0.115 hit@10:  0.3713
KGC: [ es ] | training loss:  0.4392 | val loss:  0.4817 | hit@1:  0.122 hit@10:  0.3663
KGC: [ es ] | training loss:  0.5195 | val loss:  0.7418 | hit@1:  0.0626 hit@10:  0.3692
KGC: [ es ] | training loss:  0.5843 | val loss:  0.6759 | hit@1:  0.0642 hit@10:  0.3586
KGC: [ es ] | training loss:  0.5365 | val loss:  0.6958 | hit@1:  0.0983 hit@10:  0.3713
KGC: [ es ] | training loss:  0.5239 | val loss:  0.6895 | hit@1:  0.1029 hit@10:  0.3638
KGC: [ es ] | training loss:  0.5145 | val loss:  0.7004 | hit@1:  0.0877 hit@10:  0.3511
KGC: [ es ] | training loss:  0.4789 | val loss:  0.6752 | hit@1:  0.0387 hit@10:  0.3208
KGC: [ es ] | training loss:  0.458 | val loss:  0.7068 | hit@1:  0.0778 hit@10:  0.3516
KGC: [ es ] | training loss:  0.4349 | val loss:  0.6678 | hit@1:  0.0665 hit@10:  0.3547
KGC: [ es ] | training loss:  0.4173 | val loss:  0.6833 | hit@1:  0.0497 hit@10:  0.3628
KGC: [ es ] | training loss:  0.4191 | val loss:  0.6601 | hit@1:  0.0688 hit@10:  0.3659
KGC: [ es ] | training loss:  0.4029 | val loss:  0.6647 | hit@1:  0.0636 hit@10:  0.3368
KGC: [ es ] | training loss:  0.3894 | val loss:  0.7034 | hit@1:  0.0694 hit@10:  0.3489
KGC: [ es ] | training loss:  0.3774 | val loss:  0.7205 | hit@1:  0.0701 hit@10:  0.337
KGC: [ es ] | training loss:  0.3849 | val loss:  0.745 | hit@1:  0.0578 hit@10:  0.3268
KGC: [ es ] | training loss:  0.3715 | val loss:  0.717 | hit@1:  0.0696 hit@10:  0.2834
KGC: [ es ] | training loss:  0.3648 | val loss:  0.7256 | hit@1:  0.1146 hit@10:  0.3499
KGC: [ es ] | training loss:  0.3609 | val loss:  0.68 | hit@1:  0.1087 hit@10:  0.3426
KGC: [ es ] | training loss:  0.3498 | val loss:  0.7544 | hit@1:  0.052 hit@10:  0.3287
KGC: [ es ] | training loss:  0.3444 | val loss:  0.746 | hit@1:  0.0447 hit@10:  0.3008
KGC: [ es ] | training loss:  0.3452 | val loss:  0.7051 | hit@1:  0.0726 hit@10:  0.337
KGC: [ es ] | training loss:  0.3449 | val loss:  0.7148 | hit@1:  0.0647 hit@10:  0.3314
KGC: [ es ] | training loss:  0.3431 | val loss:  0.7134 | hit@1:  0.0898 hit@10:  0.3565
KGC: [ es ] | training loss:  0.3357 | val loss:  0.736 | hit@1:  0.0669 hit@10:  0.3555
KGC: [ es ] | training loss:  0.3222 | val loss:  0.7343 | hit@1:  0.0917 hit@10:  0.3545
KGC: [ es ] | training loss:  0.3212 | val loss:  0.7293 | hit@1:  0.0536 hit@10:  0.3601
KGC: [ es ] | training loss:  0.3007 | val loss:  0.7392 | hit@1:  0.0696 hit@10:  0.3699
KGC: [ es ] | training loss:  0.3174 | val loss:  0.7309 | hit@1:  0.0775 hit@10:  0.3393
KGC: [ es ] | training loss:  0.3982 | val loss:  1.0124 | hit@1:  0.0247 hit@10:  0.206
KGC: [ es ] | training loss:  0.5482 | val loss:  0.8674 | hit@1:  0.0902 hit@10:  0.2996
KGC: [ es ] | training loss:  0.5087 | val loss:  0.8659 | hit@1:  0.0509 hit@10:  0.2491
KGC: [ es ] | training loss:  0.5055 | val loss:  0.9803 | hit@1:  0.0572 hit@10:  0.2324
KGC: [ es ] | training loss:  0.7023 | val loss:  1.4055 | hit@1:  0.0137 hit@10:  0.1056
KGC: [ es ] | training loss:  0.7187 | val loss:  0.9295 | hit@1:  0.036 hit@10:  0.2272
KGC: [ es ] | training loss:  0.6484 | val loss:  0.9709 | hit@1:  0.0096 hit@10:  0.0549
KGC: [ es ] | training loss:  0.6741 | val loss:  0.8978 | hit@1:  0.02 hit@10:  0.1353
KGC: [ es ] | training loss:  0.6875 | val loss:  0.9392 | hit@1:  0.022 hit@10:  0.2133
KGC: [ es ] | training loss:  0.5805 | val loss:  0.8351 | hit@1:  0.0185 hit@10:  0.2291
KGC: [ es ] | training loss:  0.5419 | val loss:  0.8103 | hit@1:  0.0489 hit@10:  0.2472
KGC: [ es ] | training loss:  0.5279 | val loss:  0.9045 | hit@1:  0.0183 hit@10:  0.1495
KGC: [ es ] | training loss:  0.5428 | val loss:  0.8241 | hit@1:  0.031 hit@10:  0.2732
KGC: [ es ] | training loss:  0.5299 | val loss:  0.806 | hit@1:  0.0272 hit@10:  0.2042
KGC: [ es ] | training loss:  0.5119 | val loss:  0.861 | hit@1:  0.0249 hit@10:  0.2262
KGC: [ es ] | training loss:  0.4586 | val loss:  0.8744 | hit@1:  0.0214 hit@10:  0.1582
KGC: [ es ] | training loss:  0.4416 | val loss:  0.8049 | hit@1:  0.0229 hit@10:  0.2655
KGC: [ es ] | training loss:  0.4616 | val loss:  0.7813 | hit@1:  0.0482 hit@10:  0.262
The performance (hit@1, hit@10) of language [ es ] is:  [0.4817, 0.122, 0.3663]
KGC: [ fr ] | training loss:  0.8162 | val loss:  0.6388 | hit@1:  0.1369 hit@10:  0.3946
KGC: [ fr ] | training loss:  0.5866 | val loss:  0.5649 | hit@1:  0.1546 hit@10:  0.4011
KGC: [ fr ] | training loss:  0.4887 | val loss:  0.5351 | hit@1:  0.1482 hit@10:  0.4107
KGC: [ fr ] | training loss:  0.4742 | val loss:  0.53 | hit@1:  0.1736 hit@10:  0.4198
KGC: [ fr ] | training loss:  0.4349 | val loss:  0.5149 | hit@1:  0.1688 hit@10:  0.4251
KGC: [ fr ] | training loss:  0.4097 | val loss:  0.5032 | hit@1:  0.1534 hit@10:  0.4251
KGC: [ fr ] | training loss:  0.3915 | val loss:  0.5218 | hit@1:  0.1626 hit@10:  0.4167
KGC: [ fr ] | training loss:  0.3758 | val loss:  0.5219 | hit@1:  0.1331 hit@10:  0.4296
KGC: [ fr ] | training loss:  0.3764 | val loss:  0.5223 | hit@1:  0.1676 hit@10:  0.426
KGC: [ fr ] | training loss:  0.3692 | val loss:  0.5523 | hit@1:  0.1482 hit@10:  0.4021
KGC: [ fr ] | training loss:  0.3808 | val loss:  0.5496 | hit@1:  0.118 hit@10:  0.3858
KGC: [ fr ] | training loss:  0.4199 | val loss:  0.5954 | hit@1:  0.1019 hit@10:  0.333
KGC: [ fr ] | training loss:  0.5572 | val loss:  0.7163 | hit@1:  0.0307 hit@10:  0.3045
KGC: [ fr ] | training loss:  0.4856 | val loss:  0.6888 | hit@1:  0.1343 hit@10:  0.3975
KGC: [ fr ] | training loss:  0.4673 | val loss:  0.6974 | hit@1:  0.1009 hit@10:  0.3591
KGC: [ fr ] | training loss:  0.4802 | val loss:  0.7021 | hit@1:  0.0837 hit@10:  0.3728
KGC: [ fr ] | training loss:  0.5346 | val loss:  1.236 | hit@1:  0.0743 hit@10:  0.1942
KGC: [ fr ] | training loss:  1.1267 | val loss:  0.9722 | hit@1:  0.0031 hit@10:  0.0281
KGC: [ fr ] | training loss:  0.7204 | val loss:  0.9575 | hit@1:  0.0338 hit@10:  0.2163
KGC: [ fr ] | training loss:  0.7934 | val loss:  1.0195 | hit@1:  0.0259 hit@10:  0.1527
KGC: [ fr ] | training loss:  1.0627 | val loss:  1.5001 | hit@1:  0.0228 hit@10:  0.1275
KGC: [ fr ] | training loss:  1.074 | val loss:  1.0314 | hit@1:  0.0693 hit@10:  0.2421
KGC: [ fr ] | training loss:  0.9812 | val loss:  3.518 | hit@1:  0.0031 hit@10:  0.0149
KGC: [ fr ] | training loss:  2.8367 | val loss:  2.8316 | hit@1:  0.0005 hit@10:  0.0031
KGC: [ fr ] | training loss:  2.8335 | val loss:  2.8311 | hit@1:  0.0002 hit@10:  0.0053
KGC: [ fr ] | training loss:  2.8332 | val loss:  2.831 | hit@1:  0.0002 hit@10:  0.0036
KGC: [ fr ] | training loss:  2.8332 | val loss:  2.831 | hit@1:  0.0002 hit@10:  0.0029
KGC: [ fr ] | training loss:  2.8332 | val loss:  2.831 | hit@1:  0.0002 hit@10:  0.0031
KGC: [ fr ] | training loss:  2.8332 | val loss:  2.831 | hit@1:  0.0002 hit@10:  0.0043
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.831 | hit@1:  0.0002 hit@10:  0.0065
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.831 | hit@1:  0.0002 hit@10:  0.0026
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0029
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0031
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0012 hit@10:  0.0046
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0041
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0005 hit@10:  0.0036
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0043
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0007 hit@10:  0.0304
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0031
KGC: [ fr ] | training loss:  2.8331 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0957
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0041
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0014 hit@10:  0.005
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0014 hit@10:  0.0038
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0005 hit@10:  0.0055
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0022 hit@10:  0.0055
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0026
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0012 hit@10:  0.0041
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0108
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0034
KGC: [ fr ] | training loss:  2.833 | val loss:  2.8309 | hit@1:  0.0002 hit@10:  0.0026
The performance (hit@1, hit@10) of language [ fr ] is:  [0.5032, 0.1534, 0.4251]
KGC: [ ja ] | training loss:  1.2666 | val loss:  1.0361 | hit@1:  0.1142 hit@10:  0.3686
KGC: [ ja ] | training loss:  0.9973 | val loss:  0.9482 | hit@1:  0.1799 hit@10:  0.3816
KGC: [ ja ] | training loss:  0.8771 | val loss:  0.8567 | hit@1:  0.1735 hit@10:  0.3936
KGC: [ ja ] | training loss:  0.7886 | val loss:  0.7885 | hit@1:  0.1739 hit@10:  0.4029
KGC: [ ja ] | training loss:  0.7033 | val loss:  0.7528 | hit@1:  0.179 hit@10:  0.4075
KGC: [ ja ] | training loss:  0.6658 | val loss:  0.7541 | hit@1:  0.1832 hit@10:  0.4172
KGC: [ ja ] | training loss:  0.6179 | val loss:  0.7094 | hit@1:  0.1859 hit@10:  0.4255
KGC: [ ja ] | training loss:  0.5753 | val loss:  0.7069 | hit@1:  0.1702 hit@10:  0.4029
KGC: [ ja ] | training loss:  0.5648 | val loss:  0.7014 | hit@1:  0.1401 hit@10:  0.4043
KGC: [ ja ] | training loss:  0.5472 | val loss:  0.7258 | hit@1:  0.1573 hit@10:  0.407
KGC: [ ja ] | training loss:  0.5222 | val loss:  0.6912 | hit@1:  0.1152 hit@10:  0.3839
KGC: [ ja ] | training loss:  0.5074 | val loss:  0.6886 | hit@1:  0.1549 hit@10:  0.3802
KGC: [ ja ] | training loss:  0.4961 | val loss:  0.7329 | hit@1:  0.1725 hit@10:  0.3978
KGC: [ ja ] | training loss:  0.4748 | val loss:  0.7464 | hit@1:  0.1656 hit@10:  0.3811
KGC: [ ja ] | training loss:  0.4723 | val loss:  0.7444 | hit@1:  0.1462 hit@10:  0.3876
KGC: [ ja ] | training loss:  0.463 | val loss:  0.7455 | hit@1:  0.1304 hit@10:  0.3918
KGC: [ ja ] | training loss:  0.4361 | val loss:  0.7318 | hit@1:  0.1466 hit@10:  0.3945
KGC: [ ja ] | training loss:  0.4221 | val loss:  0.749 | hit@1:  0.1179 hit@10:  0.4029
KGC: [ ja ] | training loss:  0.4284 | val loss:  0.7593 | hit@1:  0.1378 hit@10:  0.3982
KGC: [ ja ] | training loss:  0.4219 | val loss:  0.7587 | hit@1:  0.1152 hit@10:  0.408
KGC: [ ja ] | training loss:  0.4176 | val loss:  0.7845 | hit@1:  0.1073 hit@10:  0.4163
KGC: [ ja ] | training loss:  0.4182 | val loss:  0.7636 | hit@1:  0.0967 hit@10:  0.3747
KGC: [ ja ] | training loss:  0.3976 | val loss:  0.7667 | hit@1:  0.1189 hit@10:  0.3978
KGC: [ ja ] | training loss:  0.3966 | val loss:  0.785 | hit@1:  0.1008 hit@10:  0.3881
KGC: [ ja ] | training loss:  0.4051 | val loss:  0.7827 | hit@1:  0.1078 hit@10:  0.3663
KGC: [ ja ] | training loss:  0.413 | val loss:  0.7684 | hit@1:  0.0944 hit@10:  0.37
KGC: [ ja ] | training loss:  0.425 | val loss:  0.7755 | hit@1:  0.099 hit@10:  0.3626
KGC: [ ja ] | training loss:  0.4253 | val loss:  0.8279 | hit@1:  0.105 hit@10:  0.3696
KGC: [ ja ] | training loss:  0.4251 | val loss:  0.7879 | hit@1:  0.1133 hit@10:  0.3904
KGC: [ ja ] | training loss:  0.4314 | val loss:  0.7772 | hit@1:  0.1753 hit@10:  0.4033
KGC: [ ja ] | training loss:  0.4323 | val loss:  0.8203 | hit@1:  0.1503 hit@10:  0.3797
KGC: [ ja ] | training loss:  0.4408 | val loss:  0.8089 | hit@1:  0.1508 hit@10:  0.3784
KGC: [ ja ] | training loss:  0.4415 | val loss:  0.807 | hit@1:  0.1517 hit@10:  0.3682
KGC: [ ja ] | training loss:  0.4502 | val loss:  0.7918 | hit@1:  0.1013 hit@10:  0.3825
KGC: [ ja ] | training loss:  0.4447 | val loss:  0.8137 | hit@1:  0.0883 hit@10:  0.3733
KGC: [ ja ] | training loss:  0.4317 | val loss:  0.79 | hit@1:  0.1147 hit@10:  0.3881
KGC: [ ja ] | training loss:  0.4362 | val loss:  0.8235 | hit@1:  0.0648 hit@10:  0.3663
KGC: [ ja ] | training loss:  0.4495 | val loss:  0.8078 | hit@1:  0.0296 hit@10:  0.3039
KGC: [ ja ] | training loss:  0.4627 | val loss:  0.8282 | hit@1:  0.0342 hit@10:  0.2632
KGC: [ ja ] | training loss:  0.4705 | val loss:  0.8017 | hit@1:  0.0897 hit@10:  0.284
KGC: [ ja ] | training loss:  0.4988 | val loss:  0.8302 | hit@1:  0.031 hit@10:  0.303
KGC: [ ja ] | training loss:  0.5541 | val loss:  0.8763 | hit@1:  0.0527 hit@10:  0.2761
KGC: [ ja ] | training loss:  0.5306 | val loss:  0.8535 | hit@1:  0.0666 hit@10:  0.2969
KGC: [ ja ] | training loss:  0.5083 | val loss:  0.8502 | hit@1:  0.1517 hit@10:  0.3571
KGC: [ ja ] | training loss:  0.5339 | val loss:  0.8651 | hit@1:  0.1189 hit@10:  0.3284
KGC: [ ja ] | training loss:  0.5272 | val loss:  0.8396 | hit@1:  0.1531 hit@10:  0.3682
KGC: [ ja ] | training loss:  0.5212 | val loss:  0.8427 | hit@1:  0.1198 hit@10:  0.3552
KGC: [ ja ] | training loss:  0.5075 | val loss:  0.8312 | hit@1:  0.161 hit@10:  0.3774
KGC: [ ja ] | training loss:  0.5306 | val loss:  0.8443 | hit@1:  0.0236 hit@10:  0.3529
KGC: [ ja ] | training loss:  0.5369 | val loss:  0.8304 | hit@1:  0.1573 hit@10:  0.3719
The performance (hit@1, hit@10) of language [ ja ] is:  [0.6886, 0.1549, 0.3802]
