Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=7, epoch=10, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', model_name='XLM-KG', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlm_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.01)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
KGC: [ en_nb ] | hit@1:  0.7246 hit@5:  0.79381 mrr:  0.71856
KGC: [ en_da ] | hit@1:  0.72433 hit@5:  0.79426 mrr:  0.72042
KGC: [ en_zh ] | hit@1:  0.1896 hit@5:  0.28113 mrr:  0.20333
KGC: [ en_fr ] | hit@1:  0.87731 hit@5:  0.90446 mrr:  0.87208
KGC: [ en_es ] | hit@1:  0.70182 hit@5:  0.77485 mrr:  0.69424
KGC: [ en_it ] | hit@1:  0.68476 hit@5:  0.75755 mrr:  0.6793
KGC: [ en_pl ] | hit@1:  0.67212 hit@5:  0.73991 mrr:  0.66631
KGC: [ en_fa ] | hit@1:  0.30043 hit@5:  0.41526 mrr:  0.31058
KGC: [ en_ru ] | hit@1:  0.45087 hit@5:  0.55403 mrr:  0.45339
KGC: [ en_sv ] | hit@1:  0.71196 hit@5:  0.78282 mrr:  0.70684
KGC: [ en_cs ] | hit@1:  0.69692 hit@5:  0.76777 mrr:  0.69395
KGC: [ en_ast ] | hit@1:  0.71382 hit@5:  0.7833 mrr:  0.71202
KGC: [ en_pt ] | hit@1:  0.71119 hit@5:  0.78349 mrr:  0.70419
KGC: [ en_de ] | hit@1:  0.82264 hit@5:  0.84609 mrr:  0.81866
KGC: [ en_hu ] | hit@1:  0.68432 hit@5:  0.75646 mrr:  0.68347
KGC: [ en_ar ] | hit@1:  0.24795 hit@5:  0.35617 mrr:  0.26009
KGC: [ en_ja ] | hit@1:  0.23146 hit@5:  0.33277 mrr:  0.24304
KGC: [ en_fi ] | hit@1:  0.70497 hit@5:  0.77412 mrr:  0.70055
KGC: [ en_nl ] | hit@1:  0.71628 hit@5:  0.78644 mrr:  0.70906
KGC: [ en_ca ] | hit@1:  0.69739 hit@5:  0.77175 mrr:  0.69332
