Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=6, epoch=10, lm_lr=1e-06, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', model_name='XLM', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlm_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.01)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
KGC: [ en_nb ] | hit@1:  0.44265 hit@5:  0.51952 mrr:  0.44771
KGC: [ en_da ] | hit@1:  0.44851 hit@5:  0.52043 mrr:  0.45125
KGC: [ en_zh ] | hit@1:  0.04059 hit@5:  0.06213 mrr:  0.04735
KGC: [ en_fr ] | hit@1:  0.7859 hit@5:  0.8212 mrr:  0.78393
KGC: [ en_es ] | hit@1:  0.43824 hit@5:  0.51205 mrr:  0.43979
KGC: [ en_it ] | hit@1:  0.42814 hit@5:  0.49847 mrr:  0.43011
KGC: [ en_pl ] | hit@1:  0.42665 hit@5:  0.49505 mrr:  0.42793
KGC: [ en_fa ] | hit@1:  0.03387 hit@5:  0.05887 mrr:  0.04164
KGC: [ en_ru ] | hit@1:  0.15244 hit@5:  0.20552 mrr:  0.16287
KGC: [ en_sv ] | hit@1:  0.44423 hit@5:  0.51531 mrr:  0.44662
KGC: [ en_cs ] | hit@1:  0.42299 hit@5:  0.49982 mrr:  0.4288
KGC: [ en_ast ] | hit@1:  0.45659 hit@5:  0.53025 mrr:  0.4618
KGC: [ en_pt ] | hit@1:  0.44776 hit@5:  0.52106 mrr:  0.44986
KGC: [ en_de ] | hit@1:  0.76481 hit@5:  0.79112 mrr:  0.76055
KGC: [ en_hu ] | hit@1:  0.40935 hit@5:  0.48105 mrr:  0.41549
KGC: [ en_ar ] | hit@1:  0.01998 hit@5:  0.03728 mrr:  0.0277
KGC: [ en_ja ] | hit@1:  0.03562 hit@5:  0.05742 mrr:  0.04331
KGC: [ en_fi ] | hit@1:  0.44313 hit@5:  0.51519 mrr:  0.44555
KGC: [ en_nl ] | hit@1:  0.45946 hit@5:  0.53064 mrr:  0.45995
KGC: [ en_ca ] | hit@1:  0.41663 hit@5:  0.49094 mrr:  0.42222
Namespace(adam_epsilon=1e-06, batch_num=8, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=4, epoch=10, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base', model_name='XLM', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlm_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.0001)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
KGC: [ en_vo ] | hit@1:  0.49729 hit@5:  0.56669 mrr:  0.50155
KGC: [ en_eo ] | hit@1:  0.30966 hit@5:  0.37193 mrr:  0.31825
