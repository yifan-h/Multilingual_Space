Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=6, epoch=10, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', model_name='mBERT', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/mbert_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.01)
KGC: [ en_nb ] | hit@1:  0.47566 hit@5:  0.56324 mrr:  0.4818
KGC: [ en_da ] | hit@1:  0.48196 hit@5:  0.56925 mrr:  0.48898
KGC: [ en_zh ] | hit@1:  0.04519 hit@5:  0.07271 mrr:  0.05399
KGC: [ en_fr ] | hit@1:  0.73697 hit@5:  0.7635 mrr:  0.7351
KGC: [ en_es ] | hit@1:  0.45327 hit@5:  0.53326 mrr:  0.45696
KGC: [ en_it ] | hit@1:  0.44057 hit@5:  0.51813 mrr:  0.44523
KGC: [ en_pl ] | hit@1:  0.44653 hit@5:  0.52645 mrr:  0.45168
KGC: [ en_fa ] | hit@1:  0.06234 hit@5:  0.1129 mrr:  0.07735
KGC: [ en_ru ] | hit@1:  0.15809 hit@5:  0.20966 mrr:  0.16629
KGC: [ en_sv ] | hit@1:  0.46959 hit@5:  0.55477 mrr:  0.47565
KGC: [ en_cs ] | hit@1:  0.45383 hit@5:  0.53512 mrr:  0.45972
KGC: [ en_ast ] | hit@1:  0.49092 hit@5:  0.57472 mrr:  0.49533
KGC: [ en_pt ] | hit@1:  0.46202 hit@5:  0.5395 mrr:  0.46701
KGC: [ en_de ] | hit@1:  0.74214 hit@5:  0.76552 mrr:  0.73979
KGC: [ en_hu ] | hit@1:  0.44433 hit@5:  0.52503 mrr:  0.44999
KGC: [ en_ar ] | hit@1:  0.05304 hit@5:  0.0971 mrr:  0.06635
KGC: [ en_ja ] | hit@1:  0.06029 hit@5:  0.10576 mrr:  0.0736
KGC: [ en_fi ] | hit@1:  0.46686 hit@5:  0.55163 mrr:  0.47319
KGC: [ en_nl ] | hit@1:  0.47764 hit@5:  0.55957 mrr:  0.48041
KGC: [ en_ca ] | hit@1:  0.44716 hit@5:  0.53002 mrr:  0.45499
/anaconda/anaconda3/envs/yf_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 610, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=4, epoch=10, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', model_name='XLMR', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlmr_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.01)
Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=2, epoch=10, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', model_name='mBERT', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/mbert_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.01)
KGC: [ en_vo ] | hit@1:  0.77624 hit@5:  0.84597 mrr:  0.77348
KGC: [ en_eo ] | hit@1:  0.5566 hit@5:  0.63536 mrr:  0.55798

