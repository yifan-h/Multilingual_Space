Some weights of the model checkpoint at /cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Overwriting existing adapter 'ep'.
Overwriting existing adapter 'tp'.
Overwriting existing adapter 'es'.
Overwriting existing adapter 'ts'.
Overwriting existing adapter fusion module 'ep,tp,es,ts'
Namespace(adam_epsilon=1e-06, batch_num=16, data_dir='/cluster/work/sachan/yifan/data/wikidata/downstream', device=2, epoch=10, lr=1e-06, model_dir='/cluster/work/sachan/yifan/huggingface_models/xlm-roberta-large', model_name='XLMR-KG', modelkg_dir='/cluster/project/sachan/yifan/projects/Multilingual_Space/tmp/xlmr_adapter', neg_num=1, patience=2, task_name='ea', tmp_dir='./tmp/checkpoints', weight_decay=0.01)
KGC: [ en_nb ] | hit@1:  0.76292 hit@5:  0.82577 mrr:  0.75694
KGC: [ en_da ] | hit@1:  0.76972 hit@5:  0.83208 mrr:  0.76343
KGC: [ en_zh ] | hit@1:  0.21167 hit@5:  0.29927 mrr:  0.22325
KGC: [ en_mg ] | hit@1:  0.86939 hit@5:  0.91594 mrr:  0.858
KGC: [ en_fr ] | hit@1:  0.84697 hit@5:  0.87572 mrr:  0.83917
KGC: [ en_es ] | hit@1:  0.73469 hit@5:  0.80094 mrr:  0.72642
KGC: [ en_it ] | hit@1:  0.72012 hit@5:  0.78684 mrr:  0.71308
KGC: [ en_pl ] | hit@1:  0.71163 hit@5:  0.77581 mrr:  0.70505
KGC: [ en_fa ] | hit@1:  0.40552 hit@5:  0.51694 mrr:  0.40715
KGC: [ en_ru ] | hit@1:  0.4634 hit@5:  0.55001 mrr:  0.46192
KGC: [ en_vo ] | hit@1:  0.87949 hit@5:  0.92079 mrr:  0.87175
KGC: [ en_sv ] | hit@1:  0.75094 hit@5:  0.81541 mrr:  0.74366
KGC: [ en_eo ] | hit@1:  0.68854 hit@5:  0.75339 mrr:  0.68289
KGC: [ en_io ] | hit@1:  0.86255 hit@5:  0.91137 mrr:  0.85648
KGC: [ en_cs ] | hit@1:  0.72667 hit@5:  0.78491 mrr:  0.71875
KGC: [ en_ast ] | hit@1:  0.78386 hit@5:  0.84592 mrr:  0.77849
KGC: [ en_pt ] | hit@1:  0.7421 hit@5:  0.80762 mrr:  0.73543
KGC: [ en_de ] | hit@1:  0.80645 hit@5:  0.83093 mrr:  0.7997
KGC: [ en_hu ] | hit@1:  0.72703 hit@5:  0.78921 mrr:  0.72165
KGC: [ en_ar ] | hit@1:  0.36366 hit@5:  0.47818 mrr:  0.37093
KGC: [ en_ja ] | hit@1:  0.31465 hit@5:  0.42304 mrr:  0.32235
KGC: [ en_fi ] | hit@1:  0.74253 hit@5:  0.80609 mrr:  0.73682
KGC: [ en_nl ] | hit@1:  0.75628 hit@5:  0.82293 mrr:  0.74723
KGC: [ en_yo ] | hit@1:  0.86126 hit@5:  0.91099 mrr:  0.85266
KGC: [ en_ca ] | hit@1:  0.73856 hit@5:  0.8064 mrr:  0.7319
